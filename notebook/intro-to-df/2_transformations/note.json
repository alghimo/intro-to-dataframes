{
  "paragraphs": [
    {
      "text": "%md ## Transformations",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:02:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905344645_-1743523733",
      "id": "20160417-150224_1362432544",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eTransformations\u003c/h2\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:02:24 PM",
      "dateStarted": "Apr 17, 2016 3:02:44 PM",
      "dateFinished": "Apr 17, 2016 3:02:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Foreword: different ways of working with expressions\r\n\r\nWhen working with DataFrames, most of the methods will accept different types of expressions:",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 12:24:38 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905364544_234901084",
      "id": "20160417-150244_1644963948",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eForeword: different ways of working with expressions\u003c/h3\u003e\n\u003cp\u003eWhen working with DataFrames, most of the methods will accept different types of expressions:\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:02:44 PM",
      "dateStarted": "Apr 17, 2016 3:02:59 PM",
      "dateFinished": "Apr 17, 2016 3:02:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Column Expressions\r\nYou will usually find them in one of these forms:\r\n```\r\nmyDf(\"column_name\")\r\n// or\r\nnew Column(\"column_name\")\r\n```\r\n---\r\nAdditionally, all sql functions return a column as well. Further reference:\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\u003e\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003e (scroll down to find different types of transformation functions: collection, math, date, etc.)\r\n\r\n---\r\nThere is also an **implicit** conversion defined in \u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLImplicits\u003e that will convert a Scala Symbol to a ColumnName instance.\r\n\r\nIn practice, you can treat this as if it were a regular Column, so you can use something like this:\r\n```\r\nmyDf.select(\u0027col_a, \u0027col_b)\r\n```\r\nor\r\n```\r\nmyDf.select($\"col_a\", $\"col_b\")\r\n```",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 12:25:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1462019104006_-809714374",
      "id": "20160430-122504_1312944220",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eColumn Expressions\u003c/h4\u003e\n\u003cp\u003eYou will usually find them in one of these forms:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emyDf(\"column_name\")\n// or\nnew Column(\"column_name\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr /\u003e\n\u003cp\u003eAdditionally, all sql functions return a column as well. Further reference:\n\u003cbr  /\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\u003c/a\u003e\n\u003cbr  /\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003c/a\u003e (scroll down to find different types of transformation functions: collection, math, date, etc.)\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003eThere is also an \u003cstrong\u003eimplicit\u003c/strong\u003e conversion defined in \u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLImplicits\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLImplicits\u003c/a\u003e that will convert a Scala Symbol to a ColumnName instance.\u003c/p\u003e\n\u003cp\u003eIn practice, you can treat this as if it were a regular Column, so you can use something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emyDf.select(\u0027col_a, \u0027col_b)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emyDf.select($\"col_a\", $\"col_b\")\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Apr 30, 2016 12:25:04 PM",
      "dateStarted": "Apr 30, 2016 12:25:49 PM",
      "dateFinished": "Apr 30, 2016 12:25:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### SQL Expressions\r\nIn many cases, you can also use either strings or SQL expressions. The thing is that usually, you cannot mix \"string\" or \"sql expressions\" with column expressions.\r\nOne thing you can do if you want to introduce a SQL expressions as a column expression is to wrap it into the *expr* function.\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003e\r\n\r\n---\r\nLet\u0027s show it with some examples:",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 12:26:44 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905399518_242073395",
      "id": "20160417-150319_367175170",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eSQL Expressions\u003c/h4\u003e\n\u003cp\u003eIn many cases, you can also use either strings or SQL expressions. The thing is that usually, you cannot mix \u0026ldquo;string\u0026rdquo; or \u0026ldquo;sql expressions\u0026rdquo; with column expressions.\n\u003cbr  /\u003eOne thing you can do if you want to introduce a SQL expressions as a column expression is to wrap it into the \u003cem\u003eexpr\u003c/em\u003e function.\n\u003cbr  /\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003c/a\u003e\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003eLet\u0027s show it with some examples:\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:03:19 PM",
      "dateStarted": "Apr 30, 2016 12:26:44 PM",
      "dateFinished": "Apr 30, 2016 12:26:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.Column\r\nclickstreamDf.select(\u0027prev_id, clickstreamDf(\"curr_id\"), new Column(\"n\"), $\"type\") // Works",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:04:03 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905429462_227114161",
      "id": "20160417-150349_369762896",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Column\nres26: org.apache.spark.sql.DataFrame \u003d [prev_id: int, curr_id: int, n: int, type: string]\n"
      },
      "dateCreated": "Apr 17, 2016 3:03:49 PM",
      "dateStarted": "Apr 17, 2016 3:04:03 PM",
      "dateFinished": "Apr 17, 2016 3:04:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.select(\"prev_id\", \"curr_id\", \"n\") // Works",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:04:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905443573_1224753258",
      "id": "20160417-150403_874820436",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res28: org.apache.spark.sql.DataFrame \u003d [prev_id: int, curr_id: int, n: int]\n"
      },
      "dateCreated": "Apr 17, 2016 3:04:03 PM",
      "dateStarted": "Apr 17, 2016 3:04:14 PM",
      "dateFinished": "Apr 17, 2016 3:04:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.select(\"prev_id\", \u0027curr_id) // Fails",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:04:24 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905454961_-1482340003",
      "id": "20160417-150414_33377310",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: overloaded method value select with alternatives:\n  (col: String,cols: String*)org.apache.spark.sql.DataFrame \u003cand\u003e\n  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame\n cannot be applied to (String, Symbol)\n              clickstreamDf.select(\"prev_id\", \u0027curr_id) // Fails\n                            ^\n"
      },
      "dateCreated": "Apr 17, 2016 3:04:14 PM",
      "dateStarted": "Apr 17, 2016 3:04:24 PM",
      "dateFinished": "Apr 17, 2016 3:04:24 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Projections: \"select\"\r\nThe \"select\" transformation lets you take some of the columns of a DataFrame. You can do a lot more with select, like renaming columns, adding new ones, etc..",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:04:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905464872_366009037",
      "id": "20160417-150424_1891338939",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eProjections: \u0026ldquo;select\u0026rdquo;\u003c/h3\u003e\n\u003cp\u003eThe \u0026ldquo;select\u0026rdquo; transformation lets you take some of the columns of a DataFrame. You can do a lot more with select, like renaming columns, adding new ones, etc..\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:04:24 PM",
      "dateStarted": "Apr 17, 2016 3:04:39 PM",
      "dateFinished": "Apr 17, 2016 3:04:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.when\r\nz.show(\r\n    clickstreamDf.select(\r\n        \u0027curr_id,\r\n        $\"curr_title\".as(\"title\"),\r\n        clickstreamDf(\"n\"),\r\n        when(\u0027n \u003e\u003d 100, \"Lots of visits\")\r\n        .when(\u0027n between(50, 99), \"Average visits\")\r\n        .when(\u0027n between(1, 49), \"Some visits\")\r\n        .otherwise(\"No visits at all!\")\r\n        .as(\"visits_rank\")\r\n    )\r\n)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:40:19 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905479272_1839627973",
      "id": "20160417-150439_1180486179",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "curr_id\ttitle\tn\tvisits_rank\n3632887\t!!\t121\tLots of visits\n3632887\t!!\t93\tAverage visits\n3632887\t!!\t46\tSome visits\n3632887\t!!\t10\tSome visits\n3632887\t!!\t11\tSome visits\n2556962\t!!!_(album)\t19\tSome visits\n2556962\t!!!_(album)\t25\tSome visits\n2556962\t!!!_(album)\t16\tSome visits\n2556962\t!!!_(album)\t44\tSome visits\n2556962\t!!!_(album)\t15\tSome visits\n2556962\t!!!_(album)\t297\tLots of visits\n6893310\t!Hero_(album)\t11\tSome visits\n6893310\t!Hero_(album)\t26\tSome visits\n6893310\t!Hero_(album)\t16\tSome visits\n6893310\t!Hero_(album)\t23\tSome visits\n22602473\t!Oka_Tokat\t16\tSome visits\n22602473\t!Oka_Tokat\t20\tSome visits\n22602473\t!Oka_Tokat\t57\tAverage visits\n22602473\t!Oka_Tokat\t12\tSome visits\n22602473\t!Oka_Tokat\t23\tSome visits\n22602473\t!Oka_Tokat\t10\tSome visits\n22602473\t!Oka_Tokat\t11\tSome visits\n22602473\t!Oka_Tokat\t22\tSome visits\n6810768\t!T.O.O.H.!\t20\tSome visits\n6810768\t!T.O.O.H.!\t81\tAverage visits\n6810768\t!T.O.O.H.!\t51\tAverage visits\n6810768\t!T.O.O.H.!\t35\tSome visits\n3243047\t!_(album)\t21\tSome visits\n3243047\t!_(album)\t208\tLots of visits\n3243047\t!_(album)\t78\tAverage visits\n3243047\t!_(album)\t28\tSome visits\n899480\t\"A\"_Device\t58\tAverage visits\n899480\t\"A\"_Device\t15\tSome visits\n899480\t\"A\"_Device\t17\tSome visits\n899480\t\"A\"_Device\t13\tSome visits\n899480\t\"A\"_Device\t29\tSome visits\n899480\t\"A\"_Device\t11\tSome visits\n899480\t\"A\"_Device\t24\tSome visits\n899480\t\"A\"_Device\t33\tSome visits\n899480\t\"A\"_Device\t47\tSome visits\n1282996\t\"A\"_Is_for_Alibi\t43\tSome visits\n1282996\t\"A\"_Is_for_Alibi\t45\tSome visits\n1282996\t\"A\"_Is_for_Alibi\t10\tSome visits\n1282996\t\"A\"_Is_for_Alibi\t207\tLots of visits\n1282996\t\"A\"_Is_for_Alibi\t18\tSome visits\n1282996\t\"A\"_Is_for_Alibi\t31\tSome visits\n1282996\t\"A\"_Is_for_Alibi\t272\tLots of visits\n1282996\t\"A\"_Is_for_Alibi\t10\tSome visits\n9003666\t\"And\"_theory_of_conservatism\t17\tSome visits\n6893310\t!Hero_(album)\t21\tSome visits\n6893310\t!Hero_(album)\t26\tSome visits\n2516600\t!Kung_language\t432\tLots of visits\n2516600\t!Kung_language\t197\tLots of visits\n2516600\t!Kung_language\t154\tLots of visits\n2516600\t!Kung_language\t74\tAverage visits\n2516600\t!Kung_language\t20\tSome visits\n2516600\t!Kung_language\t21\tSome visits\n2516600\t!Kung_language\t12\tSome visits\n2516600\t!Kung_language\t29\tSome visits\n2516600\t!Kung_language\t33\tSome visits\n2516600\t!Kung_language\t12\tSome visits\n2516600\t!Kung_language\t20\tSome visits\n2516600\t!Kung_language\t11\tSome visits\n2516600\t!Kung_language\t100\tLots of visits\n2516600\t!Kung_language\t21\tSome visits\n2516600\t!Kung_language\t45\tSome visits\n2516600\t!Kung_language\t56\tAverage visits\n29988427\t!Women_Art_Revolution\t300\tLots of visits\n29988427\t!Women_Art_Revolution\t93\tAverage visits\n29988427\t!Women_Art_Revolution\t24\tSome visits\n29988427\t!Women_Art_Revolution\t14\tSome visits\n29988427\t!Women_Art_Revolution\t23\tSome visits\n29988427\t!Women_Art_Revolution\t27\tSome visits\n64486\t!_(disambiguation)\t650\tLots of visits\n64486\t!_(disambiguation)\t226\tLots of visits\n64486\t!_(disambiguation)\t23\tSome visits\n64486\t!_(disambiguation)\t14\tSome visits\n64486\t!_(disambiguation)\t237\tLots of visits\n16250456\t\"B\"_Is_for_Burglar\t145\tLots of visits\n16250456\t\"B\"_Is_for_Burglar\t104\tLots of visits\n16250456\t\"B\"_Is_for_Burglar\t40\tSome visits\n16250456\t\"B\"_Is_for_Burglar\t17\tSome visits\n16250456\t\"B\"_Is_for_Burglar\t59\tAverage visits\n16250456\t\"B\"_Is_for_Burglar\t20\tSome visits\n1118809\t\"Crocodile\"_Dundee_II\t24\tSome visits\n1118809\t\"Crocodile\"_Dundee_II\t895\tLots of visits\n1118809\t\"Crocodile\"_Dundee_II\t399\tLots of visits\n1118809\t\"Crocodile\"_Dundee_II\t1867\tLots of visits\n1118809\t\"Crocodile\"_Dundee_II\t208\tLots of visits\n1118809\t\"Crocodile\"_Dundee_II\t27\tSome visits\n1118809\t\"Crocodile\"_Dundee_II\t11\tSome visits\n1118809\t\"Crocodile\"_Dundee_II\t11\tSome visits\n1118809\t\"Crocodile\"_Dundee_II\t203\tLots of visits\n1118809\t\"Crocodile\"_Dundee_II\t24\tSome visits\n1118809\t\"Crocodile\"_Dundee_II\t111\tLots of visits\n1118809\t\"Crocodile\"_Dundee_II\t15\tSome visits\n1118809\t\"Crocodile\"_Dundee_II\t27\tSome visits\n1118809\t\"Crocodile\"_Dundee_II\t38\tSome visits\n1118809\t\"Crocodile\"_Dundee_II\t335\tLots of visits\n9003666\t\"And\"_theory_of_conservatism\t109\tLots of visits\n9003666\t\"And\"_theory_of_conservatism\t18\tSome visits\n39072529\t\"Bassy\"_Bob_Brockmann\t49\tSome visits\n39072529\t\"Bassy\"_Bob_Brockmann\t10\tSome visits\nnull\t\"Bigfoot\"_Wallace\t15\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t12\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t24\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t15\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t42\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t15\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t25\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t73\tAverage visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t30\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t10\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t25\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t593\tLots of visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t36\tSome visits\n25033979\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t93\tAverage visits\nnull\t\"Chúc_Mừng_Năm_Mới\"_or_best_wishes_for_the_new_year.\t51\tAverage visits\nnull\t\"Cool_Hand_Conor\"_O\u0027Neill\t14\tSome visits\n331586\t\"Crocodile\"_Dundee\t6820\tLots of visits\n331586\t\"Crocodile\"_Dundee\t20\tSome visits\n331586\t\"Crocodile\"_Dundee\t781\tLots of visits\n331586\t\"Crocodile\"_Dundee\t59\tAverage visits\n331586\t\"Crocodile\"_Dundee\t38\tSome visits\n331586\t\"Crocodile\"_Dundee\t154\tLots of visits\n331586\t\"Crocodile\"_Dundee\t14\tSome visits\n331586\t\"Crocodile\"_Dundee\t13\tSome visits\n331586\t\"Crocodile\"_Dundee\t348\tLots of visits\n331586\t\"Crocodile\"_Dundee\t66\tAverage visits\n331586\t\"Crocodile\"_Dundee\t12\tSome visits\n331586\t\"Crocodile\"_Dundee\t297\tLots of visits\n331586\t\"Crocodile\"_Dundee\t52\tAverage visits\n331586\t\"Crocodile\"_Dundee\t31\tSome visits\n331586\t\"Crocodile\"_Dundee\t221\tLots of visits\n331586\t\"Crocodile\"_Dundee\t34\tSome visits\n331586\t\"Crocodile\"_Dundee\t60\tAverage visits\n331586\t\"Crocodile\"_Dundee\t10\tSome visits\n331586\t\"Crocodile\"_Dundee\t12\tSome visits\n331586\t\"Crocodile\"_Dundee\t85\tAverage visits\n331586\t\"Crocodile\"_Dundee\t13\tSome visits\n331586\t\"Crocodile\"_Dundee\t18\tSome visits\n331586\t\"Crocodile\"_Dundee\t153\tLots of visits\n331586\t\"Crocodile\"_Dundee\t18\tSome visits\n331586\t\"Crocodile\"_Dundee\t17\tSome visits\n331586\t\"Crocodile\"_Dundee\t21\tSome visits\n331586\t\"Crocodile\"_Dundee\t1283\tLots of visits\n331586\t\"Crocodile\"_Dundee\t30\tSome visits\n331586\t\"Crocodile\"_Dundee\t10\tSome visits\n331586\t\"Crocodile\"_Dundee\t13\tSome visits\n3632887\t!!\t415\tLots of visits\n3632887\t!!\t113\tLots of visits\n3632887\t!!\t33\tSome visits\n600744\t!!!\t25\tSome visits\n600744\t!!!\t1193\tLots of visits\n600744\t!!!\t1065\tLots of visits\n600744\t!!!\t44\tSome visits\n600744\t!!!\t11\tSome visits\n600744\t!!!\t21\tSome visits\n600744\t!!!\t14\tSome visits\n600744\t!!!\t70\tAverage visits\n600744\t!!!\t139\tLots of visits\n600744\t!!!\t31\tSome visits\n600744\t!!!\t40\tSome visits\n600744\t!!!\t95\tAverage visits\n600744\t!!!\t35\tSome visits\n600744\t!!!\t11\tSome visits\n600744\t!!!\t110\tLots of visits\n600744\t!!!\t14\tSome visits\n600744\t!!!\t136\tLots of visits\n600744\t!!!\t31\tSome visits\n600744\t!!!\t88\tAverage visits\n600744\t!!!\t21\tSome visits\n600744\t!!!\t14\tSome visits\n600744\t!!!\t33\tSome visits\n600744\t!!!\t11\tSome visits\n600744\t!!!\t105\tLots of visits\n600744\t!!!\t14\tSome visits\n600744\t!!!\t210\tLots of visits\n600744\t!!!\t17\tSome visits\n600744\t!!!\t11\tSome visits\n600744\t!!!\t22\tSome visits\n600744\t!!!\t17\tSome visits\n600744\t!!!\t20\tSome visits\n600744\t!!!\t20\tSome visits\n600744\t!!!\t14\tSome visits\n600744\t!!!\t33\tSome visits\n600744\t!!!\t15\tSome visits\n600744\t!!!\t105\tLots of visits\n600744\t!!!\t105\tLots of visits\n600744\t!!!\t53\tAverage visits\n600744\t!!!\t63\tAverage visits\n600744\t!!!\t29\tSome visits\n600744\t!!!\t56\tAverage visits\n2556962\t!!!_(album)\t332\tLots of visits\n2556962\t!!!_(album)\t25\tSome visits\n2556962\t!!!_(album)\t328\tLots of visits\n2556962\t!!!_(album)\t18\tSome visits\n2556962\t!!!_(album)\t25\tSome visits\n6893310\t!Hero_(album)\t200\tLots of visits\n"
      },
      "dateCreated": "Apr 17, 2016 3:04:39 PM",
      "dateStarted": "Apr 30, 2016 1:08:48 PM",
      "dateFinished": "Apr 30, 2016 1:08:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Projections: \"selectExpr\"\r\nUsing selectExpr has the advantage that we can generate the new columns by using SQL expressions, which is simpler in some cases.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:05:27 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905506989_-1279131287",
      "id": "20160417-150506_838780663",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eProjections: \u0026ldquo;selectExpr\u0026rdquo;\u003c/h3\u003e\n\u003cp\u003eUsing selectExpr has the advantage that we can generate the new columns by using SQL expressions, which is simpler in some cases.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:05:06 PM",
      "dateStarted": "Apr 17, 2016 3:05:25 PM",
      "dateFinished": "Apr 17, 2016 3:05:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "z.show(\r\n    clickstreamDf.selectExpr(\r\n        \"curr_id\",\r\n        \"curr_id IS NOT NULL AS hasCurrentId\",\r\n        \"curr_title AS title\",\r\n        \"trim(regexp_replace(curr_title, \u0027[^a-zA-Z0-9\\\\s]\u0027, \u0027 \u0027)) AS sanitized_title\",\r\n        \"n * 3 AS expected_visits\"\r\n    )\r\n)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:40:19 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905525309_-1366607205",
      "id": "20160417-150525_1537752983",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "curr_id\thasCurrentId\ttitle\tsanitized_title\texpected_visits\n3632887\ttrue\t!!\t\t363\n3632887\ttrue\t!!\t\t279\n3632887\ttrue\t!!\t\t138\n3632887\ttrue\t!!\t\t30\n3632887\ttrue\t!!\t\t33\n2556962\ttrue\t!!!_(album)\talbum\t57\n2556962\ttrue\t!!!_(album)\talbum\t75\n2556962\ttrue\t!!!_(album)\talbum\t48\n2556962\ttrue\t!!!_(album)\talbum\t132\n2556962\ttrue\t!!!_(album)\talbum\t45\n2556962\ttrue\t!!!_(album)\talbum\t891\n6893310\ttrue\t!Hero_(album)\tHero  album\t33\n6893310\ttrue\t!Hero_(album)\tHero  album\t78\n6893310\ttrue\t!Hero_(album)\tHero  album\t48\n6893310\ttrue\t!Hero_(album)\tHero  album\t69\n22602473\ttrue\t!Oka_Tokat\tOka Tokat\t48\n22602473\ttrue\t!Oka_Tokat\tOka Tokat\t60\n22602473\ttrue\t!Oka_Tokat\tOka Tokat\t171\n22602473\ttrue\t!Oka_Tokat\tOka Tokat\t36\n22602473\ttrue\t!Oka_Tokat\tOka Tokat\t69\n22602473\ttrue\t!Oka_Tokat\tOka Tokat\t30\n22602473\ttrue\t!Oka_Tokat\tOka Tokat\t33\n22602473\ttrue\t!Oka_Tokat\tOka Tokat\t66\n6810768\ttrue\t!T.O.O.H.!\tT O O H\t60\n6810768\ttrue\t!T.O.O.H.!\tT O O H\t243\n6810768\ttrue\t!T.O.O.H.!\tT O O H\t153\n6810768\ttrue\t!T.O.O.H.!\tT O O H\t105\n3243047\ttrue\t!_(album)\talbum\t63\n3243047\ttrue\t!_(album)\talbum\t624\n3243047\ttrue\t!_(album)\talbum\t234\n3243047\ttrue\t!_(album)\talbum\t84\n899480\ttrue\t\"A\"_Device\tA  Device\t174\n899480\ttrue\t\"A\"_Device\tA  Device\t45\n899480\ttrue\t\"A\"_Device\tA  Device\t51\n899480\ttrue\t\"A\"_Device\tA  Device\t39\n899480\ttrue\t\"A\"_Device\tA  Device\t87\n899480\ttrue\t\"A\"_Device\tA  Device\t33\n899480\ttrue\t\"A\"_Device\tA  Device\t72\n899480\ttrue\t\"A\"_Device\tA  Device\t99\n899480\ttrue\t\"A\"_Device\tA  Device\t141\n1282996\ttrue\t\"A\"_Is_for_Alibi\tA  Is for Alibi\t129\n1282996\ttrue\t\"A\"_Is_for_Alibi\tA  Is for Alibi\t135\n1282996\ttrue\t\"A\"_Is_for_Alibi\tA  Is for Alibi\t30\n1282996\ttrue\t\"A\"_Is_for_Alibi\tA  Is for Alibi\t621\n1282996\ttrue\t\"A\"_Is_for_Alibi\tA  Is for Alibi\t54\n1282996\ttrue\t\"A\"_Is_for_Alibi\tA  Is for Alibi\t93\n1282996\ttrue\t\"A\"_Is_for_Alibi\tA  Is for Alibi\t816\n1282996\ttrue\t\"A\"_Is_for_Alibi\tA  Is for Alibi\t30\n9003666\ttrue\t\"And\"_theory_of_conservatism\tAnd  theory of conservatism\t51\n6893310\ttrue\t!Hero_(album)\tHero  album\t63\n6893310\ttrue\t!Hero_(album)\tHero  album\t78\n2516600\ttrue\t!Kung_language\tKung language\t1296\n2516600\ttrue\t!Kung_language\tKung language\t591\n2516600\ttrue\t!Kung_language\tKung language\t462\n2516600\ttrue\t!Kung_language\tKung language\t222\n2516600\ttrue\t!Kung_language\tKung language\t60\n2516600\ttrue\t!Kung_language\tKung language\t63\n2516600\ttrue\t!Kung_language\tKung language\t36\n2516600\ttrue\t!Kung_language\tKung language\t87\n2516600\ttrue\t!Kung_language\tKung language\t99\n2516600\ttrue\t!Kung_language\tKung language\t36\n2516600\ttrue\t!Kung_language\tKung language\t60\n2516600\ttrue\t!Kung_language\tKung language\t33\n2516600\ttrue\t!Kung_language\tKung language\t300\n2516600\ttrue\t!Kung_language\tKung language\t63\n2516600\ttrue\t!Kung_language\tKung language\t135\n2516600\ttrue\t!Kung_language\tKung language\t168\n29988427\ttrue\t!Women_Art_Revolution\tWomen Art Revolution\t900\n29988427\ttrue\t!Women_Art_Revolution\tWomen Art Revolution\t279\n29988427\ttrue\t!Women_Art_Revolution\tWomen Art Revolution\t72\n29988427\ttrue\t!Women_Art_Revolution\tWomen Art Revolution\t42\n29988427\ttrue\t!Women_Art_Revolution\tWomen Art Revolution\t69\n29988427\ttrue\t!Women_Art_Revolution\tWomen Art Revolution\t81\n64486\ttrue\t!_(disambiguation)\tdisambiguation\t1950\n64486\ttrue\t!_(disambiguation)\tdisambiguation\t678\n64486\ttrue\t!_(disambiguation)\tdisambiguation\t69\n64486\ttrue\t!_(disambiguation)\tdisambiguation\t42\n64486\ttrue\t!_(disambiguation)\tdisambiguation\t711\n16250456\ttrue\t\"B\"_Is_for_Burglar\tB  Is for Burglar\t435\n16250456\ttrue\t\"B\"_Is_for_Burglar\tB  Is for Burglar\t312\n16250456\ttrue\t\"B\"_Is_for_Burglar\tB  Is for Burglar\t120\n16250456\ttrue\t\"B\"_Is_for_Burglar\tB  Is for Burglar\t51\n16250456\ttrue\t\"B\"_Is_for_Burglar\tB  Is for Burglar\t177\n16250456\ttrue\t\"B\"_Is_for_Burglar\tB  Is for Burglar\t60\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t72\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t2685\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t1197\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t5601\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t624\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t81\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t33\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t33\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t609\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t72\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t333\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t45\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t81\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t114\n1118809\ttrue\t\"Crocodile\"_Dundee_II\tCrocodile  Dundee II\t1005\n9003666\ttrue\t\"And\"_theory_of_conservatism\tAnd  theory of conservatism\t327\n9003666\ttrue\t\"And\"_theory_of_conservatism\tAnd  theory of conservatism\t54\n39072529\ttrue\t\"Bassy\"_Bob_Brockmann\tBassy  Bob Brockmann\t147\n39072529\ttrue\t\"Bassy\"_Bob_Brockmann\tBassy  Bob Brockmann\t30\nnull\tfalse\t\"Bigfoot\"_Wallace\tBigfoot  Wallace\t45\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t36\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t72\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t45\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t126\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t45\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t75\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t219\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t90\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t30\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t75\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t1779\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t108\n25033979\ttrue\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tC  is for  Please Insert Sophomoric Genitalia Reference HERE\t279\nnull\tfalse\t\"Chúc_Mừng_Năm_Mới\"_or_best_wishes_for_the_new_year.\tCh c M ng N m M i  or best wishes for the new year\t153\nnull\tfalse\t\"Cool_Hand_Conor\"_O\u0027Neill\tCool Hand Conor  O Neill\t42\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t20460\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t60\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t2343\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t177\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t114\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t462\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t42\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t39\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t1044\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t198\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t36\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t891\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t156\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t93\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t663\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t102\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t180\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t30\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t36\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t255\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t39\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t54\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t459\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t54\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t51\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t63\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t3849\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t90\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t30\n331586\ttrue\t\"Crocodile\"_Dundee\tCrocodile  Dundee\t39\n3632887\ttrue\t!!\t\t1245\n3632887\ttrue\t!!\t\t339\n3632887\ttrue\t!!\t\t99\n600744\ttrue\t!!!\t\t75\n600744\ttrue\t!!!\t\t3579\n600744\ttrue\t!!!\t\t3195\n600744\ttrue\t!!!\t\t132\n600744\ttrue\t!!!\t\t33\n600744\ttrue\t!!!\t\t63\n600744\ttrue\t!!!\t\t42\n600744\ttrue\t!!!\t\t210\n600744\ttrue\t!!!\t\t417\n600744\ttrue\t!!!\t\t93\n600744\ttrue\t!!!\t\t120\n600744\ttrue\t!!!\t\t285\n600744\ttrue\t!!!\t\t105\n600744\ttrue\t!!!\t\t33\n600744\ttrue\t!!!\t\t330\n600744\ttrue\t!!!\t\t42\n600744\ttrue\t!!!\t\t408\n600744\ttrue\t!!!\t\t93\n600744\ttrue\t!!!\t\t264\n600744\ttrue\t!!!\t\t63\n600744\ttrue\t!!!\t\t42\n600744\ttrue\t!!!\t\t99\n600744\ttrue\t!!!\t\t33\n600744\ttrue\t!!!\t\t315\n600744\ttrue\t!!!\t\t42\n600744\ttrue\t!!!\t\t630\n600744\ttrue\t!!!\t\t51\n600744\ttrue\t!!!\t\t33\n600744\ttrue\t!!!\t\t66\n600744\ttrue\t!!!\t\t51\n600744\ttrue\t!!!\t\t60\n600744\ttrue\t!!!\t\t60\n600744\ttrue\t!!!\t\t42\n600744\ttrue\t!!!\t\t99\n600744\ttrue\t!!!\t\t45\n600744\ttrue\t!!!\t\t315\n600744\ttrue\t!!!\t\t315\n600744\ttrue\t!!!\t\t159\n600744\ttrue\t!!!\t\t189\n600744\ttrue\t!!!\t\t87\n600744\ttrue\t!!!\t\t168\n2556962\ttrue\t!!!_(album)\talbum\t996\n2556962\ttrue\t!!!_(album)\talbum\t75\n2556962\ttrue\t!!!_(album)\talbum\t984\n2556962\ttrue\t!!!_(album)\talbum\t54\n2556962\ttrue\t!!!_(album)\talbum\t75\n6893310\ttrue\t!Hero_(album)\tHero  album\t600\n"
      },
      "dateCreated": "Apr 17, 2016 3:05:25 PM",
      "dateStarted": "Apr 30, 2016 1:08:18 PM",
      "dateFinished": "Apr 30, 2016 1:08:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Adding a new column: withColumn\r\nIf you just want to append a new column to the DataFrame, you can do it using the \"withColumn\" method. This method takes two arguments: the name of the column, and the expression to calculate its value.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:05:59 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905539773_-2105325093",
      "id": "20160417-150539_843681659",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eAdding a new column: withColumn\u003c/h3\u003e\n\u003cp\u003eIf you just want to append a new column to the DataFrame, you can do it using the \u0026ldquo;withColumn\u0026rdquo; method. This method takes two arguments: the name of the column, and the expression to calculate its value.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:05:39 PM",
      "dateStarted": "Apr 17, 2016 3:05:56 PM",
      "dateFinished": "Apr 17, 2016 3:05:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.withColumn(\"expected_visits\", \u0027n * 3).show(5)\r\nclickstreamDf.withColumn(\"lit_column\", lit(123L)).show(5)\r\nclickstreamDf.withColumn(\"type_and_title\", concat_ws(\" -\u003e \", \u0027type, \u0027curr_title)).show(5)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 6:57:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905556933_66997845",
      "id": "20160417-150556_1584891073",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------+--------+---+--------------------+--------------------+-------+---------------+\n| prev_id| curr_id|  n|          prev_title|          curr_title|   type|expected_visits|\n+--------+--------+---+--------------------+--------------------+-------+---------------+\n|    null| 9003666|109|     other-wikipedia|\"And\"_theory_of_c...|  other|            327|\n|    null| 9003666| 18|        other-google|\"And\"_theory_of_c...|  other|             54|\n|    null|39072529| 49|        other-google|\"Bassy\"_Bob_Brock...|  other|            147|\n|    null|39072529| 10|         other-other|\"Bassy\"_Bob_Brock...|  other|             30|\n|11273993|    null| 15|Colt_1851_Navy_Re...|   \"Bigfoot\"_Wallace|redlink|             45|\n+--------+--------+---+--------------------+--------------------+-------+---------------+\nonly showing top 5 rows\n\n+--------+--------+---+--------------------+--------------------+-------+----------+\n| prev_id| curr_id|  n|          prev_title|          curr_title|   type|lit_column|\n+--------+--------+---+--------------------+--------------------+-------+----------+\n|    null| 9003666|109|     other-wikipedia|\"And\"_theory_of_c...|  other|       123|\n|    null| 9003666| 18|        other-google|\"And\"_theory_of_c...|  other|       123|\n|    null|39072529| 49|        other-google|\"Bassy\"_Bob_Brock...|  other|       123|\n|    null|39072529| 10|         other-other|\"Bassy\"_Bob_Brock...|  other|       123|\n|11273993|    null| 15|Colt_1851_Navy_Re...|   \"Bigfoot\"_Wallace|redlink|       123|\n+--------+--------+---+--------------------+--------------------+-------+----------+\nonly showing top 5 rows\n\n+--------+--------+---+--------------------+--------------------+-------+--------------------+\n| prev_id| curr_id|  n|          prev_title|          curr_title|   type|      type_and_title|\n+--------+--------+---+--------------------+--------------------+-------+--------------------+\n|    null| 9003666|109|     other-wikipedia|\"And\"_theory_of_c...|  other|other -\u003e \"And\"_th...|\n|    null| 9003666| 18|        other-google|\"And\"_theory_of_c...|  other|other -\u003e \"And\"_th...|\n|    null|39072529| 49|        other-google|\"Bassy\"_Bob_Brock...|  other|other -\u003e \"Bassy\"_...|\n|    null|39072529| 10|         other-other|\"Bassy\"_Bob_Brock...|  other|other -\u003e \"Bassy\"_...|\n|11273993|    null| 15|Colt_1851_Navy_Re...|   \"Bigfoot\"_Wallace|redlink|redlink -\u003e \"Bigfo...|\n+--------+--------+---+--------------------+--------------------+-------+--------------------+\nonly showing top 5 rows\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:05:56 PM",
      "dateStarted": "Apr 17, 2016 6:57:14 PM",
      "dateFinished": "Apr 17, 2016 6:57:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Filtering data: filter\nWhen you want to select just the part of your DataFrame that matches certain condition(s), you can use **filter** or **where** (both are equivalent).",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 6:57:19 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460919320260_997312684",
      "id": "20160417-185520_395756971",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eFiltering data: filter\u003c/h3\u003e\n\u003cp\u003eWhen you want to select just the part of your DataFrame that matches certain condition(s), you can use \u003cstrong\u003efilter\u003c/strong\u003e or \u003cstrong\u003ewhere\u003c/strong\u003e (both are equivalent).\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 6:55:20 PM",
      "dateStarted": "Apr 17, 2016 6:57:14 PM",
      "dateFinished": "Apr 17, 2016 6:57:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// What articles of type \"link\" got more than 150 visits?\r\nz.show(clickstreamDf.filter(\u0027type \u003d\u003d\u003d \"link\").filter(\"n \u003e 150\"))",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:07:51 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905576360_-360211351",
      "id": "20160417-150616_1233825672",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "prev_id\tcurr_id\tn\tprev_title\tcurr_title\ttype\n600744\t2556962\t297\t!!!\t!!!_(album)\tlink\n1337475\t3243047\t208\tThe_Dismemberment_Plan\t!_(album)\tlink\n470006\t1282996\t207\tSue_Grafton\t\"A\"_Is_for_Alibi\tlink\n4619790\t25033979\t593\tPuscifer\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\tlink\n2564144\t331586\t154\tCrocodile_Dundee_in_Los_Angeles\t\"Crocodile\"_Dundee\tlink\n8078282\t331586\t348\tAustralia_(Some(2008)_film)\t\"Crocodile\"_Dundee\tlink\n1118809\t331586\t297\t\"Crocodile\"_Dundee_II\t\"Crocodile\"_Dundee\tlink\n171612\t331586\t221\t1986_in_film\t\"Crocodile\"_Dundee\tlink\n70209\t331586\t153\tCinema_of_Australia\t\"Crocodile\"_Dundee\tlink\n44789934\t331586\t1283\tDeaths_in_2015\t\"Crocodile\"_Dundee\tlink\n"
      },
      "dateCreated": "Apr 17, 2016 3:06:16 PM",
      "dateStarted": "Apr 30, 2016 1:07:51 PM",
      "dateFinished": "Apr 30, 2016 1:07:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Warning: Filter and null values\r\nYou should be careful about null values when filterering. As happens with other SQL languages, most operations involving NULL values will return NULL. Let\u0027s see some examples. For instance, we\u0027ll try to to select this row filtering on prev_id:\r\n```\r\n+-------+-------+---+-----------+----------+-----+\r\n|prev_id|curr_id|  n| prev_title|curr_title| type|\r\n+-------+-------+---+-----------+----------+-----+\r\n|   null|3632887| 46|other-empty|        !!|other|\r\n+-------+-------+---+-----------+----------+-----+\r\n```",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 6:58:00 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905595602_-499859878",
      "id": "20160417-150635_149117620",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eWarning: Filter and null values\u003c/h3\u003e\n\u003cp\u003eYou should be careful about null values when filterering. As happens with other SQL languages, most operations involving NULL values will return NULL. Let\u0027s see some examples. For instance, we\u0027ll try to to select this row filtering on prev_id:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e+-------+-------+---+-----------+----------+-----+\n|prev_id|curr_id|  n| prev_title|curr_title| type|\n+-------+-------+---+-----------+----------+-----+\n|   null|3632887| 46|other-empty|        !!|other|\n+-------+-------+---+-----------+----------+-----+\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:06:35 PM",
      "dateStarted": "Apr 17, 2016 3:06:48 PM",
      "dateFinished": "Apr 17, 2016 3:06:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Without filtering on prev_id - Works\r\nclickstreamDf.filter(\"curr_id \u003d 3632887 AND n \u003d 46 AND prev_title \u003d \u0027other-empty\u0027 AND curr_title \u003d \u0027!!\u0027 AND type \u003d \u0027other\u0027\").show(5)\r\n// Filtering on prev_id - Doesn\u0027t work\r\nclickstreamDf.filter(\"curr_id \u003d 3632887 AND n \u003d 46 AND prev_title \u003d \u0027other-empty\u0027 AND curr_title \u003d \u0027!!\u0027 AND type \u003d \u0027other\u0027 AND prev_id \u003c\u003e 600744\").show(5)\r\n// NULL \u003c\u003e 600744 is NULL, which evaluates to false, that\u0027s why the condition is not met... We can add an extra check to make it work\r\nclickstreamDf.filter(\"curr_id \u003d 3632887 AND n \u003d 46 AND prev_title \u003d \u0027other-empty\u0027 AND curr_title \u003d \u0027!!\u0027 AND type \u003d \u0027other\u0027 AND (prev_id IS NULL OR prev_id \u003c\u003e 600744)\").show(5)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:07:19 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905608141_296985743",
      "id": "20160417-150648_2022589529",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------+-------+---+-----------+----------+-----+\n|prev_id|curr_id|  n| prev_title|curr_title| type|\n+-------+-------+---+-----------+----------+-----+\n|   null|3632887| 46|other-empty|        !!|other|\n+-------+-------+---+-----------+----------+-----+\n\n+-------+-------+---+----------+----------+----+\n|prev_id|curr_id|  n|prev_title|curr_title|type|\n+-------+-------+---+----------+----------+----+\n+-------+-------+---+----------+----------+----+\n\n+-------+-------+---+-----------+----------+-----+\n|prev_id|curr_id|  n| prev_title|curr_title| type|\n+-------+-------+---+-----------+----------+-----+\n|   null|3632887| 46|other-empty|        !!|other|\n+-------+-------+---+-----------+----------+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:06:48 PM",
      "dateStarted": "Apr 17, 2016 3:07:19 PM",
      "dateFinished": "Apr 17, 2016 3:07:21 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Using UDFs (User-Defined functions)\r\nYou can use your own functions to generate new columns, filter, etc. You just need to wrap them inside a \"udf\" block.\r\n\r\n**Note**: At the time of this writing, UDFs are limited to a maximum of 22 variables (cf. \u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.UDFRegistration\u003e). ",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:07:41 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905628380_-227796433",
      "id": "20160417-150708_2140196608",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eUsing UDFs (User-Defined functions)\u003c/h3\u003e\n\u003cp\u003eYou can use your own functions to generate new columns, filter, etc. You just need to wrap them inside a \u0026ldquo;udf\u0026rdquo; block.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: At the time of this writing, UDFs are limited to a maximum of 22 variables (cf. \u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.UDFRegistration\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.UDFRegistration\u003c/a\u003e).\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:07:08 PM",
      "dateStarted": "Apr 17, 2016 3:07:38 PM",
      "dateFinished": "Apr 17, 2016 3:07:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val myFunction \u003d (numViews: Int, articleType: String, title: String) \u003d\u003e s\"[${articleType}][${numViews} views]: ${title}\"\r\nval myUdf \u003d udf { myFunction }\r\nz.show(clickstreamDf.select(\u0027n, \u0027type, \u0027curr_title, myUdf(\u0027n, \u0027type, \u0027curr_title).as(\"row_str\")))",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:10:04 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905658048_1263367710",
      "id": "20160417-150738_339236988",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "n\ttype\tcurr_title\trow_str\n121\tother\t!!\t[other][121 views]: !!\n93\tother\t!!\t[other][93 views]: !!\n46\tother\t!!\t[other][46 views]: !!\n10\tother\t!!\t[other][10 views]: !!\n11\tother\t!!\t[other][11 views]: !!\n19\tlink\t!!!_(album)\t[link][19 views]: !!!_(album)\n25\tother\t!!!_(album)\t[other][25 views]: !!!_(album)\n16\tother\t!!!_(album)\t[other][16 views]: !!!_(album)\n44\tother\t!!!_(album)\t[other][44 views]: !!!_(album)\n15\tlink\t!!!_(album)\t[link][15 views]: !!!_(album)\n297\tlink\t!!!_(album)\t[link][297 views]: !!!_(album)\n11\tother\t!Hero_(album)\t[other][11 views]: !Hero_(album)\n26\tlink\t!Hero_(album)\t[link][26 views]: !Hero_(album)\n16\tother\t!Hero_(album)\t[other][16 views]: !Hero_(album)\n23\tother\t!Hero_(album)\t[other][23 views]: !Hero_(album)\n16\tlink\t!Oka_Tokat\t[link][16 views]: !Oka_Tokat\n20\tlink\t!Oka_Tokat\t[link][20 views]: !Oka_Tokat\n57\tother\t!Oka_Tokat\t[other][57 views]: !Oka_Tokat\n12\tother\t!Oka_Tokat\t[other][12 views]: !Oka_Tokat\n23\tother\t!Oka_Tokat\t[other][23 views]: !Oka_Tokat\n10\tlink\t!Oka_Tokat\t[link][10 views]: !Oka_Tokat\n11\tlink\t!Oka_Tokat\t[link][11 views]: !Oka_Tokat\n22\tlink\t!Oka_Tokat\t[link][22 views]: !Oka_Tokat\n20\tother\t!T.O.O.H.!\t[other][20 views]: !T.O.O.H.!\n81\tother\t!T.O.O.H.!\t[other][81 views]: !T.O.O.H.!\n51\tlink\t!T.O.O.H.!\t[link][51 views]: !T.O.O.H.!\n35\tother\t!T.O.O.H.!\t[other][35 views]: !T.O.O.H.!\n21\tother\t!_(album)\t[other][21 views]: !_(album)\n208\tlink\t!_(album)\t[link][208 views]: !_(album)\n78\tlink\t!_(album)\t[link][78 views]: !_(album)\n28\tother\t!_(album)\t[other][28 views]: !_(album)\n58\tlink\t\"A\"_Device\t[link][58 views]: \"A\"_Device\n15\tlink\t\"A\"_Device\t[link][15 views]: \"A\"_Device\n17\tother\t\"A\"_Device\t[other][17 views]: \"A\"_Device\n13\tother\t\"A\"_Device\t[other][13 views]: \"A\"_Device\n29\tother\t\"A\"_Device\t[other][29 views]: \"A\"_Device\n11\tlink\t\"A\"_Device\t[link][11 views]: \"A\"_Device\n24\tother\t\"A\"_Device\t[other][24 views]: \"A\"_Device\n33\tlink\t\"A\"_Device\t[link][33 views]: \"A\"_Device\n47\tlink\t\"A\"_Device\t[link][47 views]: \"A\"_Device\n43\tlink\t\"A\"_Is_for_Alibi\t[link][43 views]: \"A\"_Is_for_Alibi\n45\tother\t\"A\"_Is_for_Alibi\t[other][45 views]: \"A\"_Is_for_Alibi\n10\tother\t\"A\"_Is_for_Alibi\t[other][10 views]: \"A\"_Is_for_Alibi\n207\tlink\t\"A\"_Is_for_Alibi\t[link][207 views]: \"A\"_Is_for_Alibi\n18\tother\t\"A\"_Is_for_Alibi\t[other][18 views]: \"A\"_Is_for_Alibi\n31\tother\t\"A\"_Is_for_Alibi\t[other][31 views]: \"A\"_Is_for_Alibi\n272\tother\t\"A\"_Is_for_Alibi\t[other][272 views]: \"A\"_Is_for_Alibi\n10\tlink\t\"A\"_Is_for_Alibi\t[link][10 views]: \"A\"_Is_for_Alibi\n17\tlink\t\"And\"_theory_of_conservatism\t[link][17 views]: \"And\"_theory_of_conservatism\n21\tnull\t!Hero_(album)\t[null][21 views]: !Hero_(album)\n26\tnull\t!Hero_(album)\t[null][26 views]: !Hero_(album)\n432\tnull\t!Kung_language\t[null][432 views]: !Kung_language\n197\tnull\t!Kung_language\t[null][197 views]: !Kung_language\n154\tnull\t!Kung_language\t[null][154 views]: !Kung_language\n74\tnull\t!Kung_language\t[null][74 views]: !Kung_language\n20\tnull\t!Kung_language\t[null][20 views]: !Kung_language\n21\tnull\t!Kung_language\t[null][21 views]: !Kung_language\n12\tnull\t!Kung_language\t[null][12 views]: !Kung_language\n29\tnull\t!Kung_language\t[null][29 views]: !Kung_language\n33\tnull\t!Kung_language\t[null][33 views]: !Kung_language\n12\tnull\t!Kung_language\t[null][12 views]: !Kung_language\n20\tnull\t!Kung_language\t[null][20 views]: !Kung_language\n11\tnull\t!Kung_language\t[null][11 views]: !Kung_language\n100\tnull\t!Kung_language\t[null][100 views]: !Kung_language\n21\tnull\t!Kung_language\t[null][21 views]: !Kung_language\n45\tnull\t!Kung_language\t[null][45 views]: !Kung_language\n56\tnull\t!Kung_language\t[null][56 views]: !Kung_language\n300\tnull\t!Women_Art_Revolution\t[null][300 views]: !Women_Art_Revolution\n93\tnull\t!Women_Art_Revolution\t[null][93 views]: !Women_Art_Revolution\n24\tnull\t!Women_Art_Revolution\t[null][24 views]: !Women_Art_Revolution\n14\tnull\t!Women_Art_Revolution\t[null][14 views]: !Women_Art_Revolution\n23\tnull\t!Women_Art_Revolution\t[null][23 views]: !Women_Art_Revolution\n27\tnull\t!Women_Art_Revolution\t[null][27 views]: !Women_Art_Revolution\n650\tnull\t!_(disambiguation)\t[null][650 views]: !_(disambiguation)\n226\tnull\t!_(disambiguation)\t[null][226 views]: !_(disambiguation)\n23\tnull\t!_(disambiguation)\t[null][23 views]: !_(disambiguation)\n14\tnull\t!_(disambiguation)\t[null][14 views]: !_(disambiguation)\n237\tnull\t!_(disambiguation)\t[null][237 views]: !_(disambiguation)\n145\tnull\t\"B\"_Is_for_Burglar\t[null][145 views]: \"B\"_Is_for_Burglar\n104\tnull\t\"B\"_Is_for_Burglar\t[null][104 views]: \"B\"_Is_for_Burglar\n40\tnull\t\"B\"_Is_for_Burglar\t[null][40 views]: \"B\"_Is_for_Burglar\n17\tnull\t\"B\"_Is_for_Burglar\t[null][17 views]: \"B\"_Is_for_Burglar\n59\tnull\t\"B\"_Is_for_Burglar\t[null][59 views]: \"B\"_Is_for_Burglar\n20\tnull\t\"B\"_Is_for_Burglar\t[null][20 views]: \"B\"_Is_for_Burglar\n24\tnull\t\"Crocodile\"_Dundee_II\t[null][24 views]: \"Crocodile\"_Dundee_II\n895\tnull\t\"Crocodile\"_Dundee_II\t[null][895 views]: \"Crocodile\"_Dundee_II\n399\tnull\t\"Crocodile\"_Dundee_II\t[null][399 views]: \"Crocodile\"_Dundee_II\n1867\tnull\t\"Crocodile\"_Dundee_II\t[null][1867 views]: \"Crocodile\"_Dundee_II\n208\tnull\t\"Crocodile\"_Dundee_II\t[null][208 views]: \"Crocodile\"_Dundee_II\n27\tnull\t\"Crocodile\"_Dundee_II\t[null][27 views]: \"Crocodile\"_Dundee_II\n11\tnull\t\"Crocodile\"_Dundee_II\t[null][11 views]: \"Crocodile\"_Dundee_II\n11\tnull\t\"Crocodile\"_Dundee_II\t[null][11 views]: \"Crocodile\"_Dundee_II\n203\tnull\t\"Crocodile\"_Dundee_II\t[null][203 views]: \"Crocodile\"_Dundee_II\n24\tnull\t\"Crocodile\"_Dundee_II\t[null][24 views]: \"Crocodile\"_Dundee_II\n111\tnull\t\"Crocodile\"_Dundee_II\t[null][111 views]: \"Crocodile\"_Dundee_II\n15\tnull\t\"Crocodile\"_Dundee_II\t[null][15 views]: \"Crocodile\"_Dundee_II\n27\tnull\t\"Crocodile\"_Dundee_II\t[null][27 views]: \"Crocodile\"_Dundee_II\n38\tnull\t\"Crocodile\"_Dundee_II\t[null][38 views]: \"Crocodile\"_Dundee_II\n335\tnull\t\"Crocodile\"_Dundee_II\t[null][335 views]: \"Crocodile\"_Dundee_II\n109\tother\t\"And\"_theory_of_conservatism\t[other][109 views]: \"And\"_theory_of_conservatism\n18\tother\t\"And\"_theory_of_conservatism\t[other][18 views]: \"And\"_theory_of_conservatism\n49\tother\t\"Bassy\"_Bob_Brockmann\t[other][49 views]: \"Bassy\"_Bob_Brockmann\n10\tother\t\"Bassy\"_Bob_Brockmann\t[other][10 views]: \"Bassy\"_Bob_Brockmann\n15\tredlink\t\"Bigfoot\"_Wallace\t[redlink][15 views]: \"Bigfoot\"_Wallace\n12\tlink\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[link][12 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n24\tlink\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[link][24 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n15\tother\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[other][15 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n42\tother\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[other][42 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n15\tother\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[other][15 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n25\tother\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[other][25 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n73\tlink\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[link][73 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n30\tother\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[other][30 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n10\tlink\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[link][10 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n25\tother\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[other][25 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n593\tlink\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[link][593 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n36\tother\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[other][36 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n93\tother\t\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t[other][93 views]: \"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\n51\tredlink\t\"Chúc_Mừng_Năm_Mới\"_or_best_wishes_for_the_new_year.\t[redlink][51 views]: \"Chúc_Mừng_Năm_Mới\"_or_best_wishes_for_the_new_year.\n14\tredlink\t\"Cool_Hand_Conor\"_O\u0027Neill\t[redlink][14 views]: \"Cool_Hand_Conor\"_O\u0027Neill\n6820\tother\t\"Crocodile\"_Dundee\t[other][6820 views]: \"Crocodile\"_Dundee\n20\tother\t\"Crocodile\"_Dundee\t[other][20 views]: \"Crocodile\"_Dundee\n781\tother\t\"Crocodile\"_Dundee\t[other][781 views]: \"Crocodile\"_Dundee\n59\tlink\t\"Crocodile\"_Dundee\t[link][59 views]: \"Crocodile\"_Dundee\n38\tother\t\"Crocodile\"_Dundee\t[other][38 views]: \"Crocodile\"_Dundee\n154\tlink\t\"Crocodile\"_Dundee\t[link][154 views]: \"Crocodile\"_Dundee\n14\tother\t\"Crocodile\"_Dundee\t[other][14 views]: \"Crocodile\"_Dundee\n13\tlink\t\"Crocodile\"_Dundee\t[link][13 views]: \"Crocodile\"_Dundee\n348\tlink\t\"Crocodile\"_Dundee\t[link][348 views]: \"Crocodile\"_Dundee\n66\tlink\t\"Crocodile\"_Dundee\t[link][66 views]: \"Crocodile\"_Dundee\n12\tother\t\"Crocodile\"_Dundee\t[other][12 views]: \"Crocodile\"_Dundee\n297\tlink\t\"Crocodile\"_Dundee\t[link][297 views]: \"Crocodile\"_Dundee\n52\tother\t\"Crocodile\"_Dundee\t[other][52 views]: \"Crocodile\"_Dundee\n31\tother\t\"Crocodile\"_Dundee\t[other][31 views]: \"Crocodile\"_Dundee\n221\tlink\t\"Crocodile\"_Dundee\t[link][221 views]: \"Crocodile\"_Dundee\n34\tother\t\"Crocodile\"_Dundee\t[other][34 views]: \"Crocodile\"_Dundee\n60\tlink\t\"Crocodile\"_Dundee\t[link][60 views]: \"Crocodile\"_Dundee\n10\tother\t\"Crocodile\"_Dundee\t[other][10 views]: \"Crocodile\"_Dundee\n12\tlink\t\"Crocodile\"_Dundee\t[link][12 views]: \"Crocodile\"_Dundee\n85\tlink\t\"Crocodile\"_Dundee\t[link][85 views]: \"Crocodile\"_Dundee\n13\tother\t\"Crocodile\"_Dundee\t[other][13 views]: \"Crocodile\"_Dundee\n18\tother\t\"Crocodile\"_Dundee\t[other][18 views]: \"Crocodile\"_Dundee\n153\tlink\t\"Crocodile\"_Dundee\t[link][153 views]: \"Crocodile\"_Dundee\n18\tlink\t\"Crocodile\"_Dundee\t[link][18 views]: \"Crocodile\"_Dundee\n17\tlink\t\"Crocodile\"_Dundee\t[link][17 views]: \"Crocodile\"_Dundee\n21\tother\t\"Crocodile\"_Dundee\t[other][21 views]: \"Crocodile\"_Dundee\n1283\tlink\t\"Crocodile\"_Dundee\t[link][1283 views]: \"Crocodile\"_Dundee\n30\tlink\t\"Crocodile\"_Dundee\t[link][30 views]: \"Crocodile\"_Dundee\n10\tother\t\"Crocodile\"_Dundee\t[other][10 views]: \"Crocodile\"_Dundee\n13\tlink\t\"Crocodile\"_Dundee\t[link][13 views]: \"Crocodile\"_Dundee\n415\tnull\t!!\t[null][415 views]: !!\n113\tnull\t!!\t[null][113 views]: !!\n33\tnull\t!!\t[null][33 views]: !!\n25\tnull\t!!!\t[null][25 views]: !!!\n1193\tnull\t!!!\t[null][1193 views]: !!!\n1065\tnull\t!!!\t[null][1065 views]: !!!\n44\tnull\t!!!\t[null][44 views]: !!!\n11\tnull\t!!!\t[null][11 views]: !!!\n21\tnull\t!!!\t[null][21 views]: !!!\n14\tnull\t!!!\t[null][14 views]: !!!\n70\tnull\t!!!\t[null][70 views]: !!!\n139\tnull\t!!!\t[null][139 views]: !!!\n31\tnull\t!!!\t[null][31 views]: !!!\n40\tnull\t!!!\t[null][40 views]: !!!\n95\tnull\t!!!\t[null][95 views]: !!!\n35\tnull\t!!!\t[null][35 views]: !!!\n11\tnull\t!!!\t[null][11 views]: !!!\n110\tnull\t!!!\t[null][110 views]: !!!\n14\tnull\t!!!\t[null][14 views]: !!!\n136\tnull\t!!!\t[null][136 views]: !!!\n31\tnull\t!!!\t[null][31 views]: !!!\n88\tnull\t!!!\t[null][88 views]: !!!\n21\tnull\t!!!\t[null][21 views]: !!!\n14\tnull\t!!!\t[null][14 views]: !!!\n33\tnull\t!!!\t[null][33 views]: !!!\n11\tnull\t!!!\t[null][11 views]: !!!\n105\tnull\t!!!\t[null][105 views]: !!!\n14\tnull\t!!!\t[null][14 views]: !!!\n210\tnull\t!!!\t[null][210 views]: !!!\n17\tnull\t!!!\t[null][17 views]: !!!\n11\tnull\t!!!\t[null][11 views]: !!!\n22\tnull\t!!!\t[null][22 views]: !!!\n17\tnull\t!!!\t[null][17 views]: !!!\n20\tnull\t!!!\t[null][20 views]: !!!\n20\tnull\t!!!\t[null][20 views]: !!!\n14\tnull\t!!!\t[null][14 views]: !!!\n33\tnull\t!!!\t[null][33 views]: !!!\n15\tnull\t!!!\t[null][15 views]: !!!\n105\tnull\t!!!\t[null][105 views]: !!!\n105\tnull\t!!!\t[null][105 views]: !!!\n53\tnull\t!!!\t[null][53 views]: !!!\n63\tnull\t!!!\t[null][63 views]: !!!\n29\tnull\t!!!\t[null][29 views]: !!!\n56\tnull\t!!!\t[null][56 views]: !!!\n332\tnull\t!!!_(album)\t[null][332 views]: !!!_(album)\n25\tnull\t!!!_(album)\t[null][25 views]: !!!_(album)\n328\tnull\t!!!_(album)\t[null][328 views]: !!!_(album)\n18\tnull\t!!!_(album)\t[null][18 views]: !!!_(album)\n25\tnull\t!!!_(album)\t[null][25 views]: !!!_(album)\n200\tnull\t!Hero_(album)\t[null][200 views]: !Hero_(album)\n"
      },
      "dateCreated": "Apr 17, 2016 3:07:38 PM",
      "dateStarted": "Apr 30, 2016 1:10:04 PM",
      "dateFinished": "Apr 30, 2016 1:10:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Working with grouped data.\r\nWhen we want to perform operations on groups of data, we can use \"groupBy\" in combination with the different aggregated functions.\r\n\r\nFor instance, how many visits did we get for every type?\r\n\r\n---\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData\u003e\r\n\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003e",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:08:19 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905680766_1657750657",
      "id": "20160417-150800_387642255",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eWorking with grouped data.\u003c/h3\u003e\n\u003cp\u003eWhen we want to perform operations on groups of data, we can use \u0026ldquo;groupBy\u0026rdquo; in combination with the different aggregated functions.\u003c/p\u003e\n\u003cp\u003eFor instance, how many visits did we get for every type?\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:08:00 PM",
      "dateStarted": "Apr 17, 2016 3:08:16 PM",
      "dateFinished": "Apr 17, 2016 3:08:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "z.show(clickstreamDf.groupBy(\u0027type).sum(\"n\"))",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 12:59:34 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "type",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "sum(n)",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "type",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "sum(n)",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905696034_66059782",
      "id": "20160417-150816_1456212786",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "type\tsum(n)\nother\t9493\nredlink\t80\nlink\t4755\nnull\t13145\n"
      },
      "dateCreated": "Apr 17, 2016 3:08:16 PM",
      "dateStarted": "Apr 30, 2016 12:59:30 PM",
      "dateFinished": "Apr 30, 2016 12:59:30 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// A DF is transformed into a GroupedData object by its groupBy method\r\nval clickstreamGroupedByPrevId \u003d clickstreamDf.groupBy(\u0027curr_title)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:29:16 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905716025_1152788245",
      "id": "20160417-150836_204031641",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "clickstreamGroupedByPrevId: org.apache.spark.sql.GroupedData \u003d org.apache.spark.sql.GroupedData@3ec7e0ef\n"
      },
      "dateCreated": "Apr 17, 2016 3:08:36 PM",
      "dateStarted": "Apr 30, 2016 1:29:16 PM",
      "dateFinished": "Apr 30, 2016 1:29:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// What are the average visits per prev_id?\r\nz.show(clickstreamGroupedByPrevId.sum(\"n\"))",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:30:47 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "curr_title",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "sum(n)",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905731005_-1793218759",
      "id": "20160417-150851_1622942856",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "curr_title\tsum(n)\n!Kung_language\t1237\n\"Chúc_Mừng_Năm_Mới\"_or_best_wishes_for_the_new_year.\t51\n\"C\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\t993\n\"A\"_Device\t247\n\"A\"_Is_for_Alibi\t636\n\"Crocodile\"_Dundee_II\t4195\n!_(album)\t335\n\"Cool_Hand_Conor\"_O\u0027Neill\t14\n\"Crocodile\"_Dundee\t10703\n\"Bigfoot\"_Wallace\t15\n\"Bassy\"_Bob_Brockmann\t59\n\"B\"_Is_for_Burglar\t385\n!!!_(album)\t1144\n!Hero_(album)\t323\n!!!\t4161\n!_(disambiguation)\t1150\n!Women_Art_Revolution\t481\n!Oka_Tokat\t171\n!!\t842\n\"And\"_theory_of_conservatism\t144\n!T.O.O.H.!\t187\n"
      },
      "dateCreated": "Apr 17, 2016 3:08:51 PM",
      "dateStarted": "Apr 30, 2016 1:30:47 PM",
      "dateFinished": "Apr 30, 2016 1:30:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Putting it all together:\r\nimport org.apache.spark.sql.functions.{avg, sum}\r\nval groupedClickstreamDf \u003d clickstreamDf.groupBy(\u0027type).agg(sum(\"n\").as(\"total_views\"), avg(\"n\").as(\"average_views\"))\r\nz.show(groupedClickstreamDf)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:07:28 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 273.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "type",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "total_views",
              "index": 1.0,
              "aggr": "sum"
            },
            {
              "name": "average_views",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "yAxis": {
              "name": "total_views",
              "index": 1.0,
              "aggr": "sum"
            },
            "xAxis": {
              "name": "average_views",
              "index": 2.0,
              "aggr": "sum"
            },
            "group": {
              "name": "type",
              "index": 0.0,
              "aggr": "sum"
            }
          },
          "lineWithFocus": true
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905747111_392185642",
      "id": "20160417-150907_1034404564",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "type\ttotal_views\taverage_views\nother\t9493\t175.7962962962963\nredlink\t80\t26.666666666666668\nlink\t4755\t113.21428571428571\nnull\t13145\t131.45\n"
      },
      "dateCreated": "Apr 17, 2016 3:09:07 PM",
      "dateStarted": "Apr 30, 2016 1:03:45 PM",
      "dateFinished": "Apr 30, 2016 1:03:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// here is how the physical plan looks like\r\ngroupedClickstreamDf.explain",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:09:39 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905765854_-738960800",
      "id": "20160417-150925_702520758",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nTungstenAggregate(key\u003d[type#23], functions\u003d[(sum(cast(n#20 as bigint)),mode\u003dFinal,isDistinct\u003dfalse),(avg(cast(n#20 as bigint)),mode\u003dFinal,isDistinct\u003dfalse)], output\u003d[type#23,total_views#488L,average_views#489])\n+- TungstenExchange hashpartitioning(type#23,200), None\n   +- TungstenAggregate(key\u003d[type#23], functions\u003d[(sum(cast(n#20 as bigint)),mode\u003dPartial,isDistinct\u003dfalse),(avg(cast(n#20 as bigint)),mode\u003dPartial,isDistinct\u003dfalse)], output\u003d[type#23,sum#504L,sum#505,count#506L])\n      +- Scan ParquetRelation[type#23,n#20] InputPaths: file:/data/clickstream_df.parquet\n"
      },
      "dateCreated": "Apr 17, 2016 3:09:25 PM",
      "dateStarted": "Apr 17, 2016 3:09:39 PM",
      "dateFinished": "Apr 17, 2016 3:09:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Example: Group by \"buckets\"\r\nLet\u0027s assume that we want to group based on the number of views, in these buckets:\r\n- [0, 80]: \"Low\"\r\n- [80, 160]: \"Average\"\r\n- [160, ]: \"High\"",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:09:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905779478_-175072154",
      "id": "20160417-150939_117215281",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eExample: Group by \u0026ldquo;buckets\u0026rdquo;\u003c/h4\u003e\n\u003cp\u003eLet\u0027s assume that we want to group based on the number of views, in these buckets:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e[0, 80]: \u0026ldquo;Low\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e[80, 160]: \u0026ldquo;Average\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e[160, ]: \u0026ldquo;High\u0026rdquo;\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:09:39 PM",
      "dateStarted": "Apr 17, 2016 3:09:51 PM",
      "dateFinished": "Apr 17, 2016 3:09:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.when\r\nval myBuckets \u003d when(\u0027n between (0, 80), \"Low\")\r\n  .when(\u0027n between (80, 160), \"Average\")\r\n  .otherwise(\"High\")\r\n  .as(\"views\")\r\nz.show(clickstreamDf.groupBy(myBuckets).count)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:10:32 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "views",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "views",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905791539_-1860657085",
      "id": "20160417-150951_444396635",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "views\tcount\nHigh\t30\nAverage\t23\nLow\t146\n"
      },
      "dateCreated": "Apr 17, 2016 3:09:51 PM",
      "dateStarted": "Apr 30, 2016 1:10:29 PM",
      "dateFinished": "Apr 30, 2016 1:10:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### User-Defined Aggregated Functions (UDAFs)\r\nLike for UDFs, you can write your own functions for aggregations. However, it is not as simple as using UDFs: You will need to create your own class. Let\u0027s create a UDAF that will generate an array from the grouped data.\r\n\r\n---\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row\u003e",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:10:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905809262_95052222",
      "id": "20160417-151009_1933609379",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eUser-Defined Aggregated Functions (UDAFs)\u003c/h3\u003e\n\u003cp\u003eLike for UDFs, you can write your own functions for aggregations. However, it is not as simple as using UDFs: You will need to create your own class. Let\u0027s create a UDAF that will generate an array from the grouped data.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:10:09 PM",
      "dateStarted": "Apr 17, 2016 3:10:50 PM",
      "dateFinished": "Apr 17, 2016 3:10:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\r\nimport org.apache.spark.sql.Row\r\nimport scala.collection.mutable.ArrayBuffer\r\nimport org.apache.spark.sql.types.{ArrayType, StringType, StructType}\r\n\r\nclass GroupToSet extends UserDefinedAggregateFunction {\r\n    def inputSchema \u003d new StructType().add(\"x\", StringType)\r\n    def bufferSchema \u003d new StructType().add(\"buff\", ArrayType(StringType))\r\n    def dataType \u003d ArrayType(StringType)\r\n    def deterministic \u003d true\r\n\r\n    def initialize(buffer: MutableAggregationBuffer) \u003d {\r\n        buffer.update(0, ArrayBuffer.empty[String])\r\n    }\r\n\r\n    def update(buffer: MutableAggregationBuffer, input: Row) \u003d {\r\n        if (!input.isNullAt(0))\r\n            buffer.update(0, buffer.getSeq[String](0) :+ input.getString(0))\r\n    }\r\n\r\n    def merge(buffer1: MutableAggregationBuffer, buffer2: Row) \u003d {\r\n        buffer1.update(0, buffer1.getSeq[String](0) ++ buffer2.getSeq[String](0))\r\n    }\r\n\r\n    def evaluate(buffer: Row) \u003d buffer.getSeq[String](0).sorted.distinct\r\n}",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 12:36:51 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905850476_109550087",
      "id": "20160417-151050_907527697",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\nimport org.apache.spark.sql.Row\nimport scala.collection.mutable.ArrayBuffer\nimport org.apache.spark.sql.types.{ArrayType, StringType, StructType}\ndefined class GroupToSet\n"
      },
      "dateCreated": "Apr 17, 2016 3:10:50 PM",
      "dateStarted": "Apr 30, 2016 12:36:51 PM",
      "dateFinished": "Apr 30, 2016 12:36:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val groupToSet \u003d new GroupToSet()\r\nz.show(clickstreamDf.groupBy(\u0027curr_id).agg(groupToSet(\u0027prev_id).as(\u0027prev_ids)))",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:11:00 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905880642_-186275845",
      "id": "20160417-151120_1727353729",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "curr_id\tprev_ids\n3243047\tWrappedArray(1337475, 3284285)\n16250456\tWrappedArray(1282996, 2301720, 470006)\n9003666\tWrappedArray(26181056)\n22602473\tWrappedArray(34376590, 35978874, 37104582, 7360687, 8127304)\n899480\tWrappedArray(194844, 206427, 2098292, 773691, 855901, 878246)\n64486\tWrappedArray(600744, 7712754)\n3632887\tWrappedArray(64486)\n6893310\tWrappedArray(1921683)\n39072529\tWrappedArray()\n600744\tWrappedArray(11108427, 11184232, 12730440, 1327495, 14172091, 1446971, 1507641, 15580374, 156725, 1803975, 18636054, 2061699, 2064029, 2490600, 25014178, 2556962, 26762896, 26780719, 29631, 3140017, 3259409, 34552784, 3632887, 3768982, 39256216, 40022038, 40604381, 4499, 558500, 64486, 7007982, 7712754, 8526330, 870667, 929155)\n2556962\tWrappedArray(2061699, 600744, 64486)\n6810768\tWrappedArray(31976181)\n25033979\tWrappedArray(113468, 12571133, 1375614, 14096078, 147692, 159547, 1893465, 28639397, 33622887, 4619790)\n331586\tWrappedArray(10040606, 1118809, 1248074, 1448969, 152171, 171612, 1872502, 196020, 22344579, 2376452, 2564144, 331460, 34557, 37386608, 37882, 4008173, 44789934, 489033, 5644, 6127928, 643649, 70209, 7033, 72766, 8078282, 8306521, 865241)\n1282996\tWrappedArray(2301720, 39606873, 470006)\nnull\tWrappedArray(11273993, 1438509, 69161)\n2516600\tWrappedArray(1383618, 17333, 1758827, 22980, 247700, 261237, 27164415, 34314219, 524853, 524854, 713020, 7863678)\n1118809\tWrappedArray(171292, 19824417, 2259599, 2564144, 331460, 331586, 657547, 693780, 70209)\n29988427\tWrappedArray(1686995, 420777, 6814223)\n"
      },
      "dateCreated": "Apr 17, 2016 3:11:20 PM",
      "dateStarted": "Apr 30, 2016 1:11:00 PM",
      "dateFinished": "Apr 30, 2016 1:11:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Window functions\r\nWindow functions come in handy when you want to perform operations on groups of data, but still want to keep the whole DataFrame.\r\n\r\nFollowing up on the example before, let´s assume that we want to know, for each \"prev\\_id\", the weight or share for each of the types. We want to get something like this:\r\n\r\n```\r\n+-------+-------------+------+\r\n|prev_id|         type| share|\r\n+-------+-------------+------+\r\n|   1234|         link|   0.5|\r\n|   1234|        other|  0.25|\r\n|   1234|      redlink|  0.25|\r\n+-------+-------------+------+\r\n```\r\n\r\n\r\nTo calculate this \"share\", we need to calculate two numbers: the total count for each prev\\_id (countPrevId), and the total count for each prev\\_id - type pair (countPrevIdType). From that, we can get the share by calculating countPrevIdType / countPrevId.\r\n\r\n---\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window\u003e\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003e (search \"window\" among methods)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 12:37:32 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905899576_-1589439759",
      "id": "20160417-151139_236075944",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eWindow functions\u003c/h3\u003e\n\u003cp\u003eWindow functions come in handy when you want to perform operations on groups of data, but still want to keep the whole DataFrame.\u003c/p\u003e\n\u003cp\u003eFollowing up on the example before, let´s assume that we want to know, for each \u0026ldquo;prev_id\u0026rdquo;, the weight or share for each of the types. We want to get something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e+-------+-------------+------+\n|prev_id|         type| share|\n+-------+-------------+------+\n|   1234|         link|   0.5|\n|   1234|        other|  0.25|\n|   1234|      redlink|  0.25|\n+-------+-------------+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo calculate this \u0026ldquo;share\u0026rdquo;, we need to calculate two numbers: the total count for each prev_id (countPrevId), and the total count for each prev_id - type pair (countPrevIdType). From that, we can get the share by calculating countPrevIdType / countPrevId.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window\u003c/a\u003e\n\u003cbr  /\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003c/a\u003e (search \u0026ldquo;window\u0026rdquo; among methods)\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:11:39 PM",
      "dateStarted": "Apr 30, 2016 12:37:32 PM",
      "dateFinished": "Apr 30, 2016 12:37:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Long.MinValue and Long.MaxValue will make our Window unbounded on the lower / upper side.\r\nimport org.apache.spark.sql.expressions.Window\r\nimport org.apache.spark.sql.functions.lit\r\nval prevIdWindow \u003d Window.partitionBy(\u0027prev_id).orderBy(\u0027type).rowsBetween(Long.MinValue, Long.MaxValue)\r\nval prevIdTypeWindow \u003d Window.partitionBy(\u0027prev_id, \u0027type).orderBy(lit(1)).rowsBetween(Long.MinValue, Long.MaxValue)\r\nz.show(\r\n    clickstreamDf.groupBy(\u0027prev_id, \u0027type)\r\n        .count\r\n        .select(\r\n            \u0027prev_id,\r\n            \u0027type,\r\n            (sum(\"count\").over(prevIdTypeWindow) / sum(\"count\").over(prevIdWindow)).as(\u0027share)\r\n        )\r\n        .filter(\u0027share!\u003d\u003d1.0)\r\n)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:39:03 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "type",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "share",
              "index": 2.0,
              "aggr": "avg"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "prev_id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "type",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905921328_437664278",
      "id": "20160417-151201_391017010",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "prev_id\ttype\tshare\n331460\tnull\t0.5\n331460\tlink\t0.5\n1921683\tnull\t0.5\n1921683\tlink\t0.5\n64486\tnull\t0.3333333333333333\n64486\tlink\t0.3333333333333333\n64486\tother\t0.3333333333333333\n2061699\tnull\t0.6666666666666666\n2061699\tlink\t0.3333333333333333\n2301720\tnull\t0.5\n2301720\tlink\t0.5\n600744\tnull\t0.6666666666666666\n600744\tlink\t0.3333333333333333\n2564144\tnull\t0.5\n2564144\tlink\t0.5\nnull\tnull\t0.4782608695652174\nnull\tother\t0.5217391304347826\n470006\tnull\t0.5\n470006\tlink\t0.5\n70209\tnull\t0.5\n70209\tlink\t0.5\n"
      },
      "dateCreated": "Apr 17, 2016 3:12:01 PM",
      "dateStarted": "Apr 30, 2016 1:33:22 PM",
      "dateFinished": "Apr 30, 2016 1:33:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Window Example 2 - Calculate accumulated value.\r\nFor this example, we\u0027ll create a new DataFrame with expenses data, this will be the structure:\r\n- Name\r\n- Date\r\n- Amount\r\n\r\nWe want to generate the accumulated expenses for every person, ordered by date.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:12:47 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905945269_-282170266",
      "id": "20160417-151225_1651357142",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eWindow Example 2 - Calculate accumulated value.\u003c/h4\u003e\n\u003cp\u003eFor this example, we\u0027ll create a new DataFrame with expenses data, this will be the structure:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eName\u003c/li\u003e\n\u003cli\u003eDate\u003c/li\u003e\n\u003cli\u003eAmount\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe want to generate the accumulated expenses for every person, ordered by date.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:12:25 PM",
      "dateStarted": "Apr 17, 2016 3:12:44 PM",
      "dateFinished": "Apr 17, 2016 3:12:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.sql.{Date \u003d\u003e JavaDate}\r\nval rawData \u003d Array(\r\n  (\"John\", JavaDate.valueOf(\"2016-03-10\"), 123),\r\n  (\"Claire\", JavaDate.valueOf(\"2016-01-25\"), 32),\r\n  (\"John\", JavaDate.valueOf(\"2016-01-09\"), 423),\r\n  (\"Tim\", JavaDate.valueOf(\"2015-11-23\"), 13),\r\n  (\"Claire\", JavaDate.valueOf(\"2016-02-03\"), 319),\r\n  (\"Tim\", JavaDate.valueOf(\"2016-03-07\"), 38),\r\n  (\"John\", JavaDate.valueOf(\"2016-02-27\"), 112),\r\n  (\"Claire\", JavaDate.valueOf(\"2016-01-29\"), 81),\r\n  (\"John\", JavaDate.valueOf(\"2016-01-01\"), 55),\r\n  (\"Claire\", JavaDate.valueOf(\"2016-01-02\"), 71),\r\n  (\"Tim\", JavaDate.valueOf(\"2016-02-25\"), 193)\r\n)\r\nval expensesDf \u003d sc.parallelize(rawData).toDF(\"name\", \"date\", \"amount\")\r\nval expensesWindow \u003d Window.partitionBy(\u0027name).orderBy(\u0027date).rowsBetween(Long.MinValue, 0)\r\nval expensesRunSum \u003d expensesDf.select(\r\n  \u0027name,\r\n  \u0027date,\r\n  \u0027amount,\r\n  sum(\u0027amount).over(expensesWindow).as(\"accumulated_amount\")\r\n)\r\n.orderBy(\u0027name, \u0027date)\r\nz.show(expensesRunSum)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:20:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": true,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "name",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "date",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905964719_-1905410551",
      "id": "20160417-151244_614791596",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "name\tdate\tamount\taccumulated_amount\nClaire\t2016-01-02\t71\t71\nClaire\t2016-01-25\t32\t103\nClaire\t2016-01-29\t81\t184\nClaire\t2016-02-03\t319\t503\nJohn\t2016-01-01\t55\t55\nJohn\t2016-01-09\t423\t478\nJohn\t2016-02-27\t112\t590\nJohn\t2016-03-10\t123\t713\nTim\t2015-11-23\t13\t13\nTim\t2016-02-25\t193\t206\nTim\t2016-03-07\t38\t244\n"
      },
      "dateCreated": "Apr 17, 2016 3:12:44 PM",
      "dateStarted": "Apr 30, 2016 1:20:53 PM",
      "dateFinished": "Apr 30, 2016 1:20:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// physical plan?\r\nexpensesRunSum.explain",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:14:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460905984041_-672092757",
      "id": "20160417-151304_1653144392",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nSort [name#623 ASC,date#624 ASC], true, 0\n+- ConvertToUnsafe\n   +- Exchange rangepartitioning(name#623 ASC,date#624 ASC,200), None\n      +- Window [name#623,date#624,amount#625], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum(amount#625) windowspecdefinition(name#623,date#624 ASC,ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS accumulated_amount#626L], [name#623], [date#624 ASC]\n         +- Sort [name#623 ASC,date#624 ASC], false, 0\n            +- TungstenExchange hashpartitioning(name#623,200), None\n               +- Project [_1#620 AS name#623,_2#621 AS date#624,_3#622 AS amount#625]\n                  +- Scan ExistingRDD[_1#620,_2#621,_3#622]\n"
      },
      "dateCreated": "Apr 17, 2016 3:13:04 PM",
      "dateStarted": "Apr 17, 2016 3:14:13 PM",
      "dateFinished": "Apr 17, 2016 3:14:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Joins\r\n\r\nJoining datasets is another of the basic operations you\u0027ll need when working with data. These are the join types we\u0027ll be showing:\r\n\r\nLet\u0027s assume we have two DataFrames, A and B, and that we join based on column \"some\\_id\"\r\n- Inner join: Get rows that exist both in A and B.\r\n- Left join: Get all rows from A, getting also information from B, if it exists.\r\n- Right join: The same as before, but this time we get all rows from B, and get information from A if it exists\r\n- Left semi-join: Get only rows from A, but rows that also exist in B. We will not be able to get any information from rows in B.\r\n- Left anti-join: Get rows from A that do not exist in B.\r\n- Outer join: Get all rows from A and all rows from B, whether it exists in both or not.\r\n- Cross join: Perform a cartesian product of A and B: perform all of the combinations of rows in A with rows in B.\r\n\r\n\r\nFirst, let\u0027s create the our datasets",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:14:33 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906053755_331240430",
      "id": "20160417-151413_1732582760",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eJoins\u003c/h3\u003e\n\u003cp\u003eJoining datasets is another of the basic operations you\u0027ll need when working with data. These are the join types we\u0027ll be showing:\u003c/p\u003e\n\u003cp\u003eLet\u0027s assume we have two DataFrames, A and B, and that we join based on column \u0026ldquo;some_id\u0026rdquo;\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInner join: Get rows that exist both in A and B.\u003c/li\u003e\n\u003cli\u003eLeft join: Get all rows from A, getting also information from B, if it exists.\u003c/li\u003e\n\u003cli\u003eRight join: The same as before, but this time we get all rows from B, and get information from A if it exists\u003c/li\u003e\n\u003cli\u003eLeft semi-join: Get only rows from A, but rows that also exist in B. We will not be able to get any information from rows in B.\u003c/li\u003e\n\u003cli\u003eLeft anti-join: Get rows from A that do not exist in B.\u003c/li\u003e\n\u003cli\u003eOuter join: Get all rows from A and all rows from B, whether it exists in both or not.\u003c/li\u003e\n\u003cli\u003eCross join: Perform a cartesian product of A and B: perform all of the combinations of rows in A with rows in B.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFirst, let\u0027s create the our datasets\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:14:13 PM",
      "dateStarted": "Apr 17, 2016 3:14:30 PM",
      "dateFinished": "Apr 17, 2016 3:14:30 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.sql.{Date \u003d\u003e JavaDate}\r\n\r\nval rawUsers \u003d Array(\r\n  (1, \"John\", \"USA\", JavaDate.valueOf(\"2016-01-01\")),\r\n  (2, \"Claire\", \"Belgium\", JavaDate.valueOf(\"2016-01-02\")),\r\n  (3, \"Anne\", \"Belgium\", JavaDate.valueOf(\"2016-01-04\")),\r\n  (4, \"Tom\", \"France\", JavaDate.valueOf(\"2016-02-04\"))\r\n)\r\nval users \u003d sc.parallelize(rawUsers).toDF(\"id\", \"name\", \"country\", \"creation_date\")\r\nval dateFormat \u003d new java.text.SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\r\nval toTimestamp \u003d (strDate: String) \u003d\u003e new java.sql.Timestamp(dateFormat.parse(strDate).getTime)\r\n\r\nval rawActivity \u003d Array(\r\n  (1, \"confirm_email\", toTimestamp(\"2016-01-01 10:10:46\"), null),\r\n  (2, \"confirm_email\", toTimestamp(\"2016-01-02 20:17:00\"), null),\r\n  (3, \"confirm_email\", toTimestamp(\"2016-01-05 09:23:50\"), null),\r\n  (1, \"login\", toTimestamp(\"2016-01-10 12:28:28\"), null),\r\n  (1, \"search\", toTimestamp(\"2016-01-10 12:30:12\"), \"{\\\"query\\\": \\\"motorcycle\\\"}\"),\r\n  (1, \"login\", toTimestamp(\"2016-02-11 10:05:56\"), null),\r\n  (1, \"search\", toTimestamp(\"2016-02-11 10:10:46\"), \"{\\\"query\\\": \\\"car\\\"}\"),\r\n  (1, \"purchase\", toTimestamp(\"2016-02-11 13:47:28\"), \"{product: \\\"Some Fancy Car\\\"\"),\r\n  (2, \"login\", toTimestamp(\"2016-01-11 15:51:02\"), null),\r\n  (2, \"search\", toTimestamp(\"2016-01-11 16:18:39\"), \"{\\\"query\\\": \\\"helicopter\\\"}\")\r\n)\r\nval activity \u003d sc.parallelize(rawActivity).toDF(\"id\", \"type\", \"timestamp\", \"params\")",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:35:09 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906070151_-1595951487",
      "id": "20160417-151430_1032541362",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.sql.{Date\u003d\u003eJavaDate}\nrawUsers: Array[(Int, String, String, java.sql.Date)] \u003d Array((1,John,USA,2016-01-01), (2,Claire,Belgium,2016-01-02), (3,Anne,Belgium,2016-01-04), (4,Tom,France,2016-02-04))\nusers: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, country: string, creation_date: date]\ndateFormat: java.text.SimpleDateFormat \u003d java.text.SimpleDateFormat@4f76f1a0\ntoTimestamp: String \u003d\u003e java.sql.Timestamp \u003d \u003cfunction1\u003e\nrawActivity: Array[(Int, String, java.sql.Timestamp, String)] \u003d Array((1,confirm_email,2016-01-01 10:10:46.0,null), (2,confirm_email,2016-01-02 20:17:00.0,null), (3,confirm_email,2016-01-05 09:23:50.0,null), (1,login,2016-01-10 12:28:28.0,null), (1,search,2016-01-10 12:30:12.0,{\"query\": \"motorcycle\"}), (1,login,2016-02-11 10:05:56.0,null), (1,search,2016-02-11 10:10:46.0,{\"query\": \"car\"}), (1,purchase,2016-02-11 13:47:28.0,{product: \"Some Fancy Car\"), (2,login,2016-01-11 15:51:02.0,null), (2,search,2016-01-11 16:18:39.0,{\"query\": \"helicopter\"}))\nactivity: org.apache.spark.sql.DataFrame \u003d [id: int, type: string, timestamp: timestamp, params: string]\n"
      },
      "dateCreated": "Apr 17, 2016 3:14:30 PM",
      "dateStarted": "Apr 30, 2016 1:35:09 PM",
      "dateFinished": "Apr 30, 2016 1:35:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Inner Join\r\nLet\u0027s suppose that we want to see the name, activity type, timestamp and the query (if any) that the user typed, but only for users who have some activity.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:15:10 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906092913_1064633130",
      "id": "20160417-151452_726522540",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eInner Join\u003c/h4\u003e\n\u003cp\u003eLet\u0027s suppose that we want to see the name, activity type, timestamp and the query (if any) that the user typed, but only for users who have some activity.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:14:52 PM",
      "dateStarted": "Apr 17, 2016 3:15:07 PM",
      "dateFinished": "Apr 17, 2016 3:15:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val usersInnerActivity \u003d users.as(\u0027u)\r\n  .join(activity.as(\u0027a), $\"u.id\"\u003d\u003d\u003d$\"a.id\", joinType\u003d\"inner\")\r\n  .selectExpr(\"u.id\", \"u.name\", \"a.type\", \"a.timestamp\", \"get_json_object(a.params, \u0027$.query\u0027) AS query\")\r\nz.show(usersInnerActivity)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:35:37 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906107446_1181828303",
      "id": "20160417-151507_1029957162",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "id\tname\ttype\ttimestamp\tquery\n1\tJohn\tconfirm_email\t2016-01-01 10:10:46.0\tnull\n1\tJohn\tlogin\t2016-01-10 12:28:28.0\tnull\n1\tJohn\tsearch\t2016-01-10 12:30:12.0\tmotorcycle\n1\tJohn\tlogin\t2016-02-11 10:05:56.0\tnull\n1\tJohn\tsearch\t2016-02-11 10:10:46.0\tcar\n1\tJohn\tpurchase\t2016-02-11 13:47:28.0\tnull\n2\tClaire\tconfirm_email\t2016-01-02 20:17:00.0\tnull\n2\tClaire\tlogin\t2016-01-11 15:51:02.0\tnull\n2\tClaire\tsearch\t2016-01-11 16:18:39.0\thelicopter\n3\tAnne\tconfirm_email\t2016-01-05 09:23:50.0\tnull\n"
      },
      "dateCreated": "Apr 17, 2016 3:15:07 PM",
      "dateStarted": "Apr 30, 2016 1:35:37 PM",
      "dateFinished": "Apr 30, 2016 1:35:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "usersInnerActivity.explain",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:15:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906127060_-396781110",
      "id": "20160417-151527_1441916985",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nProject [id#632,name#633,type#641,timestamp#642,get_json_object(params#643,$.query) AS query#644]\n+- SortMergeJoin [id#632], [id#640]\n   :- Sort [id#632 ASC], false, 0\n   :  +- TungstenExchange hashpartitioning(id#632,200), None\n   :     +- Project [_1#628 AS id#632,_2#629 AS name#633]\n   :        +- Scan ExistingRDD[_1#628,_2#629,_3#630,_4#631] \n   +- Sort [id#640 ASC], false, 0\n      +- TungstenExchange hashpartitioning(id#640,200), None\n         +- Project [_1#636 AS id#640,_2#637 AS type#641,_3#638 AS timestamp#642,_4#639 AS params#643]\n            +- Scan ExistingRDD[_1#636,_2#637,_3#638,_4#639]\n"
      },
      "dateCreated": "Apr 17, 2016 3:15:27 PM",
      "dateStarted": "Apr 17, 2016 3:15:42 PM",
      "dateFinished": "Apr 17, 2016 3:15:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Left Join\r\nNow, imagine we want to know, for all of the users, how many \"events\" or activities we have registered.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:15:59 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906142251_-2055803043",
      "id": "20160417-151542_114933125",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eLeft Join\u003c/h3\u003e\n\u003cp\u003eNow, imagine we want to know, for all of the users, how many \u0026ldquo;events\u0026rdquo; or activities we have registered.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:15:42 PM",
      "dateStarted": "Apr 17, 2016 3:15:56 PM",
      "dateFinished": "Apr 17, 2016 3:15:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val usersLeftActivity \u003d users.as(\u0027u)\r\n  .join(activity.groupBy(\u0027id).count.as(\u0027a), $\"u.id\"\u003d\u003d\u003d$\"a.id\", \"left_outer\")\r\n  .selectExpr(\"u.id\", \"u.name\", \"a.count AS totalEvents\")\r\nz.show(usersLeftActivity)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:36:36 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "name",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "totalEvents",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "name",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906156818_-437133772",
      "id": "20160417-151556_1256056740",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "id\tname\ttotalEvents\n1\tJohn\t6\n2\tClaire\t3\n3\tAnne\t1\n4\tTom\tnull\n"
      },
      "dateCreated": "Apr 17, 2016 3:15:56 PM",
      "dateStarted": "Apr 30, 2016 1:35:58 PM",
      "dateFinished": "Apr 30, 2016 1:35:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "usersLeftActivity.explain",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:16:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906176934_-1753528325",
      "id": "20160417-151616_1879921852",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nProject [id#632,name#633,count#646L AS totalEvents#647L]\n+- SortMergeOuterJoin [id#632], [id#640], LeftOuter, None\n   :- Sort [id#632 ASC], false, 0\n   :  +- TungstenExchange hashpartitioning(id#632,200), None\n   :     +- Project [_1#628 AS id#632,_2#629 AS name#633]\n   :        +- Scan ExistingRDD[_1#628,_2#629,_3#630,_4#631] \n   +- Sort [id#640 ASC], false, 0\n      +- TungstenAggregate(key\u003d[id#640], functions\u003d[(count(1),mode\u003dFinal,isDistinct\u003dfalse)], output\u003d[id#640,count#646L])\n         +- TungstenExchange hashpartitioning(id#640,200), None\n            +- TungstenAggregate(key\u003d[id#640], functions\u003d[(count(1),mode\u003dPartial,isDistinct\u003dfalse)], output\u003d[id#640,count#650L])\n               +- Project [_1#636 AS id#640]\n                  +- Scan ExistingRDD[_1#636,_2#637,_3#638,_4#639]\n"
      },
      "dateCreated": "Apr 17, 2016 3:16:16 PM",
      "dateStarted": "Apr 17, 2016 3:16:35 PM",
      "dateFinished": "Apr 17, 2016 3:16:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Right Join\r\nLet\u0027s try the same example as before, but this time using a right join instead. We\u0027ll also introduce a new expression, to show a \"0\" instead when totalEvents is null.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:16:59 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906195626_1023619884",
      "id": "20160417-151635_1563994797",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRight Join\u003c/h3\u003e\n\u003cp\u003eLet\u0027s try the same example as before, but this time using a right join instead. We\u0027ll also introduce a new expression, to show a \u0026ldquo;0\u0026rdquo; instead when totalEvents is null.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:16:35 PM",
      "dateStarted": "Apr 17, 2016 3:16:51 PM",
      "dateFinished": "Apr 17, 2016 3:16:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "z.show(\r\n    activity.groupBy(\u0027id).count.as(\u0027a)\r\n        .join(users.as(\u0027u), $\"a.id\"\u003d\u003d\u003d$\"u.id\", \"right_outer\")\r\n        .selectExpr(\"u.id\", \"u.name\", \"COALESCE(a.count, 0) AS totalEvents\")\r\n)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:48:21 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 154.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906211544_-795072843",
      "id": "20160417-151651_370325923",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "id\tname\ttotalEvents\n1\tJohn\t6\n2\tClaire\t3\n3\tAnne\t1\n4\tTom\t0\n"
      },
      "dateCreated": "Apr 17, 2016 3:16:51 PM",
      "dateStarted": "Apr 30, 2016 1:38:32 PM",
      "dateFinished": "Apr 30, 2016 1:38:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Left Semi-Join\r\nNow, we want to get data from users, but only for users that have some activity. We can use a left semi-join for that.\r\n\r\n---\r\n**Note**: *we could achieve the same with an inner join, but a left semi join will be way faster.*",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:17:30 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906235112_468255366",
      "id": "20160417-151715_1102398861",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eLeft Semi-Join\u003c/h3\u003e\n\u003cp\u003eNow, we want to get data from users, but only for users that have some activity. We can use a left semi-join for that.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003cem\u003ewe could achieve the same with an inner join, but a left semi join will be way faster.\u003c/em\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:17:15 PM",
      "dateStarted": "Apr 17, 2016 3:17:26 PM",
      "dateFinished": "Apr 17, 2016 3:17:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val usersLeftSemiActivity \u003d users.as(\u0027u)\r\n  .join(activity.as(\u0027a), $\"u.id\"\u003d\u003d\u003d$\"a.id\", \"left_semi\")\r\nz.show(usersLeftSemiActivity)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:48:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 118.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "name",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "name",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906246762_1327045560",
      "id": "20160417-151726_1491486219",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "id\tname\tcountry\tcreation_date\n1\tJohn\tUSA\t2016-01-01\n2\tClaire\tBelgium\t2016-01-02\n3\tAnne\tBelgium\t2016-01-04\n"
      },
      "dateCreated": "Apr 17, 2016 3:17:26 PM",
      "dateStarted": "Apr 30, 2016 1:48:01 PM",
      "dateFinished": "Apr 30, 2016 1:48:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "usersLeftSemiActivity.explain",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:18:03 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906268381_565258062",
      "id": "20160417-151748_2079623330",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nLeftSemiJoinHash [id#632], [id#640], None\n:- TungstenExchange hashpartitioning(id#632,200), None\n:  +- Project [_1#628 AS id#632,_2#629 AS name#633,_3#630 AS country#634,_4#631 AS creation_date#635]\n:     +- Scan ExistingRDD[_1#628,_2#629,_3#630,_4#631] \n+- TungstenExchange hashpartitioning(id#640,200), None\n   +- Project [_1#636 AS id#640]\n      +- Scan ExistingRDD[_1#636,_2#637,_3#638,_4#639]\n"
      },
      "dateCreated": "Apr 17, 2016 3:17:48 PM",
      "dateStarted": "Apr 17, 2016 3:18:03 PM",
      "dateFinished": "Apr 17, 2016 3:18:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Left Anti-join\r\n\r\nWe have no option to perform a left-anti join directly, so we\u0027ll need to do it using a left join + filter.\r\n\r\nIn this case, we want to get users that have no activity at all.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:18:23 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906283009_-878689236",
      "id": "20160417-151803_1883457599",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eLeft Anti-join\u003c/h3\u003e\n\u003cp\u003eWe have no option to perform a left-anti join directly, so we\u0027ll need to do it using a left join + filter.\u003c/p\u003e\n\u003cp\u003eIn this case, we want to get users that have no activity at all.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:18:03 PM",
      "dateStarted": "Apr 17, 2016 3:18:19 PM",
      "dateFinished": "Apr 17, 2016 3:18:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val usersLeftAntiActivity \u003d users.as(\u0027u)\r\n  .join(activity.as(\u0027a), $\"u.id\"\u003d\u003d\u003d$\"a.id\", \"left_outer\")\r\n  .filter(\"a.id IS NULL\")\r\nusersLeftAntiActivity.show",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:51:11 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 97.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "name",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "name",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906299450_289639932",
      "id": "20160417-151819_29191990",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "usersLeftAntiActivity: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, country: string, creation_date: date, id: int, type: string, timestamp: timestamp, params: string]\n+---+----+-------+-------------+----+----+---------+------+\n| id|name|country|creation_date|  id|type|timestamp|params|\n+---+----+-------+-------------+----+----+---------+------+\n|  4| Tom| France|   2016-02-04|null|null|     null|  null|\n+---+----+-------+-------------+----+----+---------+------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:18:19 PM",
      "dateStarted": "Apr 30, 2016 1:51:12 PM",
      "dateFinished": "Apr 30, 2016 1:51:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "usersLeftAntiActivity.explain",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:18:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906314153_1928316145",
      "id": "20160417-151834_1063802400",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nFilter isnull(id#640)\n+- SortMergeOuterJoin [id#632], [id#640], LeftOuter, None\n   :- Sort [id#632 ASC], false, 0\n   :  +- TungstenExchange hashpartitioning(id#632,200), None\n   :     +- Project [_1#628 AS id#632,_2#629 AS name#633,_3#630 AS country#634,_4#631 AS creation_date#635]\n   :        +- Scan ExistingRDD[_1#628,_2#629,_3#630,_4#631] \n   +- Sort [id#640 ASC], false, 0\n      +- TungstenExchange hashpartitioning(id#640,200), None\n         +- Project [_1#636 AS id#640,_2#637 AS type#641,_3#638 AS timestamp#642,_4#639 AS params#643]\n            +- Scan ExistingRDD[_1#636,_2#637,_3#638,_4#639]\n"
      },
      "dateCreated": "Apr 17, 2016 3:18:34 PM",
      "dateStarted": "Apr 17, 2016 3:18:48 PM",
      "dateFinished": "Apr 17, 2016 3:18:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Outer Join\r\n\r\nTo illustrate better what an outer join does, we\u0027ll create two very simple datasets.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:19:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906328206_-954976788",
      "id": "20160417-151848_969425882",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eOuter Join\u003c/h3\u003e\n\u003cp\u003eTo illustrate better what an outer join does, we\u0027ll create two very simple datasets.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:18:48 PM",
      "dateStarted": "Apr 17, 2016 3:19:04 PM",
      "dateFinished": "Apr 17, 2016 3:19:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val rawA \u003d Array((1, \"A\"), (2, \"B\"))\r\nval rawB \u003d Array((2, \"X\"), (3, \"Y\"))\r\nval a \u003d sc.parallelize(rawA).toDF(\"id\", \"col_a\")\r\nval b \u003d sc.parallelize(rawB).toDF(\"id\", \"col_b\")",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:49:49 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906344133_1938350265",
      "id": "20160417-151904_1749684989",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "rawA: Array[(Int, String)] \u003d Array((1,A), (2,B))\nrawB: Array[(Int, String)] \u003d Array((2,X), (3,Y))\na: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string]\nb: org.apache.spark.sql.DataFrame \u003d [id: int, col_b: string]\n"
      },
      "dateCreated": "Apr 17, 2016 3:19:04 PM",
      "dateStarted": "Apr 30, 2016 1:49:49 PM",
      "dateFinished": "Apr 30, 2016 1:49:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"outer\")\r\n  .selectExpr(\"a.id AS id_a\", \"a.col_a\", \"b.id AS id_b\", \"b.col_b\")\r\n  .show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:50:17 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 119.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "id_a",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "col_a",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id_a",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "col_a",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906359402_261629883",
      "id": "20160417-151919_550229561",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----+-----+----+-----+\n|id_a|col_a|id_b|col_b|\n+----+-----+----+-----+\n|   1|    A|null| null|\n|   2|    B|   2|    X|\n|null| null|   3|    Y|\n+----+-----+----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:19:19 PM",
      "dateStarted": "Apr 30, 2016 1:50:17 PM",
      "dateFinished": "Apr 30, 2016 1:50:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md In the cases when you do want to perform an outer join, you\u0027ll probably want just one \"id\" column, like this:",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:19:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906376754_1077913807",
      "id": "20160417-151936_1762819658",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIn the cases when you do want to perform an outer join, you\u0027ll probably want just one \u0026ldquo;id\u0026rdquo; column, like this:\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:19:36 PM",
      "dateStarted": "Apr 17, 2016 3:19:49 PM",
      "dateFinished": "Apr 17, 2016 3:19:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"outer\")\r\n  .selectExpr(\"COALESCE(a.id, b.id) AS id\", \"a.col_a\", \"b.col_b\")\r\n  .show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:20:04 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906389202_1825126819",
      "id": "20160417-151949_156149032",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+-----+\n| id|col_a|col_b|\n+---+-----+-----+\n|  1|    A| null|\n|  2|    B|    X|\n|  3| null|    Y|\n+---+-----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:19:49 PM",
      "dateStarted": "Apr 17, 2016 3:20:05 PM",
      "dateFinished": "Apr 17, 2016 3:20:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Cross Join (Cartesian Product)\r\nNote that you will rarely need a cross join, but when you do, take into account that the resulting DataFrame can become HUGE really fast. The size of the resulting DataFrame is the multiplication of both sizes.\r\n\r\nIn this example, we will do it with DataFrames \"a\" and \"b\" from the previous example, each of which has size 2, so we expect the cartesian product to have 4 rows.\r\n\r\n---\r\n**Note**: As a rule of thumb, you usually should avoid Cross Joins",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:20:20 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906404993_30673273",
      "id": "20160417-152004_741360067",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eCross Join (Cartesian Product)\u003c/h3\u003e\n\u003cp\u003eNote that you will rarely need a cross join, but when you do, take into account that the resulting DataFrame can become HUGE really fast. The size of the resulting DataFrame is the multiplication of both sizes.\u003c/p\u003e\n\u003cp\u003eIn this example, we will do it with DataFrames \u0026ldquo;a\u0026rdquo; and \u0026ldquo;b\u0026rdquo; from the previous example, each of which has size 2, so we expect the cartesian product to have 4 rows.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: As a rule of thumb, you usually should avoid Cross Joins\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:20:04 PM",
      "dateStarted": "Apr 17, 2016 3:20:17 PM",
      "dateFinished": "Apr 17, 2016 3:20:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b))\r\n  .selectExpr(\"a.id AS id_a\", \"a.col_a\", \"b.id AS id_b\", \"b.col_b\")\r\n  .show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:20:33 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906417390_-1585656856",
      "id": "20160417-152017_1648896040",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----+-----+----+-----+\n|id_a|col_a|id_b|col_b|\n+----+-----+----+-----+\n|   1|    A|   2|    X|\n|   1|    A|   3|    Y|\n|   2|    B|   2|    X|\n|   2|    B|   3|    Y|\n+----+-----+----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:20:17 PM",
      "dateStarted": "Apr 17, 2016 3:20:33 PM",
      "dateFinished": "Apr 17, 2016 3:20:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Some tips and strategies when joining DataFrames ",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:20:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906433002_84769627",
      "id": "20160417-152033_978328659",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eSome tips and strategies when joining DataFrames\u003c/h3\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:20:33 PM",
      "dateStarted": "Apr 17, 2016 3:20:43 PM",
      "dateFinished": "Apr 17, 2016 3:20:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Use the appropiate join type\r\nSome joins are more efficients than others, so try to pick the join type that fits your needs.\r\n\r\nFor instance, if you are performing a left join between \"A\" and \"B\", and then filtering where \"B\" is not null, you can just perform an inner join instead.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:21:03 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906443807_-493477322",
      "id": "20160417-152043_799641088",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eUse the appropiate join type\u003c/h4\u003e\n\u003cp\u003eSome joins are more efficients than others, so try to pick the join type that fits your needs.\u003c/p\u003e\n\u003cp\u003eFor instance, if you are performing a left join between \u0026ldquo;A\u0026rdquo; and \u0026ldquo;B\u0026rdquo;, and then filtering where \u0026ldquo;B\u0026rdquo; is not null, you can just perform an inner join instead.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:20:43 PM",
      "dateStarted": "Apr 17, 2016 3:21:00 PM",
      "dateFinished": "Apr 17, 2016 3:21:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"left_outer\")\r\n  .filter(\"b.id IS NOT NULL\")\r\n  .select($\"a.id\".as(\u0027id_a), $\"a.col_a\", $\"b.id\".as(\u0027id_b), $\"b.col_b\")\r\n  .show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:21:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906460182_1796194031",
      "id": "20160417-152100_1591866319",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----+-----+----+-----+\n|id_a|col_a|id_b|col_b|\n+----+-----+----+-----+\n|   2|    B|   2|    X|\n+----+-----+----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:21:00 PM",
      "dateStarted": "Apr 17, 2016 3:21:15 PM",
      "dateFinished": "Apr 17, 2016 3:21:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Is equivalent to this\r\na.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"inner\")\r\n  .select($\"a.id\".as(\u0027id_a), $\"a.col_a\", $\"b.id\".as(\u0027id_b), $\"b.col_b\")\r\n  .show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:21:26 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906475723_197962424",
      "id": "20160417-152115_636591439",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----+-----+----+-----+\n|id_a|col_a|id_b|col_b|\n+----+-----+----+-----+\n|   2|    B|   2|    X|\n+----+-----+----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:21:15 PM",
      "dateStarted": "Apr 17, 2016 3:21:27 PM",
      "dateFinished": "Apr 17, 2016 3:21:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md If you are performing an inner join, but just taking data from the first DataFrame, perform an left-semi join instead.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:21:43 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906486926_1758519287",
      "id": "20160417-152126_547855241",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIf you are performing an inner join, but just taking data from the first DataFrame, perform an left-semi join instead.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:21:26 PM",
      "dateStarted": "Apr 17, 2016 3:21:40 PM",
      "dateFinished": "Apr 17, 2016 3:21:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"inner\")\r\n  .select($\"a.id\", $\"a.col_a\")\r\n  .show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:21:58 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906500284_2101325100",
      "id": "20160417-152140_1142470208",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+\n| id|col_a|\n+---+-----+\n|  2|    B|\n+---+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:21:40 PM",
      "dateStarted": "Apr 17, 2016 3:21:58 PM",
      "dateFinished": "Apr 17, 2016 3:21:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Is equivalent to this\r\na.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"left_semi\")\r\n  .show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:22:08 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906518308_-452238026",
      "id": "20160417-152158_1635334103",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+\n| id|col_a|\n+---+-----+\n|  2|    B|\n+---+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:21:58 PM",
      "dateStarted": "Apr 17, 2016 3:22:08 PM",
      "dateFinished": "Apr 17, 2016 3:22:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Joining on multiple columns\r\n\r\nYou can actually join two DataFrames on multiple columns, but you should be careful with it, as it can easily create a cartesian product, especially if you use UDFs or \"range\" conditions (like \"between\") in your join columns and the DataFrame(s) are big.\r\n\r\nIn these cases, you could create a new column with the computed value on each of the DataFrames, and join on that column.\r\n",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:22:40 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906528148_1743539271",
      "id": "20160417-152208_278219289",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eJoining on multiple columns\u003c/h4\u003e\n\u003cp\u003eYou can actually join two DataFrames on multiple columns, but you should be careful with it, as it can easily create a cartesian product, especially if you use UDFs or \u0026ldquo;range\u0026rdquo; conditions (like \u0026ldquo;between\u0026rdquo;) in your join columns and the DataFrame(s) are big.\u003c/p\u003e\n\u003cp\u003eIn these cases, you could create a new column with the computed value on each of the DataFrames, and join on that column.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:22:08 PM",
      "dateStarted": "Apr 17, 2016 3:22:37 PM",
      "dateFinished": "Apr 17, 2016 3:22:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{year, month}\r\nval signupMonthActivity \u003d users.as(\u0027u)\r\n  .join(\r\n    activity.as(\u0027a),\r\n    $\"u.id\"\u003d\u003d\u003d$\"a.id\"\r\n    \u0026\u0026 year(users(\"creation_date\")) \u003d\u003d\u003d year(activity(\"timestamp\"))\r\n    \u0026\u0026 month(users(\"creation_date\")) \u003d\u003d\u003d month(activity(\"timestamp\")),\r\n    \"inner\"\r\n  )\r\nz.show(signupMonthActivity)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:50:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906557711_459109525",
      "id": "20160417-152237_868189044",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "id\tname\tcountry\tcreation_date\tid\ttype\ttimestamp\tparams\n3\tAnne\tBelgium\t2016-01-04\t3\tconfirm_email\t2016-01-05 09:23:50.0\tnull\n2\tClaire\tBelgium\t2016-01-02\t2\tconfirm_email\t2016-01-02 20:17:00.0\tnull\n2\tClaire\tBelgium\t2016-01-02\t2\tlogin\t2016-01-11 15:51:02.0\tnull\n2\tClaire\tBelgium\t2016-01-02\t2\tsearch\t2016-01-11 16:18:39.0\t{\"query\": \"helicopter\"}\n1\tJohn\tUSA\t2016-01-01\t1\tconfirm_email\t2016-01-01 10:10:46.0\tnull\n1\tJohn\tUSA\t2016-01-01\t1\tlogin\t2016-01-10 12:28:28.0\tnull\n1\tJohn\tUSA\t2016-01-01\t1\tsearch\t2016-01-10 12:30:12.0\t{\"query\": \"motorcycle\"}\n"
      },
      "dateCreated": "Apr 17, 2016 3:22:37 PM",
      "dateStarted": "Apr 30, 2016 1:50:53 PM",
      "dateFinished": "Apr 30, 2016 1:50:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// We could achieve the same results creating a new join column on each DataFrame:\r\nimport org.apache.spark.sql.functions.expr\r\nval usersWithJoinColumn \u003d users.withColumn(\"idYearMonth\", expr(\"CONCAT_WS(\u0027:::\u0027, id, YEAR(creation_date), MONTH(creation_date))\"))\r\nval activityWithJoinColumn \u003d activity.withColumn(\"idYearMonth\", expr(\"CONCAT_WS(\u0027:::\u0027, id, YEAR(timestamp), MONTH(timestamp))\"))\r\n\r\nval signupMonthActivity2 \u003d usersWithJoinColumn.as(\u0027u)\r\n  .join(activityWithJoinColumn.as(\u0027a), $\"u.idYearMonth\"\u003d\u003d\u003d$\"a.idYearMonth\", \"inner\")\r\nz.show(signupMonthActivity2)",
      "authenticationInfo": {},
      "dateUpdated": "Apr 30, 2016 1:51:34 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906573584_-909442313",
      "id": "20160417-152253_1200972381",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "id\tname\tcountry\tcreation_date\tidYearMonth\tid\ttype\ttimestamp\tparams\tidYearMonth\n2\tClaire\tBelgium\t2016-01-02\t2:::2016:::1\t2\tconfirm_email\t2016-01-02 20:17:00.0\tnull\t2:::2016:::1\n2\tClaire\tBelgium\t2016-01-02\t2:::2016:::1\t2\tlogin\t2016-01-11 15:51:02.0\tnull\t2:::2016:::1\n2\tClaire\tBelgium\t2016-01-02\t2:::2016:::1\t2\tsearch\t2016-01-11 16:18:39.0\t{\"query\": \"helicopter\"}\t2:::2016:::1\n1\tJohn\tUSA\t2016-01-01\t1:::2016:::1\t1\tconfirm_email\t2016-01-01 10:10:46.0\tnull\t1:::2016:::1\n1\tJohn\tUSA\t2016-01-01\t1:::2016:::1\t1\tlogin\t2016-01-10 12:28:28.0\tnull\t1:::2016:::1\n1\tJohn\tUSA\t2016-01-01\t1:::2016:::1\t1\tsearch\t2016-01-10 12:30:12.0\t{\"query\": \"motorcycle\"}\t1:::2016:::1\n3\tAnne\tBelgium\t2016-01-04\t3:::2016:::1\t3\tconfirm_email\t2016-01-05 09:23:50.0\tnull\t3:::2016:::1\n"
      },
      "dateCreated": "Apr 17, 2016 3:22:53 PM",
      "dateStarted": "Apr 30, 2016 1:51:34 PM",
      "dateFinished": "Apr 30, 2016 1:51:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md There is actually not much difference in terms of performance here, but our join became way more simple to read.\r\n\r\n---\r\n**Note**: *We could have used some sort of hashing function as well, but if you do use them, **beware of hash collisions**.*",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:23:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906586031_-172617521",
      "id": "20160417-152306_538528523",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThere is actually not much difference in terms of performance here, but our join became way more simple to read.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003cem\u003eWe could have used some sort of hashing function as well, but if you do use them, \u003c/em\u003e\u003cem\u003ebeware of hash collisions\u003c/em\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:23:06 PM",
      "dateStarted": "Apr 17, 2016 3:23:21 PM",
      "dateFinished": "Apr 17, 2016 3:23:21 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "println(\"Plan with multiple join columns\")\r\nsignupMonthActivity.explain\r\nprintln(\"Plan with single join column\")\r\nsignupMonthActivity2.explain",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:23:41 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906600993_-1931289419",
      "id": "20160417-152320_233571720",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Plan with multiple join columns\n\u003d\u003d Physical Plan \u003d\u003d\nSortMergeJoin [id#632,year(creation_date#635),month(creation_date#635)], [id#640,year(cast(timestamp#642 as date)),month(cast(timestamp#642 as date))]\n:- Sort [id#632 ASC,year(creation_date#635) ASC,month(creation_date#635) ASC], false, 0\n:  +- TungstenExchange hashpartitioning(id#632,year(creation_date#635),month(creation_date#635),200), None\n:     +- Project [_1#628 AS id#632,_2#629 AS name#633,_3#630 AS country#634,_4#631 AS creation_date#635]\n:        +- Scan ExistingRDD[_1#628,_2#629,_3#630,_4#631] \n+- Sort [id#640 ASC,year(cast(timestamp#642 as date)) ASC,month(cast(timestamp#642 as date)) ASC], false, 0\n   +- TungstenExchange hashpartitioning(id#640,year(cast(timestamp#642 as date)),month(cast(timestamp#642 as date)),200), None\n      +- Project [_1#636 AS id#640,_2#637 AS type#641,_3#638 AS timestamp#642,_4#639 AS params#643]\n         +- Scan ExistingRDD[_1#636,_2#637,_3#638,_4#639]\nPlan with single join column\n\u003d\u003d Physical Plan \u003d\u003d\nSortMergeJoin [idYearMonth#677], [idYearMonth#678]\n:- Sort [idYearMonth#677 ASC], false, 0\n:  +- TungstenExchange hashpartitioning(idYearMonth#677,200), None\n:     +- Project [_1#628 AS id#632,_2#629 AS name#633,_3#630 AS country#634,_4#631 AS creation_date#635,concat_ws(:::,cast(_1#628 as string),cast(year(_4#631) as string),cast(month(_4#631) as string)) AS idYearMonth#677]\n:        +- Scan ExistingRDD[_1#628,_2#629,_3#630,_4#631] \n+- Sort [idYearMonth#678 ASC], false, 0\n   +- TungstenExchange hashpartitioning(idYearMonth#678,200), None\n      +- Project [_1#636 AS id#640,_2#637 AS type#641,_3#638 AS timestamp#642,_4#639 AS params#643,concat_ws(:::,cast(_1#636 as string),cast(year(cast(_3#638 as date)) as string),cast(month(cast(_3#638 as date)) as string)) AS idYearMonth#678]\n         +- Scan ExistingRDD[_1#636,_2#637,_3#638,_4#639]\n"
      },
      "dateCreated": "Apr 17, 2016 3:23:20 PM",
      "dateStarted": "Apr 17, 2016 3:23:41 PM",
      "dateFinished": "Apr 17, 2016 3:23:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Merging DataFrames: unionAll and except\r\nSometimes, you just want to put two DataFrames with the same schema together. In that cases, you can use \"unionAll\" to get a new DataFrame with the rows of both.\r\n\r\nAnother operation you might want to do, is to remove all rows from one DataFrame that exist in another DataFrame. In that cases, you can use except. Note that this is logically equivalent to performing a left anit-join, with the difference that in the case of except, we need to have the same schema.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:24:09 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906621385_-2034771549",
      "id": "20160417-152341_2124497753",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eMerging DataFrames: unionAll and except\u003c/h3\u003e\n\u003cp\u003eSometimes, you just want to put two DataFrames with the same schema together. In that cases, you can use \u0026ldquo;unionAll\u0026rdquo; to get a new DataFrame with the rows of both.\u003c/p\u003e\n\u003cp\u003eAnother operation you might want to do, is to remove all rows from one DataFrame that exist in another DataFrame. In that cases, you can use except. Note that this is logically equivalent to performing a left anit-join, with the difference that in the case of except, we need to have the same schema.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:23:41 PM",
      "dateStarted": "Apr 17, 2016 3:24:03 PM",
      "dateFinished": "Apr 17, 2016 3:24:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### unionAll\r\nAs part of many pipelines, sometimes you calculate partial results of your output into different DataFrames. unionAll comes in handy when you need to put these together.\r\n\r\n---\r\n**IMPORTANT** When performing a union between DataFrames, the number of columns and their order is important. Make sure the columns of the different DF in the union are in the right order.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:24:30 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906643444_-1886396391",
      "id": "20160417-152403_1444067939",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eunionAll\u003c/h3\u003e\n\u003cp\u003eAs part of many pipelines, sometimes you calculate partial results of your output into different DataFrames. unionAll comes in handy when you need to put these together.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eIMPORTANT\u003c/strong\u003e When performing a union between DataFrames, the number of columns and their order is important. Make sure the columns of the different DF in the union are in the right order.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:24:03 PM",
      "dateStarted": "Apr 17, 2016 3:24:27 PM",
      "dateFinished": "Apr 17, 2016 3:24:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val rawData1 \u003d Array(\r\n  (1, \"Doe\", \"John\", 1200.0, \"x\"),\r\n  (2, \"Smith\", \"Frank\", 2798.45, \"y\")\r\n)\r\nval rawData2 \u003d Array(\r\n  (15, \"Doe\", \"Jane\", 3194.82, \"a\"),\r\n  (32, \"Johnson\", \"Patty\", 1263.08, \"d\")\r\n)\r\nval firstDf \u003d sc.parallelize(rawData1).toDF(\"id\", \"last_name\", \"first_name\", \"salary\", \"category\")\r\nval secondDf \u003d sc.parallelize(rawData2).toDF(\"id\", \"last_name\", \"first_name\", \"salary\", \"category\")\r\n\r\nfirstDf.unionAll(secondDf).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:24:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906667126_1592933885",
      "id": "20160417-152427_628639399",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "rawData1: Array[(Int, String, String, Double, String)] \u003d Array((1,Doe,John,1200.0,x), (2,Smith,Frank,2798.45,y))\nrawData2: Array[(Int, String, String, Double, String)] \u003d Array((15,Doe,Jane,3194.82,a), (32,Johnson,Patty,1263.08,d))\nfirstDf: org.apache.spark.sql.DataFrame \u003d [id: int, last_name: string, first_name: string, salary: double, category: string]\nsecondDf: org.apache.spark.sql.DataFrame \u003d [id: int, last_name: string, first_name: string, salary: double, category: string]\n+---+---------+----------+-------+--------+\n| id|last_name|first_name| salary|category|\n+---+---------+----------+-------+--------+\n|  1|      Doe|      John| 1200.0|       x|\n|  2|    Smith|     Frank|2798.45|       y|\n| 15|      Doe|      Jane|3194.82|       a|\n| 32|  Johnson|     Patty|1263.08|       d|\n+---+---------+----------+-------+--------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:24:27 PM",
      "dateStarted": "Apr 17, 2016 3:24:42 PM",
      "dateFinished": "Apr 17, 2016 3:24:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Example: Wrong order in the columns\r\n\r\nWhat would happen if our second DataFrame had a different order in the columns? Let\u0027s try it.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:24:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906682645_112435442",
      "id": "20160417-152442_698886",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eExample: Wrong order in the columns\u003c/h4\u003e\n\u003cp\u003eWhat would happen if our second DataFrame had a different order in the columns? Let\u0027s try it.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:24:42 PM",
      "dateStarted": "Apr 17, 2016 3:24:54 PM",
      "dateFinished": "Apr 17, 2016 3:24:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val thirdDf \u003d secondDf.select(\u0027first_name, \u0027salary, \u0027last_name, \u0027id, \u0027category)\r\nval firstAndThirdDf \u003d firstDf.unionAll(thirdDf)\r\nfirstAndThirdDf.show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:25:08 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906694470_313105802",
      "id": "20160417-152454_2000745220",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "thirdDf: org.apache.spark.sql.DataFrame \u003d [first_name: string, salary: double, last_name: string, id: int, category: string]\nfirstAndThirdDf: org.apache.spark.sql.DataFrame \u003d [id: string, last_name: string, first_name: string, salary: double, category: string]\n+-----+---------+----------+-------+--------+\n|   id|last_name|first_name| salary|category|\n+-----+---------+----------+-------+--------+\n|    1|      Doe|      John| 1200.0|       x|\n|    2|    Smith|     Frank|2798.45|       y|\n| Jane|  3194.82|       Doe|   15.0|       a|\n|Patty|  1263.08|   Johnson|   32.0|       d|\n+-----+---------+----------+-------+--------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:24:54 PM",
      "dateStarted": "Apr 17, 2016 3:25:08 PM",
      "dateFinished": "Apr 17, 2016 3:25:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md As you can see, the result is quite weird: The column names are those of the first DataFrame, but the types were somehow changed. Actually, the types were inferred to be the most generic types that could fit the values, that\u0027s why \"id\" is now a string, and \"salary\" (where we also had Int values), stayed as a double.\r\n\r\nSo, how to put them together, making sure that the order is the right one? We can especifically select the columns in the right order. In case the column names are identical, we can use the column names from one DataFrame to select the columns of the second in the right order.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:25:29 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906708470_2051431981",
      "id": "20160417-152508_604239881",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eAs you can see, the result is quite weird: The column names are those of the first DataFrame, but the types were somehow changed. Actually, the types were inferred to be the most generic types that could fit the values, that\u0027s why \u0026ldquo;id\u0026rdquo; is now a string, and \u0026ldquo;salary\u0026rdquo; (where we also had Int values), stayed as a double.\u003c/p\u003e\n\u003cp\u003eSo, how to put them together, making sure that the order is the right one? We can especifically select the columns in the right order. In case the column names are identical, we can use the column names from one DataFrame to select the columns of the second in the right order.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:25:08 PM",
      "dateStarted": "Apr 17, 2016 3:25:26 PM",
      "dateFinished": "Apr 17, 2016 3:25:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val firstAndThirdDfSorted \u003d firstDf.unionAll(thirdDf.selectExpr(firstDf.columns:_*))\r\nfirstAndThirdDfSorted.show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:25:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906726014_876808913",
      "id": "20160417-152526_110988399",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "firstAndThirdDfSorted: org.apache.spark.sql.DataFrame \u003d [id: int, last_name: string, first_name: string, salary: double, category: string]\n+---+---------+----------+-------+--------+\n| id|last_name|first_name| salary|category|\n+---+---------+----------+-------+--------+\n|  1|      Doe|      John| 1200.0|       x|\n|  2|    Smith|     Frank|2798.45|       y|\n| 15|      Doe|      Jane|3194.82|       a|\n| 32|  Johnson|     Patty|1263.08|       d|\n+---+---------+----------+-------+--------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:25:26 PM",
      "dateStarted": "Apr 17, 2016 3:25:42 PM",
      "dateFinished": "Apr 17, 2016 3:25:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md If the names are not all the same, one could for instance create a Scala Map with differences and change\n```\nfirstDf.columns\n```\ninto\n```\nfirstDf.columns.map(c \u003d\u003e nameMap.getOrElse(c))\n```",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:27:34 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906742529_-999211400",
      "id": "20160417-152542_467138864",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIf the names are not all the same, one could for instance create a Scala Map with differences and change\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efirstDf.columns\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003einto\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efirstDf.columns.map(c \u003d\u0026gt; nameMap.getOrElse(c))\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:25:42 PM",
      "dateStarted": "Apr 17, 2016 3:27:32 PM",
      "dateFinished": "Apr 17, 2016 3:27:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### except\r\nExcept is a logical equivalent to a left anti-join, with the extra requirement that both DataFrames need to have the same number of columns.\r\n\r\n---\r\n**IMPORTANT** Except will not fail if the order of the columns is not the same in both DataFrames, but it will not give the expected results either.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:26:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906754684_-1388010292",
      "id": "20160417-152554_1963538067",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eexcept\u003c/h4\u003e\n\u003cp\u003eExcept is a logical equivalent to a left anti-join, with the extra requirement that both DataFrames need to have the same number of columns.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eIMPORTANT\u003c/strong\u003e Except will not fail if the order of the columns is not the same in both DataFrames, but it will not give the expected results either.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:25:54 PM",
      "dateStarted": "Apr 17, 2016 3:26:10 PM",
      "dateFinished": "Apr 17, 2016 3:26:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " val guestList \u003d sc.parallelize(Array(\r\n   (1, \"Tina\"),\r\n   (2, \"James\"),\r\n   (3, \"Foo\"),\r\n   (4, \"Bar\")\r\n)).toDF(\"id\", \"name\")\r\nval blacklistedGuests \u003d sc.parallelize(Array(\r\n  (3, \"Foo\"),\r\n  (5, \"James\")\r\n)).toDF(\"id\", \"name\")\r\nguestList.except(blacklistedGuests).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:27:49 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906770466_1119044189",
      "id": "20160417-152610_1102499508",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "guestList: org.apache.spark.sql.DataFrame \u003d [id: int, name: string]\nblacklistedGuests: org.apache.spark.sql.DataFrame \u003d [id: int, name: string]\n+---+-----+\n| id| name|\n+---+-----+\n|  2|James|\n|  4|  Bar|\n|  1| Tina|\n+---+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:26:10 PM",
      "dateStarted": "Apr 17, 2016 3:27:49 PM",
      "dateFinished": "Apr 17, 2016 3:27:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "guestList.except(blacklistedGuests.select(\u0027name, \u0027id)).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:28:01 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906869598_732325572",
      "id": "20160417-152749_662727956",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+\n| id| name|\n+---+-----+\n|  3|  Foo|\n|  2|James|\n|  1| Tina|\n|  4|  Bar|\n+---+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:27:49 PM",
      "dateStarted": "Apr 17, 2016 3:28:01 PM",
      "dateFinished": "Apr 17, 2016 3:28:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Handling null values",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 7:08:43 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906881689_-867845103",
      "id": "20160417-152801_1903445673",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eHandling null values\u003c/h3\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:28:01 PM",
      "dateStarted": "Apr 17, 2016 3:28:17 PM",
      "dateFinished": "Apr 17, 2016 3:28:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Using NULL / IS NOT NULL\r\nIt is important to be aware of NULL values in your DataFrames. As with SQL, many operations involving a NULL value will return NULL. On top of that, a NULL will \"evaluated\" or at least equivalent to false inside conditions.\r\nLet\u0027s see it with an example:",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:28:34 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906897575_489395774",
      "id": "20160417-152817_829431734",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eUsing NULL / IS NOT NULL\u003c/h4\u003e\n\u003cp\u003eIt is important to be aware of NULL values in your DataFrames. As with SQL, many operations involving a NULL value will return NULL. On top of that, a NULL will \u0026ldquo;evaluated\u0026rdquo; or at least equivalent to false inside conditions.\n\u003cbr  /\u003eLet\u0027s see it with an example:\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:28:17 PM",
      "dateStarted": "Apr 17, 2016 3:28:32 PM",
      "dateFinished": "Apr 17, 2016 3:28:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val dfWithNulls \u003d sc.parallelize(Array(\r\n  (1, \"foo\", \"a\"),\r\n  (2, null, \"b\"),\r\n  (3, \"bar\", \"x\")\r\n)).toDF(\"id\", \"title\", \"subtitle\")\r\ndfWithNulls.filter(\"title \u003c\u003e \u0027bar\u0027\").show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:28:47 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906912202_2072668147",
      "id": "20160417-152832_1015344414",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "dfWithNulls: org.apache.spark.sql.DataFrame \u003d [id: int, title: string, subtitle: string]\n+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n|  1|  foo|       a|\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:28:32 PM",
      "dateStarted": "Apr 17, 2016 3:28:47 PM",
      "dateFinished": "Apr 17, 2016 3:28:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md Did you notice any missing rows? The second row, \"(2, null, b)\" should be there as well, as the title is not \"bar\", but it\u0027s not there. This happens because we have a condition like \"NULL \u003c\u003e \u0027bar\u0027\", which is evaluated to NULL, so it gets filtered out (the result of the expression is not true). This would happen even if we compare NULL to NULL:",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:29:02 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906927558_-1116500162",
      "id": "20160417-152847_1344619275",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eDid you notice any missing rows? The second row, \u0026ldquo;(2, null, b)\u0026rdquo; should be there as well, as the title is not \u0026ldquo;bar\u0026rdquo;, but it\u0027s not there. This happens because we have a condition like \u0026ldquo;NULL \u003c\u003e \u0027bar\u0027\u0026ldquo;, which is evaluated to NULL, so it gets filtered out (the result of the expression is not true). This would happen even if we compare NULL to NULL:\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:28:47 PM",
      "dateStarted": "Apr 17, 2016 3:28:59 PM",
      "dateFinished": "Apr 17, 2016 3:28:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "dfWithNulls.filter(\u0027title \u003d\u003d\u003d null).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906939512_1446327586",
      "id": "20160417-152859_511698826",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:28:59 PM",
      "dateStarted": "Apr 17, 2016 3:29:14 PM",
      "dateFinished": "Apr 17, 2016 3:29:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md There are many ways to deal with NULL values, for instance, using the \"IS NULL\" / \"IS NOT NULL\" expressions in SQL, or the \"isNull\" / \"isNotNull\" Column methods.",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:29:27 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906954134_2021758698",
      "id": "20160417-152914_323660251",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThere are many ways to deal with NULL values, for instance, using the \u0026ldquo;IS NULL\u0026rdquo; / \u0026ldquo;IS NOT NULL\u0026rdquo; expressions in SQL, or the \u0026ldquo;isNull\u0026rdquo; / \u0026ldquo;isNotNull\u0026rdquo; Column methods.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:29:14 PM",
      "dateStarted": "Apr 17, 2016 3:29:25 PM",
      "dateFinished": "Apr 17, 2016 3:29:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "dfWithNulls.filter(\"title IS NULL OR title \u003c\u003e \u0027bar\u0027\").show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:29:40 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906965022_688988509",
      "id": "20160417-152925_462449418",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n|  1|  foo|       a|\n|  2| null|       b|\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:29:25 PM",
      "dateStarted": "Apr 17, 2016 3:29:40 PM",
      "dateFinished": "Apr 17, 2016 3:29:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "dfWithNulls.filter(\u0027title.isNull).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:29:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906980327_2097584880",
      "id": "20160417-152940_1675528857",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n|  2| null|       b|\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:29:40 PM",
      "dateStarted": "Apr 17, 2016 3:29:52 PM",
      "dateFinished": "Apr 17, 2016 3:29:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Coalesce will take the first non-null value from its arguments, left to right\r\ndfWithNulls.filter(\"COALESCE(title, \u0027\u0027) \u003c\u003e \u0027bar\u0027\").show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:30:03 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460906992142_-1600606223",
      "id": "20160417-152952_756781979",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n|  1|  foo|       a|\n|  2| null|       b|\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:29:52 PM",
      "dateStarted": "Apr 17, 2016 3:30:03 PM",
      "dateFinished": "Apr 17, 2016 3:30:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### The \"\u003d\u003d\u003d\" and \"\u003c\u003d\u003e\" operators",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:30:18 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907003898_54983618",
      "id": "20160417-153003_58834832",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eThe \u0026ldquo;\u003d\u003d\u003d\u0026rdquo; and \u0026ldquo;\u003c\u003d\u003e\u0026rdquo; operators\u003c/h4\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:30:03 PM",
      "dateStarted": "Apr 17, 2016 3:30:14 PM",
      "dateFinished": "Apr 17, 2016 3:30:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md The \"\u003d\u003d\u003d\" operator, used to compare values, is not null-safe. This means that an expression like this:\r\n```\r\nmyDf.filter(\u0027col_a \u003d\u003d\u003d \u0027col_b)\r\n```\r\nWill not include rows where both col\\_a and col\\_b are NULL.\r\n",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:30:34 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907014773_-1274323832",
      "id": "20160417-153014_988364289",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThe \u0026ldquo;\u003d\u003d\u003d\u0026rdquo; operator, used to compare values, is not null-safe. This means that an expression like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emyDf.filter(\u0027col_a \u003d\u003d\u003d \u0027col_b)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWill not include rows where both col_a and col_b are NULL.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:30:14 PM",
      "dateStarted": "Apr 17, 2016 3:30:31 PM",
      "dateFinished": "Apr 17, 2016 3:30:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val myDf \u003d sc.parallelize(Array(\r\n  (\"a\", \"z\"),\r\n  (null, \"y\"),\r\n  (\"b\", \"b\"),\r\n  (null, null)\r\n)).toDF(\"col_a\", \"col_b\")\r\nmyDf.filter(\u0027col_a \u003d\u003d\u003d \u0027col_b).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:31:29 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907031327_179439758",
      "id": "20160417-153031_210715903",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myDf: org.apache.spark.sql.DataFrame \u003d [col_a: string, col_b: string]\n+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    b|    b|\n+-----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:30:31 PM",
      "dateStarted": "Apr 17, 2016 3:31:29 PM",
      "dateFinished": "Apr 17, 2016 3:31:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md The \"\u003c\u003d\u003e\" operator is null-safe, meaning that it will return TRUE if we compare two null values",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:32:03 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907089280_-1668231729",
      "id": "20160417-153129_1943375399",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThe \u0026ldquo;\u003c\u003d\u003e\u0026rdquo; operator is null-safe, meaning that it will return TRUE if we compare two null values\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:31:29 PM",
      "dateStarted": "Apr 17, 2016 3:32:00 PM",
      "dateFinished": "Apr 17, 2016 3:32:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "myDf.filter(\u0027col_a \u003c\u003d\u003e \u0027col_b).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:32:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907103790_-530883332",
      "id": "20160417-153143_1279881542",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    b|    b|\n| null| null|\n+-----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:31:43 PM",
      "dateStarted": "Apr 17, 2016 3:32:15 PM",
      "dateFinished": "Apr 17, 2016 3:32:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md So, should we always use the null-safe operator?\r\n\r\n**No** Actually it depends on the use case. NULL represents value that is not known, so in some cases, you might not want these values to be matched.\r\n\r\nLet\u0027s say we are preparing a training, that will be held in groups of two people, and that we have a list of participants, where we have the name of the participant, and his/her level for the subject. Then, we have a sofisticated algorithm that groups the participants in groups of two. Do we really want to put two people of unknown level in the same class? It could happen that both have the same level, but it could happen as well that one of them has basic knowledge, and the other is an advanced user.\r\n\r\nIn short, even though it might help in most of the cases, think in your use case before adding the null-safe operator.\r\n",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:32:30 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907135450_480667499",
      "id": "20160417-153215_536537516",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eSo, should we always use the null-safe operator?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNo\u003c/strong\u003e Actually it depends on the use case. NULL represents value that is not known, so in some cases, you might not want these values to be matched.\u003c/p\u003e\n\u003cp\u003eLet\u0027s say we are preparing a training, that will be held in groups of two people, and that we have a list of participants, where we have the name of the participant, and his/her level for the subject. Then, we have a sofisticated algorithm that groups the participants in groups of two. Do we really want to put two people of unknown level in the same class? It could happen that both have the same level, but it could happen as well that one of them has basic knowledge, and the other is an advanced user.\u003c/p\u003e\n\u003cp\u003eIn short, even though it might help in most of the cases, think in your use case before adding the null-safe operator.\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:32:15 PM",
      "dateStarted": "Apr 17, 2016 3:32:27 PM",
      "dateFinished": "Apr 17, 2016 3:32:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### DataFrameNa functions\n\nTo work with null values in DataFrames, we can use the \"na\" function, that will not return a DataFrame, but an instance of a special class, \"DataFrameNa\", that provides some methods to work on rows containing null values:\n - drop: Allows us to drop rows with null values in the DataFrame. We can use different conditions to decide which rows to drop, for instance, drop any row containing null values, or rows containing more than a certain number of nulls, rows containing null values in specific columns, etc..\n - fill: Can be used to replace null values in certain columns by whatever value we want. So, we could replace a NULL name by \"Unknown\" or a NULL salary by \"0.0\".\n - replace: With this method, we can replace values in some columns, if there are null values in the same row.\n \n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions\u003e",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:32:47 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907147129_1758049171",
      "id": "20160417-153227_1279267103",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eDataFrameNa functions\u003c/h4\u003e\n\u003cp\u003eTo work with null values in DataFrames, we can use the \u0026ldquo;na\u0026rdquo; function, that will not return a DataFrame, but an instance of a special class, \u0026ldquo;DataFrameNa\u0026rdquo;, that provides some methods to work on rows containing null values:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edrop: Allows us to drop rows with null values in the DataFrame. We can use different conditions to decide which rows to drop, for instance, drop any row containing null values, or rows containing more than a certain number of nulls, rows containing null values in specific columns, etc..\u003c/li\u003e\n\u003cli\u003efill: Can be used to replace null values in certain columns by whatever value we want. So, we could replace a NULL name by \u0026ldquo;Unknown\u0026rdquo; or a NULL salary by \u0026ldquo;0.0\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003ereplace: With this method, we can replace values in some columns, if there are null values in the same row.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 17, 2016 3:32:27 PM",
      "dateStarted": "Apr 17, 2016 3:32:44 PM",
      "dateFinished": "Apr 17, 2016 3:32:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val myDf \u003d sc.parallelize(Array(\r\n  (\"a\", \"z\"),\r\n  (null, \"y\"),\r\n  (\"b\", \"b\"),\r\n  (null, null)\r\n)).toDF(\"col_a\", \"col_b\")",
      "authenticationInfo": {},
      "dateUpdated": "Apr 23, 2016 9:17:22 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907164136_-1946851230",
      "id": "20160417-153244_1747822496",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myDf: org.apache.spark.sql.DataFrame \u003d [col_a: string, col_b: string]\n"
      },
      "dateCreated": "Apr 17, 2016 3:32:44 PM",
      "dateStarted": "Apr 23, 2016 9:17:24 PM",
      "dateFinished": "Apr 23, 2016 9:17:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//Drop all rows with null values\r\nmyDf.na.drop().show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 23, 2016 9:17:28 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907181940_-293169809",
      "id": "20160417-153301_1706410263",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    a|    z|\n|    b|    b|\n+-----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:33:01 PM",
      "dateStarted": "Apr 17, 2016 3:33:14 PM",
      "dateFinished": "Apr 17, 2016 3:33:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//Drop all rows with more than one null value\r\nmyDf.na.drop(1).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 23, 2016 9:18:44 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907194041_-1895648978",
      "id": "20160417-153314_1622711439",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    a|    z|\n| null|    y|\n|    b|    b|\n+-----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:33:14 PM",
      "dateStarted": "Apr 23, 2016 9:18:44 PM",
      "dateFinished": "Apr 23, 2016 9:18:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Drop values with nulls in col_b only\r\nmyDf.na.drop(Array(\"col_b\")).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:33:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907204939_-562848139",
      "id": "20160417-153324_771985242",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    a|    z|\n| null|    y|\n|    b|    b|\n+-----+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:33:24 PM",
      "dateStarted": "Apr 17, 2016 3:33:35 PM",
      "dateFinished": "Apr 17, 2016 3:33:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Fill null values in col_a by \"Unknown A\"\r\nmyDf.na.fill(Map(\"col_a\" -\u003e \"Unknown A\")).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:33:47 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907215202_249572972",
      "id": "20160417-153335_334321548",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---------+-----+\n|    col_a|col_b|\n+---------+-----+\n|        a|    z|\n|Unknown A|    y|\n|        b|    b|\n|Unknown A| null|\n+---------+-----+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:33:35 PM",
      "dateStarted": "Apr 17, 2016 3:33:47 PM",
      "dateFinished": "Apr 17, 2016 3:33:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// When we have a null in the row, replace value \"y\" by \"maybe_y\" in col_b\r\nmyDf.na.replace(\"col_b\", Map(\"y\"-\u003e \"maybe_y\")).show()",
      "authenticationInfo": {},
      "dateUpdated": "Apr 17, 2016 3:33:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907227058_1669696486",
      "id": "20160417-153347_1655868755",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-------+\n|col_a|  col_b|\n+-----+-------+\n|    a|      z|\n| null|maybe_y|\n|    b|      b|\n| null|   null|\n+-----+-------+\n\n"
      },
      "dateCreated": "Apr 17, 2016 3:33:47 PM",
      "dateStarted": "Apr 17, 2016 3:33:56 PM",
      "dateFinished": "Apr 17, 2016 3:33:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460907236273_-301372129",
      "id": "20160417-153356_10010990",
      "dateCreated": "Apr 17, 2016 3:33:56 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Intro to DataFrames - 2 Transformations",
  "id": "2BFMDMV28",
  "angularObjects": {
    "2BKYPESN5": [],
    "2BH1HN1YM": [],
    "2BH8FKXN5": [],
    "2BJY1F3F5": [],
    "2BGQW6CTN": [],
    "2BKMXERWY": [],
    "2BJ7YMSDX": [],
    "2BGZ7XWXF": [],
    "2BJE5NCK6": [],
    "2BH3QPUEJ": [],
    "2BMDAZN5C": [],
    "2BKY8V4U1": [],
    "2BMAQG5R9": [],
    "2BKYEJJWM": [],
    "2BJDUFZ8Y": [],
    "2BMDX1KHS": [],
    "2BKJQTF89": [],
    "2BMCV5U6S": []
  },
  "config": {},
  "info": {}
}