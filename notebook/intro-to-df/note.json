{
  "paragraphs": [
    {
      "text": "%md ### Data for the training\r\n\r\nWe\u0027ll be using a small dataset created from samples of Wikipedia clickstream data.\r\n\r\nYou can find the whole dataset here:\r\n\r\n\u003chttps://datahub.io/en/dataset/wikipedia-clickstream\u003e",
      "authenticationInfo": {},
      "dateUpdated": "Mar 26, 2016 2:17:05 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194002_698238265",
      "id": "20160326-141634_1235213350",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eData for the training\u003c/h3\u003e\n\u003cp\u003eWe\u0027ll be using a small dataset created from samples of Wikipedia clickstream data.\u003c/p\u003e\n\u003cp\u003eYou can find the whole dataset here:\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"https://datahub.io/en/dataset/wikipedia-clickstream\"\u003ehttps://datahub.io/en/dataset/wikipedia-clickstream\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "dateStarted": "Mar 26, 2016 2:17:06 PM",
      "dateFinished": "Mar 26, 2016 2:17:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val rawClickstream: Array[(Option[Int], Option[Int], Int, String, String, Option[String])] \u003d Array(\r\n\t(None, Some(3632887), 121, \"other-google\", \"!!\", Some(\"other\")),\r\n\t(None, Some(3632887), 93, \"other-wikipedia\", \"!!\", Some(\"other\")),\r\n\t(None, Some(3632887), 46, \"other-empty\", \"!!\", Some(\"other\")),\r\n\t(None, Some(3632887), 10, \"other-other\", \"!!\", Some(\"other\")),\r\n\t(Some(64486), Some(3632887), 11, \"!_(disambiguation)\", \"!!\", Some(\"other\")),\r\n\t(Some(2061699), Some(2556962), 19, \"Louden_Up_Now\", \"!!!_(album)\", Some(\"link\")),\r\n\t(None, Some(2556962), 25, \"other-empty\", \"!!!_(album)\", Some(\"other\")),\r\n\t(None, Some(2556962), 16, \"other-google\", \"!!!_(album)\", Some(\"other\")),\r\n\t(None, Some(2556962), 44, \"other-wikipedia\", \"!!!_(album)\", Some(\"other\")),\r\n\t(Some(64486), Some(2556962), 15, \"!_(disambiguation)\", \"!!!_(album)\", Some(\"link\")),\r\n\t(Some(600744), Some(2556962), 297, \"!!!\", \"!!!_(album)\", Some(\"link\")),\r\n\t(None, Some(6893310), 11, \"other-empty\", \"!Hero_(album)\", Some(\"other\")),\r\n\t(Some(1921683), Some(6893310), 26, \"!Hero\", \"!Hero_(album)\", Some(\"link\")),\r\n\t(None, Some(6893310), 16, \"other-wikipedia\", \"!Hero_(album)\", Some(\"other\")),\r\n\t(None, Some(6893310), 23, \"other-google\", \"!Hero_(album)\", Some(\"other\")),\r\n\t(Some(8127304), Some(22602473), 16, \"Jericho_Rosales\", \"!Oka_Tokat\", Some(\"link\")),\r\n\t(Some(35978874), Some(22602473), 20, \"List_of_telenovelas_of_ABS-CBN\", \"!Oka_Tokat\", Some(\"link\")),\r\n\t(None, Some(22602473), 57, \"other-google\", \"!Oka_Tokat\", Some(\"other\")),\r\n\t(None, Some(22602473), 12, \"other-wikipedia\", \"!Oka_Tokat\", Some(\"other\")),\r\n\t(None, Some(22602473), 23, \"other-empty\", \"!Oka_Tokat\", Some(\"other\")),\r\n\t(Some(7360687), Some(22602473), 10, \"Rica_Peralejo\", \"!Oka_Tokat\", Some(\"link\")),\r\n\t(Some(37104582), Some(22602473), 11, \"Jeepney_TV\", \"!Oka_Tokat\", Some(\"link\")),\r\n\t(Some(34376590), Some(22602473), 22, \"Oka_Tokat_(Some(2012)_TV_series)\", \"!Oka_Tokat\", Some(\"link\")),\r\n\t(None, Some(6810768), 20, \"other-wikipedia\", \"!T.O.O.H.!\", Some(\"other\")),\r\n\t(None, Some(6810768), 81, \"other-google\", \"!T.O.O.H.!\", Some(\"other\")),\r\n\t(Some(31976181), Some(6810768), 51, \"List_of_death_metal_bands,_!–K\", \"!T.O.O.H.!\", Some(\"link\")),\r\n\t(None, Some(6810768), 35, \"other-empty\", \"!T.O.O.H.!\", Some(\"other\")),\r\n\t(None, Some(3243047), 21, \"other-empty\", \"!_(album)\", Some(\"other\")),\r\n\t(Some(1337475), Some(3243047), 208, \"The_Dismemberment_Plan\", \"!_(album)\", Some(\"link\")),\r\n\t(Some(3284285), Some(3243047), 78, \"The_Dismemberment_Plan_Is_Terrified\", \"!_(album)\", Some(\"link\")),\r\n\t(None, Some(3243047), 28, \"other-wikipedia\", \"!_(album)\", Some(\"other\")),\r\n\t(Some(2098292), Some(899480), 58, \"United_States_military_award_devices\", \"\\\"A\\\"_Device\", Some(\"link\")),\r\n\t(Some(194844), Some(899480), 15, \"USS_Yorktown_(CV-5)\", \"\\\"A\\\"_Device\", Some(\"link\")),\r\n\t(None, Some(899480), 17, \"other-google\", \"\\\"A\\\"_Device\", Some(\"other\")),\r\n\t(None, Some(899480), 13, \"other-empty\", \"\\\"A\\\"_Device\", Some(\"other\")),\r\n\t(None, Some(899480), 29, \"other-wikipedia\", \"\\\"A\\\"_Device\", Some(\"other\")),\r\n\t(Some(878246), Some(899480), 11, \"American_Defense_Service_Medal\", \"\\\"A\\\"_Device\", Some(\"link\")),\r\n\t(Some(855901), Some(899480), 24, \"Overseas_Service_Ribbon\", \"\\\"A\\\"_Device\", Some(\"other\")),\r\n\t(Some(206427), Some(899480), 33, \"USS_Ranger_(CV-4)\", \"\\\"A\\\"_Device\", Some(\"link\")),\r\n\t(Some(773691), Some(899480), 47, \"Antarctica_Service_Medal\", \"\\\"A\\\"_Device\", Some(\"link\")),\r\n\t(Some(2301720), Some(1282996), 43, \"Kinsey_Millhone\", \"\\\"A\\\"_Is_for_Alibi\", Some(\"link\")),\r\n\t(None, Some(1282996), 45, \"other-empty\", \"\\\"A\\\"_Is_for_Alibi\", Some(\"other\")),\r\n\t(None, Some(1282996), 10, \"other-yahoo\", \"\\\"A\\\"_Is_for_Alibi\", Some(\"other\")),\r\n\t(Some(470006), Some(1282996), 207, \"Sue_Grafton\", \"\\\"A\\\"_Is_for_Alibi\", Some(\"link\")),\r\n\t(None, Some(1282996), 18, \"other-other\", \"\\\"A\\\"_Is_for_Alibi\", Some(\"other\")),\r\n\t(None, Some(1282996), 31, \"other-wikipedia\", \"\\\"A\\\"_Is_for_Alibi\", Some(\"other\")),\r\n\t(None, Some(1282996), 272, \"other-google\", \"\\\"A\\\"_Is_for_Alibi\", Some(\"other\")),\r\n\t(Some(39606873), Some(1282996), 10, \"\\\"W\\\"_Is_for_Wasted\", \"\\\"A\\\"_Is_for_Alibi\", Some(\"link\")),\r\n\t(Some(26181056), Some(9003666), 17, \"And\", \"\\\"And\\\"_theory_of_conservatism\", Some(\"link\")),\r\n\t(None, Some(9003666), 109, \"other-wikipedia\", \"\\\"And\\\"_theory_of_conservatism\", Some(\"other\")),\r\n\t(None, Some(9003666), 18, \"other-google\", \"\\\"And\\\"_theory_of_conservatism\", Some(\"other\")),\r\n\t(None, Some(39072529), 49, \"other-google\", \"\\\"Bassy\\\"_Bob_Brockmann\", Some(\"other\")),\r\n\t(None, Some(39072529), 10, \"other-other\", \"\\\"Bassy\\\"_Bob_Brockmann\", Some(\"other\")),\r\n\t(Some(11273993), None, 15, \"Colt_1851_Navy_Revolver\", \"\\\"Bigfoot\\\"_Wallace\", Some(\"redlink\")),\r\n\t(Some(12571133), Some(25033979), 12, \"\\\"V\\\"_Is_for_Vagina\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"link\")),\r\n\t(Some(113468), Some(25033979), 24, \"The_Mission\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"link\")),\r\n\t(Some(14096078), Some(25033979), 15, \"Trent_Reznor_discography\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"other\")),\r\n\t(None, Some(25033979), 42, \"other-empty\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"other\")),\r\n\t(Some(1375614), Some(25033979), 15, \"Tapeworm_(band)\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"other\")),\r\n\t(Some(159547), Some(25033979), 25, \"Milla_Jovovich\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"other\")),\r\n\t(Some(28639397), Some(25033979), 73, \"Sound_into_Blood_into_Wine\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"link\")),\r\n\t(Some(1893465), Some(25033979), 30, \"Carina_Round\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"other\")),\r\n\t(Some(33622887), Some(25033979), 10, \"Conditions_of_My_Parole\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"link\")),\r\n\t(Some(147692), Some(25033979), 25, \"Tim_Alexander\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"other\")),\r\n\t(Some(4619790), Some(25033979), 593, \"Puscifer\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"link\")),\r\n\t(None, Some(25033979), 36, \"other-wikipedia\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"other\")),\r\n\t(None, Some(25033979), 93, \"other-google\", \"\\\"C\\\"_is_for_(Please_Insert_Sophomoric_Genitalia_Reference_HERE)\", Some(\"other\")),\r\n\t(Some(69161), None, 51, \"Tết\", \"\\\"Chúc_Mừng_Năm_Mới\\\"_or_best_wishes_for_the_new_year.\", Some(\"redlink\")),\r\n\t(Some(1438509), None, 14, \"List_of_Old_West_gunfighters\", \"\\\"Cool_Hand_Conor\\\"_O\u0027Neill\", Some(\"redlink\")),\r\n\t(None, Some(331586), 6820, \"other-google\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(None, Some(331586), 20, \"other-twitter\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(None, Some(331586), 781, \"other-wikipedia\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(489033), Some(331586), 59, \"List_of_Academy_Awards_ceremonies\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(10040606), Some(331586), 38, \"List_of_Australian_films\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(2564144), Some(331586), 154, \"Crocodile_Dundee_in_Los_Angeles\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(6127928), Some(331586), 14, \"Bobby_Alto\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(152171), Some(331586), 13, \"Baz_Luhrmann\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(8078282), Some(331586), 348, \"Australia_(Some(2008)_film)\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(37386608), Some(331586), 66, \"2015_in_film\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(34557), Some(331586), 12, \"1980s\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(1118809), Some(331586), 297, \"\\\"Crocodile\\\"_Dundee_II\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(7033), Some(331586), 52, \"Caitlin_Clarke\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(72766), Some(331586), 31, \"Dundee_(disambiguation)\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(171612), Some(331586), 221, \"1986_in_film\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(2376452), Some(331586), 34, \"Australian_New_Wave\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(1248074), Some(331586), 60, \"David_Gulpilil\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(865241), Some(331586), 10, \"Crocodile_Hunter\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(196020), Some(331586), 12, \"Crocodilia\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(643649), Some(331586), 85, \"List_of_most_watched_television_broadcasts\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(8306521), Some(331586), 13, \"Anne_Carlisle\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(1448969), Some(331586), 18, \"Bart_vs._Australia\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(70209), Some(331586), 153, \"Cinema_of_Australia\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(4008173), Some(331586), 18, \"59th_Academy_Awards\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(331460), Some(331586), 17, \"Bowie_knife\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(37882), Some(331586), 21, \"Crocodile\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(44789934), Some(331586), 1283, \"Deaths_in_2015\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(22344579), Some(331586), 30, \"Academy_Award_for_Best_Original_Screenplay\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(Some(1872502), Some(331586), 10, \"Boy-Scoutz_\u0027n_the_Hood\", \"\\\"Crocodile\\\"_Dundee\", Some(\"other\")),\r\n\t(Some(5644), Some(331586), 13, \"Comedy_film\", \"\\\"Crocodile\\\"_Dundee\", Some(\"link\")),\r\n\t(None, Some(3632887), 415, \"other-empty\", \"!!\", None),\r\n\t(None, Some(3632887), 113, \"other-google\", \"!!\", None),\r\n\t(None, Some(3632887), 33, \"other-wikipedia\", \"!!\", None),\r\n\t(None, Some(600744), 25, \"other-yahoo\", \"!!!\", None),\r\n\t(None, Some(600744), 1193, \"other-google\", \"!!!\", None),\r\n\t(None, Some(600744), 1065, \"other-empty\", \"!!!\", None),\r\n\t(Some(25014178), Some(600744), 44, \"Jerry_Fuchs\", \"!!!\", None),\r\n\t(Some(34552784), Some(600744), 11, \"Flight_Facilities\", \"!!!\", None),\r\n\t(Some(4499), Some(600744), 21, \"Band\", \"!!!\", None),\r\n\t(Some(1446971), Some(600744), 14, \"Gold_Standard_Laboratories\", \"!!!\", None),\r\n\t(Some(8526330), Some(600744), 70, \"Myth_Takes\", \"!!!\", None),\r\n\t(Some(1507641), Some(600744), 139, \"LCD_Soundsystem\", \"!!!\", None),\r\n\t(Some(2064029), Some(600744), 31, \"Out_Hud\", \"!!!\", None),\r\n\t(Some(26780719), Some(600744), 40, \"List_of_post-punk_revival_bands\", \"!!!\", None),\r\n\t(Some(39256216), Some(600744), 95, \"Thr!!!er\", \"!!!\", None),\r\n\t(Some(11184232), Some(600744), 35, \"Tony_Hawk\u0027s_Proving_Ground\", \"!!!\", None),\r\n\t(Some(1327495), Some(600744), 11, \"Sziget_Festival\", \"!!!\", None),\r\n\t(Some(156725), Some(600744), 110, \"Warp_(record_label)\", \"!!!\", None),\r\n\t(Some(29631), Some(600744), 14, \"Sacramento,_California\", \"!!!\", None),\r\n\t(None, Some(600744), 136, \"other\", \"!!!\", None),\r\n\t(Some(929155), Some(600744), 31, \"Touch_and_Go_Records\", \"!!!\", None),\r\n\t(Some(40604381), Some(600744), 88, \"Grand_Theft_Auto_V_soundtrack\", \"!!!\", None),\r\n\t(Some(11108427), Some(600744), 21, \"FIFA_08\", \"!!!\", None),\r\n\t(Some(7712754), Some(600744), 14, \"Exclamation_mark\", \"!!!\", None),\r\n\t(Some(12730440), Some(600744), 33, \"Grand_Theft_Auto_IV_soundtrack\", \"!!!\", None),\r\n\t(Some(7007982), Some(600744), 11, \"New_rave\", \"!!!\", None),\r\n\t(Some(558500), Some(600744), 105, \"List_of_alternative_rock_artists\", \"!!!\", None),\r\n\t(Some(870667), Some(600744), 14, \"Pressure_Chief\", \"!!!\", None),\r\n\t(None, Some(600744), 210, \"other-wikipedia\", \"!!!\", None),\r\n\t(None, Some(600744), 17, \"other-bing\", \"!!!\", None),\r\n\t(Some(14172091), Some(600744), 11, \"St_Jerome\u0027s_Laneway_Festival\", \"!!!\", None),\r\n\t(Some(3768982), Some(600744), 22, \"Tsk_Tsk_Tsk\", \"!!!\", None),\r\n\t(Some(26762896), Some(600744), 17, \"This_Is_Happening\", \"!!!\", None),\r\n\t(Some(3259409), Some(600744), 20, \"The_Juan_MacLean\", \"!!!\", None),\r\n\t(Some(3140017), Some(600744), 20, \"Jim_Eno\", \"!!!\", None),\r\n\t(Some(40022038), Some(600744), 14, \"List_of_people_from_Sacramento,_California\", \"!!!\", None),\r\n\t(Some(2061699), Some(600744), 33, \"Louden_Up_Now\", \"!!!\", None),\r\n\t(Some(18636054), Some(600744), 15, \"Nic_Offer\", \"!!!\", None),\r\n\t(Some(15580374), Some(600744), 105, \"Main_Page\", \"!!!\", None),\r\n\t(Some(2490600), Some(600744), 105, \"James_Murphy_(electronic_musician)\", \"!!!\", None),\r\n\t(Some(1803975), Some(600744), 53, \"Dance-punk\", \"!!!\", None),\r\n\t(Some(64486), Some(600744), 63, \"!_(disambiguation)\", \"!!!\", None),\r\n\t(Some(3632887), Some(600744), 29, \"!!\", \"!!!\", None),\r\n\t(Some(2556962), Some(600744), 56, \"!!!_(album)\", \"!!!\", None),\r\n\t(None, Some(2556962), 332, \"other-empty\", \"!!!_(album)\", None),\r\n\t(Some(2061699), Some(2556962), 25, \"Louden_Up_Now\", \"!!!_(album)\", None),\r\n\t(Some(600744), Some(2556962), 328, \"!!!\", \"!!!_(album)\", None),\r\n\t(None, Some(2556962), 18, \"other-google\", \"!!!_(album)\", None),\r\n\t(None, Some(2556962), 25, \"other-wikipedia\", \"!!!_(album)\", None),\r\n\t(None, Some(6893310), 200, \"other-empty\", \"!Hero_(album)\", None),\r\n\t(None, Some(6893310), 21, \"other-google\", \"!Hero_(album)\", None),\r\n\t(Some(1921683), Some(6893310), 26, \"!Hero\", \"!Hero_(album)\", None),\r\n\t(None, Some(2516600), 432, \"other-empty\", \"!Kung_language\", None),\r\n\t(None, Some(2516600), 197, \"other-google\", \"!Kung_language\", None),\r\n\t(Some(1758827), Some(2516600), 154, \"ǃKung_people\", \"!Kung_language\", None),\r\n\t(Some(22980), Some(2516600), 74, \"Phoneme\", \"!Kung_language\", None),\r\n\t(None, Some(2516600), 20, \"other\", \"!Kung_language\", None),\r\n\t(Some(261237), Some(2516600), 21, \"The_Gods_Must_Be_Crazy\", \"!Kung_language\", None),\r\n\t(Some(247700), Some(2516600), 12, \"Xu_language\", \"!Kung_language\", None),\r\n\t(None, Some(2516600), 29, \"other-wikipedia\", \"!Kung_language\", None),\r\n\t(Some(1383618), Some(2516600), 33, \"Mama_and_papa\", \"!Kung_language\", None),\r\n\t(Some(7863678), Some(2516600), 12, \"List_of_endangered_languages_in_Africa\", \"!Kung_language\", None),\r\n\t(Some(524854), Some(2516600), 20, \"Alveolar_clicks\", \"!Kung_language\", None),\r\n\t(Some(34314219), Some(2516600), 11, \"Ekoka_!Kung\", \"!Kung_language\", None),\r\n\t(Some(27164415), Some(2516600), 100, \"Contents_of_the_Voyager_Golden_Record\", \"!Kung_language\", None),\r\n\t(Some(524853), Some(2516600), 21, \"Palatal_nasal\", \"!Kung_language\", None),\r\n\t(Some(17333), Some(2516600), 45, \"Khoisan_languages\", \"!Kung_language\", None),\r\n\t(Some(713020), Some(2516600), 56, \"Juǀ\u0027hoan_dialect\", \"!Kung_language\", None),\r\n\t(None, Some(29988427), 300, \"other-empty\", \"!Women_Art_Revolution\", None),\r\n\t(None, Some(29988427), 93, \"other-google\", \"!Women_Art_Revolution\", None),\r\n\t(None, Some(29988427), 24, \"other-wikipedia\", \"!Women_Art_Revolution\", None),\r\n\t(Some(420777), Some(29988427), 14, \"Zeitgeist_Films\", \"!Women_Art_Revolution\", None),\r\n\t(Some(6814223), Some(29988427), 23, \"Lynn_Hershman_Leeson\", \"!Women_Art_Revolution\", None),\r\n\t(Some(1686995), Some(29988427), 27, \"Carrie_Brownstein\", \"!Women_Art_Revolution\", None),\r\n\t(None, Some(64486), 650, \"other-empty\", \"!_(disambiguation)\", None),\r\n\t(None, Some(64486), 226, \"other-google\", \"!_(disambiguation)\", None),\r\n\t(None, Some(64486), 23, \"other-wikipedia\", \"!_(disambiguation)\", None),\r\n\t(Some(600744), Some(64486), 14, \"!!!\", \"!_(disambiguation)\", None),\r\n\t(Some(7712754), Some(64486), 237, \"Exclamation_mark\", \"!_(disambiguation)\", None),\r\n\t(None, Some(16250456), 145, \"other-empty\", \"\\\"B\\\"_Is_for_Burglar\", None),\r\n\t(None, Some(16250456), 104, \"other-google\", \"\\\"B\\\"_Is_for_Burglar\", None),\r\n\t(Some(470006), Some(16250456), 40, \"Sue_Grafton\", \"\\\"B\\\"_Is_for_Burglar\", None),\r\n\t(Some(2301720), Some(16250456), 17, \"Kinsey_Millhone\", \"\\\"B\\\"_Is_for_Burglar\", None),\r\n\t(Some(1282996), Some(16250456), 59, \"\\\"A\\\"_Is_for_Alibi\", \"\\\"B\\\"_Is_for_Burglar\", None),\r\n\t(None, Some(16250456), 20, \"other-wikipedia\", \"\\\"B\\\"_Is_for_Burglar\", None),\r\n\t(None, Some(1118809), 24, \"other-yahoo\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(None, Some(1118809), 895, \"other-google\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(Some(693780), Some(1118809), 399, \"Paul_Hogan\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(Some(331586), Some(1118809), 1867, \"\\\"Crocodile\\\"_Dundee\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(Some(2564144), Some(1118809), 208, \"Crocodile_Dundee_in_Los_Angeles\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(Some(70209), Some(1118809), 27, \"Cinema_of_Australia\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(Some(331460), Some(1118809), 11, \"Bowie_knife\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(Some(2259599), Some(1118809), 11, \"Susie_Essman\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(None, Some(1118809), 203, \"other-wikipedia\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(None, Some(1118809), 24, \"other-bing\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(Some(171292), Some(1118809), 111, \"1988_in_film\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(Some(19824417), Some(1118809), 15, \"List_of_action_films_of_the_1980s\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(None, Some(1118809), 27, \"other\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(Some(657547), Some(1118809), 38, \"Stephen_Root\", \"\\\"Crocodile\\\"_Dundee_II\", None),\r\n\t(None, Some(1118809), 335, \"other-empty\", \"\\\"Crocodile\\\"_Dundee_II\", None)\r\n)\r\nsc\r\n    .parallelize(rawClickstream)\r\n    .toDF(\"prev_id\", \"curr_id\", \"n\", \"prev_title\", \"curr_title\", \"type\")\r\n    .write\r\n    .mode(\"overwrite\")\r\n    .parquet(\"clickstream_df.parquet\")\r\nval clickstreamDf \u003d sqlContext.read.parquet(\"clickstream_df.parquet\")",
      "authenticationInfo": {},
      "dateUpdated": "Mar 26, 2016 2:17:21 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194005_695545022",
      "id": "20160326-141634_1241259390",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "rawClickstream: Array[(Option[Int], Option[Int], Int, String, String, Option[String])] \u003d Array((None,Some(3632887),121,other-google,!!,Some(other)), (None,Some(3632887),93,other-wikipedia,!!,Some(other)), (None,Some(3632887),46,other-empty,!!,Some(other)), (None,Some(3632887),10,other-other,!!,Some(other)), (Some(64486),Some(3632887),11,!_(disambiguation),!!,Some(other)), (Some(2061699),Some(2556962),19,Louden_Up_Now,!!!_(album),Some(link)), (None,Some(2556962),25,other-empty,!!!_(album),Some(other)), (None,Some(2556962),16,other-google,!!!_(album),Some(other)), (None,Some(2556962),44,other-wikipedia,!!!_(album),Some(other)), (Some(64486),Some(2556962),15,!_(disambiguation),!!!_(album),Some(link)), (Some(600744),Some(2556962),297,!!!,!!!_(album),Some(link)), (None,Some(6893310),11,other...clickstreamDf: org.apache.spark.sql.DataFrame \u003d [prev_id: int, curr_id: int, n: int, prev_title: string, curr_title: string, type: string]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "dateStarted": "Mar 26, 2016 2:17:22 PM",
      "dateFinished": "Mar 26, 2016 2:18:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md The end of your output should look like this:\r\nclickstreamDf: org.apache.spark.sql.DataFrame \u003d [prev_id: int, curr_id: int, n: int, prev_title: string, curr_title: string, type: string]\r\n\r\nThe DataFrame has the following schema:\r\n- `prev_id`: It will be either null or the ID of the previous article that the user visited.\r\n\r\n- `curr_id`: Id of the article that the user requested.\r\n\r\n- `prev_title`: Title of the previous article.\r\n\r\n- `curr_title`: Title of the current article\r\n\r\n- `n`: The number of occurrences of the (referer, resource) pair. We\u0027ll refer to this as number of views, but it actually represents the number of users that whent from \"prev\\_id\" to \"curr\\_id\".\r\n\r\n- `type`\r\n    - \"link\" if the referer and request are both articles and the referer links to the request\r\n    - \"redlink\" if the referer is an article and links to the request, but the request is not in the production enwiki.page table\r\n    - \"other\" if the *referer* and request are both articles but the referer does not link to the request. This can happen when clients search or spoof their refer",
      "authenticationInfo": {},
      "dateUpdated": "Mar 26, 2016 2:18:15 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194006_696699269",
      "id": "20160326-141634_1190420772",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThe end of your output should look like this:\n\u003cbr  /\u003eclickstreamDf: org.apache.spark.sql.DataFrame \u003d [prev_id: int, curr_id: int, n: int, prev_title: string, curr_title: string, type: string]\u003c/p\u003e\n\u003cp\u003eThe DataFrame has the following schema:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ccode\u003eprev_id\u003c/code\u003e: It will be either null or the ID of the previous article that the user visited.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ccode\u003ecurr_id\u003c/code\u003e: Id of the article that the user requested.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ccode\u003eprev_title\u003c/code\u003e: Title of the previous article.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ccode\u003ecurr_title\u003c/code\u003e: Title of the current article\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ccode\u003en\u003c/code\u003e: The number of occurrences of the (referer, resource) pair. We\u0027ll refer to this as number of views, but it actually represents the number of users that whent from \u0026ldquo;prev_id\u0026rdquo; to \u0026ldquo;curr_id\u0026rdquo;.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ccode\u003etype\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u0026ldquo;link\u0026rdquo; if the referer and request are both articles and the referer links to the request\u003c/li\u003e\n\u003cli\u003e\u0026ldquo;redlink\u0026rdquo; if the referer is an article and links to the request, but the request is not in the production enwiki.page table\u003c/li\u003e\n\u003cli\u003e\u0026ldquo;other\u0026rdquo; if the \u003cem\u003ereferer\u003c/em\u003e and request are both articles but the referer does not link to the request. This can happen when clients search or spoof their refer\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "dateStarted": "Mar 26, 2016 2:18:15 PM",
      "dateFinished": "Mar 26, 2016 2:18:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ## Exploring DataFrames\r\n\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame\u003e",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194007_696314520",
      "id": "20160326-141634_1444259552",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eExploring DataFrames\u003c/h2\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Showing the schema of a DataFrame\r\nThe first important thing to know when working with a new DataFrame, is its schema. In order to show it, you can use the printSchema method. This method will print the field names, types, and whether they are nullable or not.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194008_694390776",
      "id": "20160326-141634_662628423",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eShowing the schema of a DataFrame\u003c/h3\u003e\n\u003cp\u003eThe first important thing to know when working with a new DataFrame, is its schema. In order to show it, you can use the printSchema method. This method will print the field names, types, and whether they are nullable or not.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.printSchema",
      "authenticationInfo": {},
      "dateUpdated": "Mar 26, 2016 2:18:21 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194008_694390776",
      "id": "20160326-141634_2093195666",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "root\n |-- prev_id: integer (nullable \u003d true)\n |-- curr_id: integer (nullable \u003d true)\n |-- n: integer (nullable \u003d true)\n |-- prev_title: string (nullable \u003d true)\n |-- curr_title: string (nullable \u003d true)\n |-- type: string (nullable \u003d true)\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "dateStarted": "Mar 26, 2016 2:18:21 PM",
      "dateFinished": "Mar 26, 2016 2:18:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Showing rows in a DataFrame: the \"show\" action\r\nOne of the most common operations you\u0027ll perform when you explore data is, of course, showing it. By default, *show* will print the first 20 rows of the DataFrame",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194055_1945978948",
      "id": "20160326-141634_262453308",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eShowing rows in a DataFrame: the \u0026ldquo;show\u0026rdquo; action\u003c/h3\u003e\n\u003cp\u003eOne of the most common operations you\u0027ll perform when you explore data is, of course, showing it. By default, \u003cem\u003eshow\u003c/em\u003e will print the first 20 rows of the DataFrame\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.show",
      "authenticationInfo": {},
      "dateUpdated": "Mar 26, 2016 2:18:40 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194055_1945978948",
      "id": "20160326-141634_121336618",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------+--------+---+--------------------+-------------+-----+\n| prev_id| curr_id|  n|          prev_title|   curr_title| type|\n+--------+--------+---+--------------------+-------------+-----+\n|    null| 3632887|121|        other-google|           !!|other|\n|    null| 3632887| 93|     other-wikipedia|           !!|other|\n|    null| 3632887| 46|         other-empty|           !!|other|\n|    null| 3632887| 10|         other-other|           !!|other|\n|   64486| 3632887| 11|  !_(disambiguation)|           !!|other|\n| 2061699| 2556962| 19|       Louden_Up_Now|  !!!_(album)| link|\n|    null| 2556962| 25|         other-empty|  !!!_(album)|other|\n|    null| 2556962| 16|        other-google|  !!!_(album)|other|\n|    null| 2556962| 44|     other-wikipedia|  !!!_(album)|other|\n|   64486| 2556962| 15|  !_(disambiguation)|  !!!_(album)| link|\n|  600744| 2556962|297|                 !!!|  !!!_(album)| link|\n|    null| 6893310| 11|         other-empty|!Hero_(album)|other|\n| 1921683| 6893310| 26|               !Hero|!Hero_(album)| link|\n|    null| 6893310| 16|     other-wikipedia|!Hero_(album)|other|\n|    null| 6893310| 23|        other-google|!Hero_(album)|other|\n| 8127304|22602473| 16|     Jericho_Rosales|   !Oka_Tokat| link|\n|35978874|22602473| 20|List_of_telenovel...|   !Oka_Tokat| link|\n|    null|22602473| 57|        other-google|   !Oka_Tokat|other|\n|    null|22602473| 12|     other-wikipedia|   !Oka_Tokat|other|\n|    null|22602473| 23|         other-empty|   !Oka_Tokat|other|\n+--------+--------+---+--------------------+-------------+-----+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "dateStarted": "Mar 26, 2016 2:18:40 PM",
      "dateFinished": "Mar 26, 2016 2:18:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md We can show any number of values we want, just providing it as a parameter",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194056_1944055203",
      "id": "20160326-141634_1342557723",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eWe can show any number of values we want, just providing it as a parameter\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.show(18)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194056_1944055203",
      "id": "20160326-141634_756971432",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------+--------+---+--------------------+-------------+-----+\n| prev_id| curr_id|  n|          prev_title|   curr_title| type|\n+--------+--------+---+--------------------+-------------+-----+\n|    null| 3632887|121|        other-google|           !!|other|\n|    null| 3632887| 93|     other-wikipedia|           !!|other|\n|    null| 3632887| 46|         other-empty|           !!|other|\n|    null| 3632887| 10|         other-other|           !!|other|\n|   64486| 3632887| 11|  !_(disambiguation)|           !!|other|\n| 2061699| 2556962| 19|       Louden_Up_Now|  !!!_(album)| link|\n|    null| 2556962| 25|         other-empty|  !!!_(album)|other|\n|    null| 2556962| 16|        other-google|  !!!_(album)|other|\n|    null| 2556962| 44|     other-wikipedia|  !!!_(album)|other|\n|   64486| 2556962| 15|  !_(disambiguation)|  !!!_(album)| link|\n|  600744| 2556962|297|                 !!!|  !!!_(album)| link|\n|    null| 6893310| 11|         other-empty|!Hero_(album)|other|\n| 1921683| 6893310| 26|               !Hero|!Hero_(album)| link|\n|    null| 6893310| 16|     other-wikipedia|!Hero_(album)|other|\n|    null| 6893310| 23|        other-google|!Hero_(album)|other|\n| 8127304|22602473| 16|     Jericho_Rosales|   !Oka_Tokat| link|\n|35978874|22602473| 20|List_of_telenovel...|   !Oka_Tokat| link|\n|    null|22602473| 57|        other-google|   !Oka_Tokat|other|\n+--------+--------+---+--------------------+-------------+-----+\nonly showing top 18 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md Did you notice that some of the values are truncated? That\u0027s because only the first 20 characters of a value are shown. However, we can show the full values, by providing a setting the truncate argument to false",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194056_1944055203",
      "id": "20160326-141634_725732796",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eDid you notice that some of the values are truncated? That\u0027s because only the first 20 characters of a value are shown. However, we can show the full values, by providing a setting the truncate argument to false\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.show(18, truncate \u003d false)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194057_1943670454",
      "id": "20160326-141634_1831471059",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------+--------+---+------------------------------+-------------+-----+\n|prev_id |curr_id |n  |prev_title                    |curr_title   |type |\n+--------+--------+---+------------------------------+-------------+-----+\n|null    |3632887 |121|other-google                  |!!           |other|\n|null    |3632887 |93 |other-wikipedia               |!!           |other|\n|null    |3632887 |46 |other-empty                   |!!           |other|\n|null    |3632887 |10 |other-other                   |!!           |other|\n|64486   |3632887 |11 |!_(disambiguation)            |!!           |other|\n|2061699 |2556962 |19 |Louden_Up_Now                 |!!!_(album)  |link |\n|null    |2556962 |25 |other-empty                   |!!!_(album)  |other|\n|null    |2556962 |16 |other-google                  |!!!_(album)  |other|\n|null    |2556962 |44 |other-wikipedia               |!!!_(album)  |other|\n|64486   |2556962 |15 |!_(disambiguation)            |!!!_(album)  |link |\n|600744  |2556962 |297|!!!                           |!!!_(album)  |link |\n|null    |6893310 |11 |other-empty                   |!Hero_(album)|other|\n|1921683 |6893310 |26 |!Hero                         |!Hero_(album)|link |\n|null    |6893310 |16 |other-wikipedia               |!Hero_(album)|other|\n|null    |6893310 |23 |other-google                  |!Hero_(album)|other|\n|8127304 |22602473|16 |Jericho_Rosales               |!Oka_Tokat   |link |\n|35978874|22602473|20 |List_of_telenovelas_of_ABS-CBN|!Oka_Tokat   |link |\n|null    |22602473|57 |other-google                  |!Oka_Tokat   |other|\n+--------+--------+---+------------------------------+-------------+-----+\nonly showing top 18 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Counting the number of rows\r\nTo count how many rows a DataFrame has, just run myDf.count",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194057_1943670454",
      "id": "20160326-141634_1650976162",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eCounting the number of rows\u003c/h3\u003e\n\u003cp\u003eTo count how many rows a DataFrame has, just run myDf.count\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.count",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194057_1943670454",
      "id": "20160326-141634_320151847",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res19: Long \u003d 199\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md If you just want to count non-null values for one column, you can use the \"count\" function\r\n\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.functions$\u003e",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194058_1944824701",
      "id": "20160326-141634_1345556794",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIf you just want to count non-null values for one column, you can use the \u0026ldquo;count\u0026rdquo; function\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.functions$\"\u003ehttp://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.functions$\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.count\r\nclickstreamDf.select(count(\u0027prev_id)).show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194058_1944824701",
      "id": "20160326-141634_1770233848",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.functions.count\n+--------------+\n|count(prev_id)|\n+--------------+\n|           130|\n+--------------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Getting some basic information on columns\r\nSometimes, you\u0027ll want to quickly get some insights on a column. The describe method is intended for numeric columns, even though it will return some values for non-numeric columns. It is important that the \"count\" we get for the column here actually counts non-null rows, that\u0027s why it\u0027s not equal for all of the columns",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194059_1944439952",
      "id": "20160326-141634_1081837798",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eGetting some basic information on columns\u003c/h3\u003e\n\u003cp\u003eSometimes, you\u0027ll want to quickly get some insights on a column. The describe method is intended for numeric columns, even though it will return some values for non-numeric columns. It is important that the \u0026ldquo;count\u0026rdquo; we get for the column here actually counts non-null rows, that\u0027s why it\u0027s not equal for all of the columns\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.describe(\"prev_id\", \"prev_title\", \"n\").show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194059_1944439952",
      "id": "20160326-141634_1851315484",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------+--------------------+------------+------------------+\n|summary|             prev_id|  prev_title|                 n|\n+-------+--------------------+------------+------------------+\n|  count|                 130|         199|               199|\n|   mean|   7704131.223076923|        null|138.05527638190955|\n| stddev|1.1899559886419423E7|        null| 526.0674976470272|\n|    min|                4499|          !!|                10|\n|    max|            44789934|ǃKung_people|              6820|\n+-------+--------------------+------------+------------------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Sampling\r\nEven though we won\u0027t need it for this training, it can be faster to work on a small subset of the data, if you just want to make some tests. You can use the \"sample\" method for that. It takes two parameters:\r\n- withReplacement: true / false\r\n- fraction: The percentage of the data that we want to take\r\n- We can optionally add a seed, if we want to make sure we always get the same sample.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194059_1944439952",
      "id": "20160326-141634_168778205",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eSampling\u003c/h3\u003e\n\u003cp\u003eEven though we won\u0027t need it for this training, it can be faster to work on a small subset of the data, if you just want to make some tests. You can use the \u0026ldquo;sample\u0026rdquo; method for that. It takes two parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ewithReplacement: true / false\u003c/li\u003e\n\u003cli\u003efraction: The percentage of the data that we want to take\u003c/li\u003e\n\u003cli\u003eWe can optionally add a seed, if we want to make sure we always get the same sample.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.sample(withReplacement \u003d false, fraction \u003d 0.1).show(5)\r\n// Running the same again will probably show different results...\r\nclickstreamDf.sample(withReplacement \u003d false, fraction \u003d 0.1).show(5)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194060_1942516208",
      "id": "20160326-141634_1976877395",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------+--------+---+--------------------+-------------+-----+\n| prev_id| curr_id|  n|          prev_title|   curr_title| type|\n+--------+--------+---+--------------------+-------------+-----+\n|    null| 6893310| 16|     other-wikipedia|!Hero_(album)|other|\n|    null| 6893310| 23|        other-google|!Hero_(album)|other|\n|    null|22602473| 23|         other-empty|   !Oka_Tokat|other|\n| 7360687|22602473| 10|       Rica_Peralejo|   !Oka_Tokat| link|\n|34376590|22602473| 22|Oka_Tokat_(Some(2...|   !Oka_Tokat| link|\n+--------+--------+---+--------------------+-------------+-----+\nonly showing top 5 rows\n\n+-------+--------+---+--------------------+--------------------+-----+\n|prev_id| curr_id|  n|          prev_title|          curr_title| type|\n+-------+--------+---+--------------------+--------------------+-----+\n|   null| 3632887| 10|         other-other|                  !!|other|\n|   null| 2556962| 25|         other-empty|         !!!_(album)|other|\n|   null| 6893310| 11|         other-empty|       !Hero_(album)|other|\n|3284285| 3243047| 78|The_Dismemberment...|           !_(album)| link|\n| 113468|25033979| 24|         The_Mission|\"C\"_is_for_(Pleas...| link|\n+-------+--------+---+--------------------+--------------------+-----+\nonly showing top 5 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md If you want to get the same sample twice, you should use the seed parameter.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194060_1942516208",
      "id": "20160326-141634_1658875028",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIf you want to get the same sample twice, you should use the seed parameter.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.sample(withReplacement \u003d false, fraction \u003d 0.1, seed \u003d 42L).show(5)\r\n// We will get the same sample, because we use the same seed\r\nclickstreamDf.sample(withReplacement \u003d false, fraction \u003d 0.1, seed \u003d 42L).show(5)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194061_1942131459",
      "id": "20160326-141634_2079890242",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------+--------+---+--------------------+-------------+-----+\n|prev_id| curr_id|  n|          prev_title|   curr_title| type|\n+-------+--------+---+--------------------+-------------+-----+\n|   null| 2556962| 25|         other-empty|  !!!_(album)|other|\n| 600744| 2556962|297|                 !!!|  !!!_(album)| link|\n|   null| 6893310| 11|         other-empty|!Hero_(album)|other|\n|   null|22602473| 23|         other-empty|   !Oka_Tokat|other|\n| 878246|  899480| 11|American_Defense_...|   \"A\"_Device| link|\n+-------+--------+---+--------------------+-------------+-----+\nonly showing top 5 rows\n\n+-------+--------+---+--------------------+-------------+-----+\n|prev_id| curr_id|  n|          prev_title|   curr_title| type|\n+-------+--------+---+--------------------+-------------+-----+\n|   null| 2556962| 25|         other-empty|  !!!_(album)|other|\n| 600744| 2556962|297|                 !!!|  !!!_(album)| link|\n|   null| 6893310| 11|         other-empty|!Hero_(album)|other|\n|   null|22602473| 23|         other-empty|   !Oka_Tokat|other|\n| 878246|  899480| 11|American_Defense_...|   \"A\"_Device| link|\n+-------+--------+---+--------------------+-------------+-----+\nonly showing top 5 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### What\u0027s going on under the hoods?\r\nThe \"explain\" command gives information about the different plans of the DataFrame, or to put it in another way, about the different stages that will happen when we run an action over the DataFrame. By default, it will show only the Physical Plan, but we can use explain(true) if we wanted to see all of the plans.\r\n\r\nLet\u0027s compare the physical plans of the \"clickstreamDf\" another DataFrame that filters and selects some fields.\r\n\r\nYou can find a more detailed explanation on the plans and Catalyst optimizer here:\r\n\r\n\u003chttps://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\u003e",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194061_1942131459",
      "id": "20160326-141634_2123975075",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eWhat\u0027s going on under the hoods?\u003c/h3\u003e\n\u003cp\u003eThe \u0026ldquo;explain\u0026rdquo; command gives information about the different plans of the DataFrame, or to put it in another way, about the different stages that will happen when we run an action over the DataFrame. By default, it will show only the Physical Plan, but we can use explain(true) if we wanted to see all of the plans.\u003c/p\u003e\n\u003cp\u003eLet\u0027s compare the physical plans of the \u0026ldquo;clickstreamDf\u0026rdquo; another DataFrame that filters and selects some fields.\u003c/p\u003e\n\u003cp\u003eYou can find a more detailed explanation on the plans and Catalyst optimizer here:\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\"\u003ehttps://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194061_1942131459",
      "id": "20160326-141634_28421444",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nScan ParquetRelation[prev_id#48,curr_id#49,n#50,prev_title#51,curr_title#52,type#53] InputPaths: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/clickstream_df.parquet\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md The physical plan will get way more verbose quite quickly..",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194062_1943285706",
      "id": "20160326-141634_937839674",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThe physical plan will get way more verbose quite quickly..\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf\r\n  .filter(\"curr_id % 2 \u003d 0\")\r\n  .select(\"type\")\r\n  .distinct\r\n  .explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194062_1943285706",
      "id": "20160326-141634_1888915585",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nTungstenAggregate(key\u003d[type#53], functions\u003d[], output\u003d[type#53])\n+- TungstenExchange hashpartitioning(type#53,200), None\n   +- TungstenAggregate(key\u003d[type#53], functions\u003d[], output\u003d[type#53])\n      +- Project [type#53]\n         +- Filter ((curr_id#49 % 2) \u003d 0)\n            +- Scan ParquetRelation[type#53,curr_id#49] InputPaths: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/clickstream_df.parquet\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ## Transformations",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194063_1942900957",
      "id": "20160326-141634_753483494",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eTransformations\u003c/h2\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Foreword: different ways of working with expressions\r\n\r\nWhen working with DataFrames, most of the methods will accept different types of expressions:",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194063_1942900957",
      "id": "20160326-141634_86017299",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eForeword: different ways of working with expressions\u003c/h3\u003e\n\u003cp\u003eWhen working with DataFrames, most of the methods will accept different types of expressions:\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Column Expressions\r\nYou will usually find them in one of these forms:\r\n```\r\nmyDf(\"column_name\")\r\n// or\r\nnew Column(\"column_name\")\r\n```\r\n---\r\nAdditionally, all sql functions return a column as well. Further reference:\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\u003e\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003e (scroll down to find different types of transformation functions: collectsion, math, date, etc.)\r\n\r\n---\r\nThere is also an **implicit** conversion defined in http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLImplicits that will convert a Scala Symbol to a ColumnName instance.\r\n\r\nIn practice, you can treat this as if it were a regular Column, so you can use something like this:\r\n```\r\nmyDf.select(\u0027col_a, \u0027col_b)\r\n```\r\nor\r\n```\r\nmyDf.select($\"col_a\", $\"col_b\")\r\n```",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194063_1942900957",
      "id": "20160326-141634_436249862",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eColumn Expressions\u003c/h4\u003e\n\u003cp\u003eYou will usually find them in one of these forms:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emyDf(\"column_name\")\n// or\nnew Column(\"column_name\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr /\u003e\n\u003cp\u003eAdditionally, all sql functions return a column as well. Further reference:\n\u003cbr  /\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\u003c/a\u003e\n\u003cbr  /\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003c/a\u003e (scroll down to find different types of transformation functions: collectsion, math, date, etc.)\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003eThere is also an \u003cstrong\u003eimplicit\u003c/strong\u003e conversion defined in http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLImplicits that will convert a Scala Symbol to a ColumnName instance.\u003c/p\u003e\n\u003cp\u003eIn practice, you can treat this as if it were a regular Column, so you can use something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emyDf.select(\u0027col_a, \u0027col_b)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emyDf.select($\"col_a\", $\"col_b\")\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### SQL Expressions\r\nIn many cases, you can also use either strings or SQL expressions. The thing is that usually, you can not mix \"string\" or \"sql expressions\" with column expressions. Let\u0027s show it with some examples:",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194064_1953289177",
      "id": "20160326-141634_542259776",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eSQL Expressions\u003c/h4\u003e\n\u003cp\u003eIn many cases, you can also use either strings or SQL expressions. The thing is that usually, you can not mix \u0026ldquo;string\u0026rdquo; or \u0026ldquo;sql expressions\u0026rdquo; with column expressions. Let\u0027s show it with some examples:\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.Column\r\nclickstreamDf.select(\u0027prev_id, clickstreamDf(\"curr_id\"), new Column(\"n\"), $\"type\") // Works",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194064_1953289177",
      "id": "20160326-141634_1986941011",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Column\nres37: org.apache.spark.sql.DataFrame \u003d [prev_id: int, curr_id: int, n: int, type: string]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.select(\"prev_id\", \"curr_id\", \"n\") // Works",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194065_1952904428",
      "id": "20160326-141634_1146861027",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res39: org.apache.spark.sql.DataFrame \u003d [prev_id: int, curr_id: int, n: int]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.select(\"prev_id\", \u0027curr_id) // Fails",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194065_1952904428",
      "id": "20160326-141634_1627548964",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: overloaded method value select with alternatives:\n  (col: String,cols: String*)org.apache.spark.sql.DataFrame \u003cand\u003e\n  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame\n cannot be applied to (String, Symbol)\n              clickstreamDf.select(\"prev_id\", \u0027curr_id) // Fails\n                            ^\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Projections: \"select\"\r\nThe \"select\" transformation lets you take some of the columns of a DataFrame. You can do a lot more with select, like renaming columns, adding new ones, etc..",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194065_1952904428",
      "id": "20160326-141634_1716226109",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eProjections: \u0026ldquo;select\u0026rdquo;\u003c/h3\u003e\n\u003cp\u003eThe \u0026ldquo;select\u0026rdquo; transformation lets you take some of the columns of a DataFrame. You can do a lot more with select, like renaming columns, adding new ones, etc..\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.when\r\nclickstreamDf.select(\r\n  \u0027curr_id,\r\n  $\"curr_title\".as(\"title\"),\r\n  clickstreamDf(\"n\"),\r\n  when(\u0027n \u003e\u003d 100, \"Lots of visits\")\r\n  .when(\u0027n between(50, 99), \"Average visits\")\r\n  .when(\u0027n between(1, 49), \"Some visits\")\r\n  .otherwise(\"No visits at all!\")\r\n  .as(\"visits_rank\")\r\n).show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194066_1954058675",
      "id": "20160326-141634_1759351106",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.functions.when\n+--------+-------------+---+--------------+\n| curr_id|        title|  n|   visits_rank|\n+--------+-------------+---+--------------+\n| 3632887|           !!|121|Lots of visits|\n| 3632887|           !!| 93|Average visits|\n| 3632887|           !!| 46|   Some visits|\n| 3632887|           !!| 10|   Some visits|\n| 3632887|           !!| 11|   Some visits|\n| 2556962|  !!!_(album)| 19|   Some visits|\n| 2556962|  !!!_(album)| 25|   Some visits|\n| 2556962|  !!!_(album)| 16|   Some visits|\n| 2556962|  !!!_(album)| 44|   Some visits|\n| 2556962|  !!!_(album)| 15|   Some visits|\n| 2556962|  !!!_(album)|297|Lots of visits|\n| 6893310|!Hero_(album)| 11|   Some visits|\n| 6893310|!Hero_(album)| 26|   Some visits|\n| 6893310|!Hero_(album)| 16|   Some visits|\n| 6893310|!Hero_(album)| 23|   Some visits|\n|22602473|   !Oka_Tokat| 16|   Some visits|\n|22602473|   !Oka_Tokat| 20|   Some visits|\n|22602473|   !Oka_Tokat| 57|Average visits|\n|22602473|   !Oka_Tokat| 12|   Some visits|\n|22602473|   !Oka_Tokat| 23|   Some visits|\n+--------+-------------+---+--------------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Projections: \"selectExpr\"\r\nUsing selectExpr has the advantage that we can generate the new columns by using SQL expressions, which is simpler in some cases.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194066_1954058675",
      "id": "20160326-141634_711702999",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eProjections: \u0026ldquo;selectExpr\u0026rdquo;\u003c/h3\u003e\n\u003cp\u003eUsing selectExpr has the advantage that we can generate the new columns by using SQL expressions, which is simpler in some cases.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.selectExpr(\r\n  \"curr_id\",\r\n  \"curr_id IS NOT NULL AS hasCurrentId\",\r\n  \"curr_title AS title\",\r\n  \"trim(regexp_replace(curr_title, \u0027[^a-zA-Z0-9\\\\s]\u0027, \u0027 \u0027)) AS sanitized_title\",\r\n  \"n * 3 AS expected_visits\"\r\n).show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194067_1953673926",
      "id": "20160326-141634_108390403",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------+------------+-------------+---------------+---------------+\n| curr_id|hasCurrentId|        title|sanitized_title|expected_visits|\n+--------+------------+-------------+---------------+---------------+\n| 3632887|        true|           !!|               |            363|\n| 3632887|        true|           !!|               |            279|\n| 3632887|        true|           !!|               |            138|\n| 3632887|        true|           !!|               |             30|\n| 3632887|        true|           !!|               |             33|\n| 2556962|        true|  !!!_(album)|          album|             57|\n| 2556962|        true|  !!!_(album)|          album|             75|\n| 2556962|        true|  !!!_(album)|          album|             48|\n| 2556962|        true|  !!!_(album)|          album|            132|\n| 2556962|        true|  !!!_(album)|          album|             45|\n| 2556962|        true|  !!!_(album)|          album|            891|\n| 6893310|        true|!Hero_(album)|    Hero  album|             33|\n| 6893310|        true|!Hero_(album)|    Hero  album|             78|\n| 6893310|        true|!Hero_(album)|    Hero  album|             48|\n| 6893310|        true|!Hero_(album)|    Hero  album|             69|\n|22602473|        true|   !Oka_Tokat|      Oka Tokat|             48|\n|22602473|        true|   !Oka_Tokat|      Oka Tokat|             60|\n|22602473|        true|   !Oka_Tokat|      Oka Tokat|            171|\n|22602473|        true|   !Oka_Tokat|      Oka Tokat|             36|\n|22602473|        true|   !Oka_Tokat|      Oka Tokat|             69|\n+--------+------------+-------------+---------------+---------------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Adding a new column: withColumn\r\nIf you just want to append a new column to the DataFrame, you can do it using the \"withColumn\" method. This method takes two arguments: the name of the column, and the expression to calculate its value.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194067_1953673926",
      "id": "20160326-141634_651370565",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eAdding a new column: withColumn\u003c/h3\u003e\n\u003cp\u003eIf you just want to append a new column to the DataFrame, you can do it using the \u0026ldquo;withColumn\u0026rdquo; method. This method takes two arguments: the name of the column, and the expression to calculate its value.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.withColumn(\"expected_visits\", \u0027n * 3).show(5)\r\nclickstreamDf.withColumn(\"lit_column\", lit(123L)).show(5)\r\nclickstreamDf.withColumn(\"type_and_title\", concat_ws(\" -\u003e \", \u0027type, \u0027curr_title)).show(5)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194067_1953673926",
      "id": "20160326-141634_1860937196",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------+-------+---+------------------+----------+-----+---------------+\n|prev_id|curr_id|  n|        prev_title|curr_title| type|expected_visits|\n+-------+-------+---+------------------+----------+-----+---------------+\n|   null|3632887|121|      other-google|        !!|other|            363|\n|   null|3632887| 93|   other-wikipedia|        !!|other|            279|\n|   null|3632887| 46|       other-empty|        !!|other|            138|\n|   null|3632887| 10|       other-other|        !!|other|             30|\n|  64486|3632887| 11|!_(disambiguation)|        !!|other|             33|\n+-------+-------+---+------------------+----------+-----+---------------+\nonly showing top 5 rows\n\n+-------+-------+---+------------------+----------+-----+----------+\n|prev_id|curr_id|  n|        prev_title|curr_title| type|lit_column|\n+-------+-------+---+------------------+----------+-----+----------+\n|   null|3632887|121|      other-google|        !!|other|       123|\n|   null|3632887| 93|   other-wikipedia|        !!|other|       123|\n|   null|3632887| 46|       other-empty|        !!|other|       123|\n|   null|3632887| 10|       other-other|        !!|other|       123|\n|  64486|3632887| 11|!_(disambiguation)|        !!|other|       123|\n+-------+-------+---+------------------+----------+-----+----------+\nonly showing top 5 rows\n\n+-------+-------+---+------------------+----------+-----+--------------+\n|prev_id|curr_id|  n|        prev_title|curr_title| type|type_and_title|\n+-------+-------+---+------------------+----------+-----+--------------+\n|   null|3632887|121|      other-google|        !!|other|   other -\u003e !!|\n|   null|3632887| 93|   other-wikipedia|        !!|other|   other -\u003e !!|\n|   null|3632887| 46|       other-empty|        !!|other|   other -\u003e !!|\n|   null|3632887| 10|       other-other|        !!|other|   other -\u003e !!|\n|  64486|3632887| 11|!_(disambiguation)|        !!|other|   other -\u003e !!|\n+-------+-------+---+------------------+----------+-----+--------------+\nonly showing top 5 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// What articles of type \"link\" got more than 150 visits?\r\nclickstreamDf.filter(\u0027type \u003d\u003d\u003d \"link\").filter(\"n \u003e 150\").show(10)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194068_1951750181",
      "id": "20160326-141634_819618264",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------+--------+----+--------------------+--------------------+----+\n| prev_id| curr_id|   n|          prev_title|          curr_title|type|\n+--------+--------+----+--------------------+--------------------+----+\n|  600744| 2556962| 297|                 !!!|         !!!_(album)|link|\n| 1337475| 3243047| 208|The_Dismemberment...|           !_(album)|link|\n|  470006| 1282996| 207|         Sue_Grafton|    \"A\"_Is_for_Alibi|link|\n| 4619790|25033979| 593|            Puscifer|\"C\"_is_for_(Pleas...|link|\n| 2564144|  331586| 154|Crocodile_Dundee_...|  \"Crocodile\"_Dundee|link|\n| 8078282|  331586| 348|Australia_(Some(2...|  \"Crocodile\"_Dundee|link|\n| 1118809|  331586| 297|\"Crocodile\"_Dunde...|  \"Crocodile\"_Dundee|link|\n|  171612|  331586| 221|        1986_in_film|  \"Crocodile\"_Dundee|link|\n|   70209|  331586| 153| Cinema_of_Australia|  \"Crocodile\"_Dundee|link|\n|44789934|  331586|1283|      Deaths_in_2015|  \"Crocodile\"_Dundee|link|\n+--------+--------+----+--------------------+--------------------+----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Warning: Filter and null values\r\nYou should be careful about null values when filterering. As happens with other SQL languages, most operations involving NULL values will return NULL. Let\u0027s see some examples. For instance, we\u0027ll try to to select this row filtering on prev_id:\r\n```\r\n+-------+-------+---+-----------+----------+-----+\r\n|prev_id|curr_id|  n| prev_title|curr_title| type|\r\n+-------+-------+---+-----------+----------+-----+\r\n|   null|3632887| 46|other-empty|        !!|other|\r\n+-------+-------+---+-----------+----------+-----+\r\n```",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194068_1951750181",
      "id": "20160326-141634_1364051190",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eWarning: Filter and null values\u003c/h3\u003e\n\u003cp\u003eYou should be careful about null values when filterering. As happens with other SQL languages, most operations involving NULL values will return NULL. Let\u0027s see some examples. For instance, we\u0027ll try to to select this row filtering on prev_id:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e+-------+-------+---+-----------+----------+-----+\n|prev_id|curr_id|  n| prev_title|curr_title| type|\n+-------+-------+---+-----------+----------+-----+\n|   null|3632887| 46|other-empty|        !!|other|\n+-------+-------+---+-----------+----------+-----+\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Without filtering on prev_id - Works\r\nclickstreamDf.filter(\"curr_id \u003d 3632887 AND n \u003d 46 AND prev_title \u003d \u0027other-empty\u0027 AND curr_title \u003d \u0027!!\u0027 AND type \u003d \u0027other\u0027\").show(5)\r\n// Filtering on prev_id - Doesn\u0027t work\r\nclickstreamDf.filter(\"curr_id \u003d 3632887 AND n \u003d 46 AND prev_title \u003d \u0027other-empty\u0027 AND curr_title \u003d \u0027!!\u0027 AND type \u003d \u0027other\u0027 AND prev_id \u003c\u003e 600744\").show(5)\r\n// NULL \u003c\u003e 600744 is NULL, which evaluates to false, that\u0027s why the condition is not met... We can add an extra check to make it work\r\nclickstreamDf.filter(\"curr_id \u003d 3632887 AND n \u003d 46 AND prev_title \u003d \u0027other-empty\u0027 AND curr_title \u003d \u0027!!\u0027 AND type \u003d \u0027other\u0027 AND (prev_id IS NULL OR prev_id \u003c\u003e 600744)\").show(5)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194069_1951365432",
      "id": "20160326-141634_1220418605",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------+-------+---+-----------+----------+-----+\n|prev_id|curr_id|  n| prev_title|curr_title| type|\n+-------+-------+---+-----------+----------+-----+\n|   null|3632887| 46|other-empty|        !!|other|\n+-------+-------+---+-----------+----------+-----+\n\n+-------+-------+---+----------+----------+----+\n|prev_id|curr_id|  n|prev_title|curr_title|type|\n+-------+-------+---+----------+----------+----+\n+-------+-------+---+----------+----------+----+\n\n+-------+-------+---+-----------+----------+-----+\n|prev_id|curr_id|  n| prev_title|curr_title| type|\n+-------+-------+---+-----------+----------+-----+\n|   null|3632887| 46|other-empty|        !!|other|\n+-------+-------+---+-----------+----------+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Using UDFs (User-Defined functions)\r\nYou can use your own functions to generate new columns, filter, etc. You just need to wrap them inside a \"udf\" block.\r\n\r\n**Note**: At the time of this writing, UDFs are limited to a maximum of 22 variables (cf. \u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.UDFRegistration\u003e). ",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194069_1951365432",
      "id": "20160326-141634_959181738",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eUsing UDFs (User-Defined functions)\u003c/h3\u003e\n\u003cp\u003eYou can use your own functions to generate new columns, filter, etc. You just need to wrap them inside a \u0026ldquo;udf\u0026rdquo; block.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: At the time of this writing, UDFs are limited to a maximum of 22 variables (cf. \u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.UDFRegistration\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.UDFRegistration\u003c/a\u003e).\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val myFunction \u003d (numViews: Int, articleType: String, title: String) \u003d\u003e s\"[${articleType}][${numViews} views]: ${title}\"\r\nval myUdf \u003d udf { myFunction }\r\nclickstreamDf.select(\u0027n, \u0027type, \u0027curr_title, myUdf(\u0027n, \u0027type, \u0027curr_title).as(\"row_str\")).show(10, truncate \u003d false)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194069_1951365432",
      "id": "20160326-141634_1955216184",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myFunction: (Int, String, String) \u003d\u003e String \u003d \u003cfunction3\u003e\nmyUdf: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction3\u003e,StringType,List(IntegerType, StringType, StringType))\n+---+-----+-----------+------------------------------+\n|n  |type |curr_title |row_str                       |\n+---+-----+-----------+------------------------------+\n|121|other|!!         |[other][121 views]: !!        |\n|93 |other|!!         |[other][93 views]: !!         |\n|46 |other|!!         |[other][46 views]: !!         |\n|10 |other|!!         |[other][10 views]: !!         |\n|11 |other|!!         |[other][11 views]: !!         |\n|19 |link |!!!_(album)|[link][19 views]: !!!_(album) |\n|25 |other|!!!_(album)|[other][25 views]: !!!_(album)|\n|16 |other|!!!_(album)|[other][16 views]: !!!_(album)|\n|44 |other|!!!_(album)|[other][44 views]: !!!_(album)|\n|15 |link |!!!_(album)|[link][15 views]: !!!_(album) |\n+---+-----+-----------+------------------------------+\nonly showing top 10 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Working with grouped data.\r\nWhen we want to perform operations on groups of data, we can use \"groupBy\" in combination with the different aggregated functions.\r\n\r\nFor instance, how many visits did we get for every type?\r\n\r\n---\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData\u003e\r\n\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003e",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194070_1952519679",
      "id": "20160326-141634_523638094",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eWorking with grouped data.\u003c/h3\u003e\n\u003cp\u003eWhen we want to perform operations on groups of data, we can use \u0026ldquo;groupBy\u0026rdquo; in combination with the different aggregated functions.\u003c/p\u003e\n\u003cp\u003eFor instance, how many visits did we get for every type?\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "clickstreamDf.groupBy(\u0027type).sum(\"n\").show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194070_1952519679",
      "id": "20160326-141634_397460452",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------+------+\n|   type|sum(n)|\n+-------+------+\n|  other|  9493|\n|redlink|    80|\n|   link|  4755|\n|   null| 13145|\n+-------+------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// A DF is transformed into a GroupedData object by its groupBy method\r\nval clickstreamGroupedByPrevId \u003d clickstreamDf.groupBy(\u0027prev_id)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194071_1952134930",
      "id": "20160326-141634_1443674594",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "clickstreamGroupedByPrevId: org.apache.spark.sql.GroupedData \u003d org.apache.spark.sql.GroupedData@7cb53e05\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// What are the average visits per prev_id?\r\nclickstreamGroupedByPrevId.avg(\"n\").show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194071_1952134930",
      "id": "20160326-141634_1568657552",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------+------+\n| prev_id|avg(n)|\n+--------+------+\n|   29631|  14.0|\n|11184232|  35.0|\n|  489033|  59.0|\n|    7033|  52.0|\n|  261237|  21.0|\n|40022038|  14.0|\n|12730440|  33.0|\n|  865241|  10.0|\n| 1507641| 139.0|\n|  194844|  15.0|\n|    5644|  13.0|\n|  878246|  11.0|\n|  643649|  85.0|\n| 2376452|  34.0|\n|  524853|  21.0|\n|18636054|  15.0|\n|  524854|  20.0|\n|26181056|  17.0|\n|  331460|  14.0|\n| 1893465|  30.0|\n+--------+------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Putting it all together:\r\nimport org.apache.spark.sql.functions.{avg, sum}\r\nval groupedClickstreamDf \u003d clickstreamDf.groupBy(\u0027type).agg(sum(\"n\").as(\"total_views\"), avg(\"n\").as(\"average_views\"))\r\ngroupedClickstreamDf.show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194071_1952134930",
      "id": "20160326-141634_1918225892",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.functions.{avg, sum}\ngroupedClickstreamDf: org.apache.spark.sql.DataFrame \u003d [type: string, total_views: bigint, average_views: double]\n+-------+-----------+------------------+\n|   type|total_views|     average_views|\n+-------+-----------+------------------+\n|  other|       9493| 175.7962962962963|\n|redlink|         80|26.666666666666668|\n|   link|       4755|113.21428571428571|\n|   null|      13145|            131.45|\n+-------+-----------+------------------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// here is how the physical plan looks like\r\ngroupedClickstreamDf.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194072_1950211186",
      "id": "20160326-141634_1598058080",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nTungstenAggregate(key\u003d[type#53], functions\u003d[(sum(cast(n#50 as bigint)),mode\u003dFinal,isDistinct\u003dfalse),(avg(cast(n#50 as bigint)),mode\u003dFinal,isDistinct\u003dfalse)], output\u003d[type#53,total_views#230L,average_views#231])\n+- TungstenExchange hashpartitioning(type#53,200), None\n   +- TungstenAggregate(key\u003d[type#53], functions\u003d[(sum(cast(n#50 as bigint)),mode\u003dPartial,isDistinct\u003dfalse),(avg(cast(n#50 as bigint)),mode\u003dPartial,isDistinct\u003dfalse)], output\u003d[type#53,sum#246L,sum#247,count#248L])\n      +- Scan ParquetRelation[type#53,n#50] InputPaths: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/clickstream_df.parquet\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Example: Group by \"buckets\"\r\nLet\u0027s assume that we want to group based on the number of views, in these buckets:\r\n- [0, 80]: \"Low\"\r\n- [80, 160]: \"Average\"\r\n- [160, ]: \"High\"",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194072_1950211186",
      "id": "20160326-141634_2131661178",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eExample: Group by \u0026ldquo;buckets\u0026rdquo;\u003c/h4\u003e\n\u003cp\u003eLet\u0027s assume that we want to group based on the number of views, in these buckets:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e[0, 80]: \u0026ldquo;Low\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e[80, 160]: \u0026ldquo;Average\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e[160, ]: \u0026ldquo;High\u0026rdquo;\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.when\r\nval myBuckets \u003d when(\u0027n between (0, 80), \"Low\")\r\n  .when(\u0027n between (80, 160), \"Average\")\r\n  .otherwise(\"High\")\r\n  .as(\"views\")\r\nclickstreamDf.groupBy(myBuckets).count.show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194073_1949826437",
      "id": "20160326-141634_359735745",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.functions.when\nmyBuckets: org.apache.spark.sql.Column \u003d CASE WHEN ((n \u003e\u003d 0) \u0026\u0026 (n \u003c\u003d 80)) THEN Low WHEN ((n \u003e\u003d 80) \u0026\u0026 (n \u003c\u003d 160)) THEN Average ELSE High AS views#249\n+-------+-----+\n|  views|count|\n+-------+-----+\n|   High|   30|\n|Average|   23|\n|    Low|  146|\n+-------+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### User-Defined Aggregated Functions (UDAFs)\r\nLike for UDFs, you can write your own functions for aggregations. However, it is not as simple as using UDFs: You will need to create your own class. Let\u0027s create a UDAF that will generate an array from the grouped data.\r\n\r\n---\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row\u003e",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194073_1949826437",
      "id": "20160326-141634_912199910",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eUser-Defined Aggregated Functions (UDAFs)\u003c/h3\u003e\n\u003cp\u003eLike for UDFs, you can write your own functions for aggregations. However, it is not as simple as using UDFs: You will need to create your own class. Let\u0027s create a UDAF that will generate an array from the grouped data.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\r\nimport org.apache.spark.sql.Row\r\nimport scala.collection.mutable.ArrayBuffer\r\nimport org.apache.spark.sql.types.{ArrayType, StringType, StructType}\r\n\r\nclass GroupToSet extends UserDefinedAggregateFunction {\r\n    def inputSchema \u003d new StructType().add(\"x\", StringType)\r\n    def bufferSchema \u003d new StructType().add(\"buff\", ArrayType(StringType))\r\n    def dataType \u003d ArrayType(StringType)\r\n    def deterministic \u003d true\r\n\r\n    def initialize(buffer: MutableAggregationBuffer) \u003d {\r\n        buffer.update(0, ArrayBuffer.empty[String])\r\n    }\r\n\r\n    def update(buffer: MutableAggregationBuffer, input: Row) \u003d {\r\n        if (!input.isNullAt(0))\r\n            buffer.update(0, buffer.getSeq[String](0) :+ input.getString(0))\r\n    }\r\n\r\n    def merge(buffer1: MutableAggregationBuffer, buffer2: Row) \u003d {\r\n        buffer1.update(0, buffer1.getSeq[String](0) ++ buffer2.getSeq[String](0))\r\n    }\r\n\r\n    def evaluate(buffer: Row) \u003d buffer.getSeq[String](0).sorted.distinct\r\n}",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194073_1949826437",
      "id": "20160326-141634_458131437",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\nimport org.apache.spark.sql.Row\nimport scala.collection.mutable.ArrayBuffer\nimport org.apache.spark.sql.types.{ArrayType, StringType, StructType}\ndefined class GroupToSet\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val groupToSet \u003d new GroupToSet()\r\nclickstreamDf.groupBy(\u0027curr_id).agg(groupToSet(\u0027prev_id).as(\u0027prev_ids)).show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194074_1950980684",
      "id": "20160326-141634_1086187164",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "groupToSet: GroupToSet \u003d $iwC$$iwC$GroupToSet@467db113\n+--------+--------------------+\n| curr_id|            prev_ids|\n+--------+--------------------+\n| 3243047|  [1337475, 3284285]|\n|16250456|[1282996, 2301720...|\n| 9003666|          [26181056]|\n|22602473|[34376590, 359788...|\n|  899480|[194844, 206427, ...|\n|   64486|   [600744, 7712754]|\n| 3632887|             [64486]|\n| 6893310|           [1921683]|\n|39072529|                  []|\n|  600744|[11108427, 111842...|\n| 2556962|[2061699, 600744,...|\n| 6810768|          [31976181]|\n|25033979|[113468, 12571133...|\n|  331586|[10040606, 111880...|\n| 1282996|[2301720, 3960687...|\n|    null|[11273993, 143850...|\n| 2516600|[1383618, 17333, ...|\n| 1118809|[171292, 19824417...|\n|29988427|[1686995, 420777,...|\n+--------+--------------------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Window functions\r\nWindow functions come in handy when you want to perform operations on groups of data, but still want to keep the whole DataFrame.\r\n\r\nFollowing up on the example before, let´s assume that we want to know, for each \"prev\\_id\", the weight or share for each of the types. We want to get something like this:\r\n\r\n```\r\n+-------+-------------+------+\r\n|prev_id|         type| share|\r\n+-------+-------------+------+\r\n|   1234|         link|   0.5|\r\n|   1234|        other|  0.25|\r\n|   1234|      redlink|  0.25|\r\n+-------+-------------+------+\r\n```\r\n\r\n\r\nTo calculate this \"share\", we need to calculate two numbers: the total count for each prev\\_id (countPrevId), and the total count for each prev\\_id - type pair (countPrevIdType). From that, we can get the share by calculating countPrevIdType / countPrevId.\r\n\r\n---\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window\u003e\r\n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003e (search \"window\" among methods)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194074_1950980684",
      "id": "20160326-141634_1936665848",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eWindow functions\u003c/h3\u003e\n\u003cp\u003eWindow functions come in handy when you want to perform operations on groups of data, but still want to keep the whole DataFrame.\u003c/p\u003e\n\u003cp\u003eFollowing up on the example before, let´s assume that we want to know, for each \u0026ldquo;prev_id\u0026rdquo;, the weight or share for each of the types. We want to get something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e+-------+-------------+------+\n|prev_id|         type| share|\n+-------+-------------+------+\n|   1234|         link|   0.5|\n|   1234|        other|  0.25|\n|   1234|      redlink|  0.25|\n+-------+-------------+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo calculate this \u0026ldquo;share\u0026rdquo;, we need to calculate two numbers: the total count for each prev_id (countPrevId), and the total count for each prev_id - type pair (countPrevIdType). From that, we can get the share by calculating countPrevIdType / countPrevId.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window\u003c/a\u003e\n\u003cbr  /\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\u003c/a\u003e (search \u0026ldquo;window\u0026rdquo; among methods)\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Long.MinValue and Long.MaxValue will make our Window unbounded on the lower / upper side.\r\nimport org.apache.spark.sql.expressions.Window\r\nimport org.apache.spark.sql.functions.lit\r\nval prevIdWindow \u003d Window.partitionBy(\u0027prev_id).orderBy(\u0027type).rowsBetween(Long.MinValue, Long.MaxValue)\r\nval prevIdTypeWindow \u003d Window.partitionBy(\u0027prev_id, \u0027type).orderBy(lit(1)).rowsBetween(Long.MinValue, Long.MaxValue)\r\nclickstreamDf.groupBy(\u0027prev_id, \u0027type).count.select(\r\n  \u0027prev_id,\r\n  \u0027type,\r\n  (sum(\"count\").over(prevIdTypeWindow) / sum(\"count\").over(prevIdWindow)).as(\u0027share)\r\n).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194075_1950595935",
      "id": "20160326-141634_133269757",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions.lit\nprevIdWindow: org.apache.spark.sql.expressions.WindowSpec \u003d org.apache.spark.sql.expressions.WindowSpec@58812ce5\nprevIdTypeWindow: org.apache.spark.sql.expressions.WindowSpec \u003d org.apache.spark.sql.expressions.WindowSpec@40308cfd\n+--------+-----+-----+\n| prev_id| type|share|\n+--------+-----+-----+\n|   29631| null|  1.0|\n|11184232| null|  1.0|\n|    7033|other|  1.0|\n|  489033| link|  1.0|\n|  261237| null|  1.0|\n|40022038| null|  1.0|\n|12730440| null|  1.0|\n|  865241|other|  1.0|\n| 1507641| null|  1.0|\n|    5644| link|  1.0|\n|  194844| link|  1.0|\n|  878246| link|  1.0|\n|  643649| link|  1.0|\n| 2376452|other|  1.0|\n|  524853| null|  1.0|\n|  524854| null|  1.0|\n|18636054| null|  1.0|\n|26181056| link|  1.0|\n|  331460| null|  0.5|\n|  331460| link|  0.5|\n+--------+-----+-----+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Window Example 2 - Calculate accumulated value.\r\nFor this example, we\u0027ll create a new DataFrame with expenses data, this will be the structure:\r\n- Name\r\n- Date\r\n- Amount\r\n\r\nWe want to generate the accumulated expenses for every person, ordered by date.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194075_1950595935",
      "id": "20160326-141634_39362078",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eWindow Example 2 - Calculate accumulated value.\u003c/h4\u003e\n\u003cp\u003eFor this example, we\u0027ll create a new DataFrame with expenses data, this will be the structure:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eName\u003c/li\u003e\n\u003cli\u003eDate\u003c/li\u003e\n\u003cli\u003eAmount\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe want to generate the accumulated expenses for every person, ordered by date.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.sql.{Date \u003d\u003e JavaDate}\r\nval rawData \u003d Array(\r\n  (\"John\", JavaDate.valueOf(\"2016-03-10\"), 123),\r\n  (\"Claire\", JavaDate.valueOf(\"2016-01-25\"), 32),\r\n  (\"John\", JavaDate.valueOf(\"2016-01-09\"), 423),\r\n  (\"Tim\", JavaDate.valueOf(\"2015-11-23\"), 13),\r\n  (\"Claire\", JavaDate.valueOf(\"2016-02-03\"), 319),\r\n  (\"Tim\", JavaDate.valueOf(\"2016-03-07\"), 38),\r\n  (\"John\", JavaDate.valueOf(\"2016-02-27\"), 112),\r\n  (\"Claire\", JavaDate.valueOf(\"2016-01-29\"), 81),\r\n  (\"John\", JavaDate.valueOf(\"2016-01-01\"), 55),\r\n  (\"Claire\", JavaDate.valueOf(\"2016-01-02\"), 71),\r\n  (\"Tim\", JavaDate.valueOf(\"2016-02-25\"), 193)\r\n)\r\nval expensesDf \u003d sc.parallelize(rawData).toDF(\"name\", \"date\", \"amount\")\r\nval expensesWindow \u003d Window.partitionBy(\u0027name).orderBy(\u0027date).rowsBetween(Long.MinValue, 0)\r\nval expensesRunSum \u003d expensesDf.select(\r\n  \u0027name,\r\n  \u0027date,\r\n  \u0027amount,\r\n  sum(\u0027amount).over(expensesWindow).as(\"accumulated_amount\")\r\n)\r\n.orderBy(\u0027name, \u0027date)\r\nexpensesRunSum.show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194075_1950595935",
      "id": "20160326-141634_133481670",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.sql.{Date\u003d\u003eJavaDate}\nrawData: Array[(String, java.sql.Date, Int)] \u003d Array((John,2016-03-10,123), (Claire,2016-01-25,32), (John,2016-01-09,423), (Tim,2015-11-23,13), (Claire,2016-02-03,319), (Tim,2016-03-07,38), (John,2016-02-27,112), (Claire,2016-01-29,81), (John,2016-01-01,55), (Claire,2016-01-02,71), (Tim,2016-02-25,193))\nexpensesDf: org.apache.spark.sql.DataFrame \u003d [name: string, date: date, amount: int]\nexpensesWindow: org.apache.spark.sql.expressions.WindowSpec \u003d org.apache.spark.sql.expressions.WindowSpec@60dbd013\nexpensesRunSum: org.apache.spark.sql.DataFrame \u003d [name: string, date: date, amount: int, accumulated_amount: bigint]\n+------+----------+------+------------------+\n|  name|      date|amount|accumulated_amount|\n+------+----------+------+------------------+\n|Claire|2016-01-02|    71|                71|\n|Claire|2016-01-25|    32|               103|\n|Claire|2016-01-29|    81|               184|\n|Claire|2016-02-03|   319|               503|\n|  John|2016-01-01|    55|                55|\n|  John|2016-01-09|   423|               478|\n|  John|2016-02-27|   112|               590|\n|  John|2016-03-10|   123|               713|\n|   Tim|2015-11-23|    13|                13|\n|   Tim|2016-02-25|   193|               206|\n|   Tim|2016-03-07|    38|               244|\n+------+----------+------+------------------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// physical plan?\r\nexpensesRunSum.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194076_1948672190",
      "id": "20160326-141634_1453500222",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nSort [name#280 ASC,date#281 ASC], true, 0\n+- ConvertToUnsafe\n   +- Exchange rangepartitioning(name#280 ASC,date#281 ASC,200), None\n      +- Window [name#280,date#281,amount#282], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum(amount#282) windowspecdefinition(name#280,date#281 ASC,ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS accumulated_amount#283L], [name#280], [date#281 ASC]\n         +- Sort [name#280 ASC,date#281 ASC], false, 0\n            +- TungstenExchange hashpartitioning(name#280,200), None\n               +- Project [_1#277 AS name#280,_2#278 AS date#281,_3#279 AS amount#282]\n                  +- Scan ExistingRDD[_1#277,_2#278,_3#279]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Joins\r\n\r\nJoining datasets is another of the basic operations you\u0027ll need when working with data. These are the join types we\u0027ll be showing:\r\n\r\nLet\u0027s assume we have two DataFrames, A and B, and that we join based on column \"some\\_id\"\r\n- Inner join: Get rows that exist both in A and B.\r\n- Left join: Get all rows from A, getting also information from B, if it exists.\r\n- Right join: The same as before, but this time we get all rows from B, and get information from A if it exists\r\n- Left semi-join: Get only rows from A, but rows that also exist in B. We will not be able to get any information from rows in B.\r\n- Left anti-join: Get rows from A that do not exist in B.\r\n- Outer join: Get all rows from A and all rows from B, whether it exists in both or not.\r\n- Cross join: Perform a cartesian product of A and B: perform all of the combinations of rows in A with rows in B.\r\n\r\n\r\nFirst, let\u0027s create the our datasets",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194076_1948672190",
      "id": "20160326-141634_1351746529",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eJoins\u003c/h3\u003e\n\u003cp\u003eJoining datasets is another of the basic operations you\u0027ll need when working with data. These are the join types we\u0027ll be showing:\u003c/p\u003e\n\u003cp\u003eLet\u0027s assume we have two DataFrames, A and B, and that we join based on column \u0026ldquo;some_id\u0026rdquo;\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInner join: Get rows that exist both in A and B.\u003c/li\u003e\n\u003cli\u003eLeft join: Get all rows from A, getting also information from B, if it exists.\u003c/li\u003e\n\u003cli\u003eRight join: The same as before, but this time we get all rows from B, and get information from A if it exists\u003c/li\u003e\n\u003cli\u003eLeft semi-join: Get only rows from A, but rows that also exist in B. We will not be able to get any information from rows in B.\u003c/li\u003e\n\u003cli\u003eLeft anti-join: Get rows from A that do not exist in B.\u003c/li\u003e\n\u003cli\u003eOuter join: Get all rows from A and all rows from B, whether it exists in both or not.\u003c/li\u003e\n\u003cli\u003eCross join: Perform a cartesian product of A and B: perform all of the combinations of rows in A with rows in B.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFirst, let\u0027s create the our datasets\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.sql.{Date \u003d\u003e JavaDate}\r\n\r\nval rawUsers \u003d Array(\r\n  (1, \"John\", \"USA\", JavaDate.valueOf(\"2016-01-01\")),\r\n  (2, \"Claire\", \"Belgium\", JavaDate.valueOf(\"2016-01-02\")),\r\n  (3, \"Anne\", \"Belgium\", JavaDate.valueOf(\"2016-01-04\")),\r\n  (4, \"Tom\", \"France\", JavaDate.valueOf(\"2016-02-04\"))\r\n)\r\nval users \u003d sc.parallelize(rawUsers).toDF(\"id\", \"name\", \"country\", \"creation_date\")\r\nval dateFormat \u003d new java.text.SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\r\nval toTimestamp \u003d (strDate: String) \u003d\u003e new java.sql.Timestamp(dateFormat.parse(strDate).getTime)\r\n\r\nval rawActivity \u003d Array(\r\n  (1, \"confirm_email\", toTimestamp(\"2016-01-01 10:10:46\"), null),\r\n  (2, \"confirm_email\", toTimestamp(\"2016-01-02 20:17:00\"), null),\r\n  (3, \"confirm_email\", toTimestamp(\"2016-01-05 09:23:50\"), null),\r\n  (1, \"login\", toTimestamp(\"2016-01-10 12:28:28\"), null),\r\n  (1, \"search\", toTimestamp(\"2016-01-10 12:30:12\"), \"{\\\"query\\\": \\\"motorcycle\\\"}\"),\r\n  (1, \"login\", toTimestamp(\"2016-02-11 10:05:56\"), null),\r\n  (1, \"search\", toTimestamp(\"2016-02-11 10:10:46\"), \"{\\\"query\\\": \\\"car\\\"}\"),\r\n  (1, \"purchase\", toTimestamp(\"2016-02-11 13:47:28\"), \"{product: \\\"Some Fancy Car\\\"\"),\r\n  (2, \"login\", toTimestamp(\"2016-01-11 15:51:02\"), null),\r\n  (2, \"search\", toTimestamp(\"2016-01-11 16:18:39\"), \"{\\\"query\\\": \\\"helicopter\\\"}\")\r\n)\r\nval activity \u003d sc.parallelize(rawActivity).toDF(\"id\", \"type\", \"timestamp\", \"params\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194076_1948672190",
      "id": "20160326-141634_574411290",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.sql.{Date\u003d\u003eJavaDate}\nrawUsers: Array[(Int, String, String, java.sql.Date)] \u003d Array((1,John,USA,2016-01-01), (2,Claire,Belgium,2016-01-02), (3,Anne,Belgium,2016-01-04), (4,Tom,France,2016-02-04))\nusers: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, country: string, creation_date: date]\ndateFormat: java.text.SimpleDateFormat \u003d java.text.SimpleDateFormat@4f76f1a0\ntoTimestamp: String \u003d\u003e java.sql.Timestamp \u003d \u003cfunction1\u003e\nrawActivity: Array[(Int, String, java.sql.Timestamp, String)] \u003d Array((1,confirm_email,2016-01-01 10:10:46.0,null), (2,confirm_email,2016-01-02 20:17:00.0,null), (3,confirm_email,2016-01-05 09:23:50.0,null), (1,login,2016-01-10 12:28:28.0,null), (1,search,2016-01-10 12:30:12.0,{\"query\": \"motorcycle\"}), (1,login,2016-02-11 10:05:56.0,null), (1,search,2016-02-11 10:10:46.0,{\"query\": \"car\"}), (1,purchase,2016-02-11 13:47:28.0,{product: \"Some Fancy Car\"), (2,login,2016-01-11 15:51:02.0,null), (2,search,2016-01-11 16:18:39.0,{\"query\": \"helicopter\"}))\nactivity: org.apache.spark.sql.DataFrame \u003d [id: int, type: string, timestamp: timestamp, params: string]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Inner Join\r\nLet\u0027s suppose that we want to see the name, activity type, timestamp and the query (if any) that the user typed, but only for users who have some activity.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194077_1948287441",
      "id": "20160326-141634_1191557765",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eInner Join\u003c/h4\u003e\n\u003cp\u003eLet\u0027s suppose that we want to see the name, activity type, timestamp and the query (if any) that the user typed, but only for users who have some activity.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val usersInnerActivity \u003d users.as(\u0027u)\r\n  .join(activity.as(\u0027a), $\"u.id\"\u003d\u003d\u003d$\"a.id\", joinType\u003d\"inner\")\r\n  .selectExpr(\"u.id\", \"u.name\", \"a.type\", \"a.timestamp\", \"get_json_object(a.params, \u0027$.query\u0027) AS query\")\r\nusersInnerActivity.show(false)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194077_1948287441",
      "id": "20160326-141634_1226348080",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "usersInnerActivity: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, type: string, timestamp: timestamp, query: string]\n+---+------+-------------+---------------------+----------+\n|id |name  |type         |timestamp            |query     |\n+---+------+-------------+---------------------+----------+\n|1  |John  |confirm_email|2016-01-01 10:10:46.0|null      |\n|1  |John  |login        |2016-01-10 12:28:28.0|null      |\n|1  |John  |search       |2016-01-10 12:30:12.0|motorcycle|\n|1  |John  |login        |2016-02-11 10:05:56.0|null      |\n|1  |John  |search       |2016-02-11 10:10:46.0|car       |\n|1  |John  |purchase     |2016-02-11 13:47:28.0|null      |\n|2  |Claire|confirm_email|2016-01-02 20:17:00.0|null      |\n|2  |Claire|login        |2016-01-11 15:51:02.0|null      |\n|2  |Claire|search       |2016-01-11 16:18:39.0|helicopter|\n|3  |Anne  |confirm_email|2016-01-05 09:23:50.0|null      |\n+---+------+-------------+---------------------+----------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "usersInnerActivity.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194077_1948287441",
      "id": "20160326-141634_1831620301",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nProject [id#306,name#307,type#315,timestamp#316,get_json_object(params#317,$.query) AS query#321]\n+- SortMergeJoin [id#306], [id#314]\n   :- Sort [id#306 ASC], false, 0\n   :  +- TungstenExchange hashpartitioning(id#306,200), None\n   :     +- Project [_2#303 AS name#307,_1#302 AS id#306]\n   :        +- Scan ExistingRDD[_1#302,_2#303,_3#304,_4#305] \n   +- Sort [id#314 ASC], false, 0\n      +- TungstenExchange hashpartitioning(id#314,200), None\n         +- Project [_1#310 AS id#314,_2#311 AS type#315,_3#312 AS timestamp#316,_4#313 AS params#317]\n            +- Scan ExistingRDD[_1#310,_2#311,_3#312,_4#313]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Left Join\r\nNow, imagine we want to know, for all of the users, how many \"events\" or activities we have registered.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194078_1949441688",
      "id": "20160326-141634_2011802809",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eLeft Join\u003c/h3\u003e\n\u003cp\u003eNow, imagine we want to know, for all of the users, how many \u0026ldquo;events\u0026rdquo; or activities we have registered.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val usersLeftActivity \u003d users.as(\u0027u)\r\n  .join(activity.groupBy(\u0027id).count.as(\u0027a), $\"u.id\"\u003d\u003d\u003d$\"a.id\", \"left_outer\")\r\n  .selectExpr(\"u.id\", \"u.name\", \"a.count AS totalEvents\")\r\nusersLeftActivity.show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194078_1949441688",
      "id": "20160326-141634_239788848",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "usersLeftActivity: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, totalEvents: bigint]\n+---+------+-----------+\n| id|  name|totalEvents|\n+---+------+-----------+\n|  1|  John|          6|\n|  2|Claire|          3|\n|  3|  Anne|          1|\n|  4|   Tom|       null|\n+---+------+-----------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "usersLeftActivity.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194079_1949056939",
      "id": "20160326-141634_1451846564",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nProject [id#306,name#307,count#323L AS totalEvents#324L]\n+- SortMergeOuterJoin [id#306], [id#314], LeftOuter, None\n   :- Sort [id#306 ASC], false, 0\n   :  +- TungstenExchange hashpartitioning(id#306,200), None\n   :     +- Project [_1#302 AS id#306,_2#303 AS name#307]\n   :        +- Scan ExistingRDD[_1#302,_2#303,_3#304,_4#305] \n   +- Sort [id#314 ASC], false, 0\n      +- TungstenAggregate(key\u003d[id#314], functions\u003d[(count(1),mode\u003dFinal,isDistinct\u003dfalse)], output\u003d[id#314,count#323L])\n         +- TungstenExchange hashpartitioning(id#314,200), None\n            +- TungstenAggregate(key\u003d[id#314], functions\u003d[(count(1),mode\u003dPartial,isDistinct\u003dfalse)], output\u003d[id#314,count#327L])\n               +- Project [_1#310 AS id#314]\n                  +- Scan ExistingRDD[_1#310,_2#311,_3#312,_4#313]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Right Join\r\nLet\u0027s try the same example as before, but this time using a right join instead. We\u0027ll also introduce a new expression, to show a \"0\" instead when totalEvents is null.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194079_1949056939",
      "id": "20160326-141634_1476003934",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRight Join\u003c/h3\u003e\n\u003cp\u003eLet\u0027s try the same example as before, but this time using a right join instead. We\u0027ll also introduce a new expression, to show a \u0026ldquo;0\u0026rdquo; instead when totalEvents is null.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "activity.groupBy(\u0027id).count.as(\u0027a)\r\n  .join(users.as(\u0027u), $\"a.id\"\u003d\u003d\u003d$\"u.id\", \"right_outer\")\r\n  .selectExpr(\"u.id\", \"u.name\", \"COALESCE(a.count, 0) AS totalEvents\")\r\n  .show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194079_1949056939",
      "id": "20160326-141634_1614512241",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+------+-----------+\n| id|  name|totalEvents|\n+---+------+-----------+\n|  1|  John|          6|\n|  2|Claire|          3|\n|  3|  Anne|          1|\n|  4|   Tom|          0|\n+---+------+-----------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Left Semi-Join\r\nNow, we want to get data from users, but only for users that have some activity. We can use a left semi-join for that.\r\n\r\n---\r\n**Note**: *we could achieve the same with an inner join, but a left semi join will be way faster.*",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194080_1934821230",
      "id": "20160326-141634_1651354642",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eLeft Semi-Join\u003c/h3\u003e\n\u003cp\u003eNow, we want to get data from users, but only for users that have some activity. We can use a left semi-join for that.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003cem\u003ewe could achieve the same with an inner join, but a left semi join will be way faster.\u003c/em\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val usersLeftSemiActivity \u003d users.as(\u0027u)\r\n  .join(activity.as(\u0027a), $\"u.id\"\u003d\u003d\u003d$\"a.id\", \"left_semi\")\r\nusersLeftSemiActivity.show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194080_1934821230",
      "id": "20160326-141634_238814266",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "usersLeftSemiActivity: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, country: string, creation_date: date]\n+---+------+-------+-------------+\n| id|  name|country|creation_date|\n+---+------+-------+-------------+\n|  1|  John|    USA|   2016-01-01|\n|  2|Claire|Belgium|   2016-01-02|\n|  3|  Anne|Belgium|   2016-01-04|\n+---+------+-------+-------------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "usersLeftSemiActivity.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194080_1934821230",
      "id": "20160326-141634_104432419",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nLeftSemiJoinHash [id#306], [id#314], None\n:- TungstenExchange hashpartitioning(id#306,200), None\n:  +- Project [_1#302 AS id#306,_2#303 AS name#307,_3#304 AS country#308,_4#305 AS creation_date#309]\n:     +- Scan ExistingRDD[_1#302,_2#303,_3#304,_4#305] \n+- TungstenExchange hashpartitioning(id#314,200), None\n   +- Project [_1#310 AS id#314]\n      +- Scan ExistingRDD[_1#310,_2#311,_3#312,_4#313]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Left Anti-join\r\n\r\nWe have no option to perform a left-anti join directly, so we\u0027ll need to do it using a left join + filter.\r\n\r\nIn this case, we want to get users that have no activity at all.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194081_1934436481",
      "id": "20160326-141634_701526534",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eLeft Anti-join\u003c/h3\u003e\n\u003cp\u003eWe have no option to perform a left-anti join directly, so we\u0027ll need to do it using a left join + filter.\u003c/p\u003e\n\u003cp\u003eIn this case, we want to get users that have no activity at all.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val usersLeftAntiActivity \u003d users.as(\u0027u)\r\n  .join(activity.as(\u0027a), $\"u.id\"\u003d\u003d\u003d$\"a.id\", \"left_outer\")\r\n  .filter(\"a.id IS NULL\")\r\nusersLeftAntiActivity.show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194081_1934436481",
      "id": "20160326-141634_871234908",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "usersLeftAntiActivity: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, country: string, creation_date: date, id: int, type: string, timestamp: timestamp, params: string]\n+---+----+-------+-------------+----+----+---------+------+\n| id|name|country|creation_date|  id|type|timestamp|params|\n+---+----+-------+-------------+----+----+---------+------+\n|  4| Tom| France|   2016-02-04|null|null|     null|  null|\n+---+----+-------+-------------+----+----+---------+------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "usersLeftAntiActivity.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194082_1935590728",
      "id": "20160326-141634_1301593510",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nFilter isnull(id#314)\n+- SortMergeOuterJoin [id#306], [id#314], LeftOuter, None\n   :- Sort [id#306 ASC], false, 0\n   :  +- TungstenExchange hashpartitioning(id#306,200), None\n   :     +- Project [_1#302 AS id#306,_2#303 AS name#307,_3#304 AS country#308,_4#305 AS creation_date#309]\n   :        +- Scan ExistingRDD[_1#302,_2#303,_3#304,_4#305] \n   +- Sort [id#314 ASC], false, 0\n      +- TungstenExchange hashpartitioning(id#314,200), None\n         +- Project [_1#310 AS id#314,_2#311 AS type#315,_3#312 AS timestamp#316,_4#313 AS params#317]\n            +- Scan ExistingRDD[_1#310,_2#311,_3#312,_4#313]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Outer Join\r\n\r\nTo illustrate better what an outer join does, we\u0027ll create two very simple datasets.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194082_1935590728",
      "id": "20160326-141634_1613694228",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eOuter Join\u003c/h3\u003e\n\u003cp\u003eTo illustrate better what an outer join does, we\u0027ll create two very simple datasets.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val rawA \u003d Array((1, \"A\"), (2, \"B\"))\r\nval rawB \u003d Array((2, \"X\"), (3, \"Y\"))\r\nval a \u003d sc.parallelize(rawA).toDF(\"id\", \"col_a\")\r\nval b \u003d sc.parallelize(rawB).toDF(\"id\", \"col_b\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194082_1935590728",
      "id": "20160326-141634_787912033",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "rawA: Array[(Int, String)] \u003d Array((1,A), (2,B))\nrawB: Array[(Int, String)] \u003d Array((2,X), (3,Y))\na: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string]\nb: org.apache.spark.sql.DataFrame \u003d [id: int, col_b: string]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"outer\")\r\n  .selectExpr(\"a.id AS id_a\", \"a.col_a\", \"b.id AS id_b\", \"b.col_b\")\r\n  .show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194083_1935205979",
      "id": "20160326-141634_1053371229",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----+-----+----+-----+\n|id_a|col_a|id_b|col_b|\n+----+-----+----+-----+\n|   1|    A|null| null|\n|   2|    B|   2|    X|\n|null| null|   3|    Y|\n+----+-----+----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md In the cases when you do want to perform an outer join, you\u0027ll probably want just one \"id\" column, like this:",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194083_1935205979",
      "id": "20160326-141634_155059935",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIn the cases when you do want to perform an outer join, you\u0027ll probably want just one \u0026ldquo;id\u0026rdquo; column, like this:\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"outer\")\r\n  .selectExpr(\"COALESCE(a.id, b.id) AS id\", \"a.col_a\", \"b.col_b\")\r\n  .show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194083_1935205979",
      "id": "20160326-141634_1194663734",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+-----+\n| id|col_a|col_b|\n+---+-----+-----+\n|  1|    A| null|\n|  2|    B|    X|\n|  3| null|    Y|\n+---+-----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Cross Join (Cartesian Product)\r\nNote that you will rarely need a cross join, but when you do, take into account that the resulting DataFrame can become HUGE really fast. The size of the resulting DataFrame is the multiplication of both sizes.\r\n\r\nIn this example, we will do it with DataFrames \"a\" and \"b\" from the previous example, each of which has size 2, so we expect the cartesian product to have 4 rows.\r\n\r\n---\r\n**Note**: As a rule of thumb, you usually should avoid Cross Joins",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194084_1933282234",
      "id": "20160326-141634_1888761535",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eCross Join (Cartesian Product)\u003c/h3\u003e\n\u003cp\u003eNote that you will rarely need a cross join, but when you do, take into account that the resulting DataFrame can become HUGE really fast. The size of the resulting DataFrame is the multiplication of both sizes.\u003c/p\u003e\n\u003cp\u003eIn this example, we will do it with DataFrames \u0026ldquo;a\u0026rdquo; and \u0026ldquo;b\u0026rdquo; from the previous example, each of which has size 2, so we expect the cartesian product to have 4 rows.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: As a rule of thumb, you usually should avoid Cross Joins\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b))\r\n  .selectExpr(\"a.id AS id_a\", \"a.col_a\", \"b.id AS id_b\", \"b.col_b\")\r\n  .show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194084_1933282234",
      "id": "20160326-141634_311848791",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----+-----+----+-----+\n|id_a|col_a|id_b|col_b|\n+----+-----+----+-----+\n|   1|    A|   2|    X|\n|   1|    A|   3|    Y|\n|   2|    B|   2|    X|\n|   2|    B|   3|    Y|\n+----+-----+----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Some tips and strategies when joining DataFrames ",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194085_1932897485",
      "id": "20160326-141634_1382531657",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eSome tips and strategies when joining DataFrames\u003c/h3\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Use the appropiate join type\r\nSome joins are more efficients than others, so try to pick the join type that fits your needs.\r\n\r\nFor instance, if you are performing a left join between \"A\" and \"B\", and then filtering where \"B\" is not null, you can just perform an inner join instead.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194085_1932897485",
      "id": "20160326-141634_436498659",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eUse the appropiate join type\u003c/h4\u003e\n\u003cp\u003eSome joins are more efficients than others, so try to pick the join type that fits your needs.\u003c/p\u003e\n\u003cp\u003eFor instance, if you are performing a left join between \u0026ldquo;A\u0026rdquo; and \u0026ldquo;B\u0026rdquo;, and then filtering where \u0026ldquo;B\u0026rdquo; is not null, you can just perform an inner join instead.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"left_outer\")\r\n  .filter(\"b.id IS NOT NULL\")\r\n  .select($\"a.id\".as(\u0027id_a), $\"a.col_a\", $\"b.id\".as(\u0027id_b), $\"b.col_b\")\r\n  .show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194085_1932897485",
      "id": "20160326-141634_428823661",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----+-----+----+-----+\n|id_a|col_a|id_b|col_b|\n+----+-----+----+-----+\n|   2|    B|   2|    X|\n+----+-----+----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Is equivalent to this\r\na.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"inner\")\r\n  .select($\"a.id\".as(\u0027id_a), $\"a.col_a\", $\"b.id\".as(\u0027id_b), $\"b.col_b\")\r\n  .show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194086_1934051732",
      "id": "20160326-141634_284151095",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----+-----+----+-----+\n|id_a|col_a|id_b|col_b|\n+----+-----+----+-----+\n|   2|    B|   2|    X|\n+----+-----+----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md If you are performing an inner join, but just taking data from the first DataFrame, perform an left-semi join instead.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194086_1934051732",
      "id": "20160326-141634_1871306384",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIf you are performing an inner join, but just taking data from the first DataFrame, perform an left-semi join instead.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"inner\")\r\n  .select($\"a.id\", $\"a.col_a\")\r\n  .show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194086_1934051732",
      "id": "20160326-141634_1148385573",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+\n| id|col_a|\n+---+-----+\n|  2|    B|\n+---+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Is equivalent to this\r\na.as(\u0027a)\r\n  .join(b.as(\u0027b), $\"a.id\"\u003d\u003d\u003d$\"b.id\", \"left_semi\")\r\n  .show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194087_1933666983",
      "id": "20160326-141634_1480789822",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+\n| id|col_a|\n+---+-----+\n|  2|    B|\n+---+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Joining on multiple columns\r\n\r\nYou can actually join two DataFrames on multiple columns, but you should be careful with it, as it can easily create a cartesian product, especially if you use UDFs or \"range\" conditions (like \"between\") in your join columns and the DataFrame(s) are big.\r\n\r\nIn these cases, you could create a new column with the computed value on each of the DataFrames, and join on that column.\r\n",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194087_1933666983",
      "id": "20160326-141634_446553271",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eJoining on multiple columns\u003c/h4\u003e\n\u003cp\u003eYou can actually join two DataFrames on multiple columns, but you should be careful with it, as it can easily create a cartesian product, especially if you use UDFs or \u0026ldquo;range\u0026rdquo; conditions (like \u0026ldquo;between\u0026rdquo;) in your join columns and the DataFrame(s) are big.\u003c/p\u003e\n\u003cp\u003eIn these cases, you could create a new column with the computed value on each of the DataFrames, and join on that column.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{year, month}\r\nval signupMonthActivity \u003d users.as(\u0027u)\r\n  .join(\r\n    activity.as(\u0027a),\r\n    $\"u.id\"\u003d\u003d\u003d$\"a.id\"\r\n    \u0026\u0026 year(users(\"creation_date\")) \u003d\u003d\u003d year(activity(\"timestamp\"))\r\n    \u0026\u0026 month(users(\"creation_date\")) \u003d\u003d\u003d month(activity(\"timestamp\")),\r\n    \"inner\"\r\n  )\r\nsignupMonthActivity.show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194088_1931743239",
      "id": "20160326-141634_1912963079",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.functions.{year, month}\nsignupMonthActivity: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, country: string, creation_date: date, id: int, type: string, timestamp: timestamp, params: string]\n+---+------+-------+-------------+---+-------------+--------------------+--------------------+\n| id|  name|country|creation_date| id|         type|           timestamp|              params|\n+---+------+-------+-------------+---+-------------+--------------------+--------------------+\n|  3|  Anne|Belgium|   2016-01-04|  3|confirm_email|2016-01-05 09:23:...|                null|\n|  2|Claire|Belgium|   2016-01-02|  2|confirm_email|2016-01-02 20:17:...|                null|\n|  2|Claire|Belgium|   2016-01-02|  2|        login|2016-01-11 15:51:...|                null|\n|  2|Claire|Belgium|   2016-01-02|  2|       search|2016-01-11 16:18:...|{\"query\": \"helico...|\n|  1|  John|    USA|   2016-01-01|  1|confirm_email|2016-01-01 10:10:...|                null|\n|  1|  John|    USA|   2016-01-01|  1|        login|2016-01-10 12:28:...|                null|\n|  1|  John|    USA|   2016-01-01|  1|       search|2016-01-10 12:30:...|{\"query\": \"motorc...|\n+---+------+-------+-------------+---+-------------+--------------------+--------------------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// We could achieve the same results creating a new join column on each DataFrame:\r\nimport org.apache.spark.sql.functions.expr\r\nval usersWithJoinColumn \u003d users.withColumn(\"idYearMonth\", expr(\"CONCAT_WS(\u0027:::\u0027, id, YEAR(creation_date), MONTH(creation_date))\"))\r\nval activityWithJoinColumn \u003d activity.withColumn(\"idYearMonth\", expr(\"CONCAT_WS(\u0027:::\u0027, id, YEAR(timestamp), MONTH(timestamp))\"))\r\n\r\nval signupMonthActivity2 \u003d usersWithJoinColumn.as(\u0027u)\r\n  .join(activityWithJoinColumn.as(\u0027a), $\"u.idYearMonth\"\u003d\u003d\u003d$\"a.idYearMonth\", \"inner\")\r\nsignupMonthActivity2.show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194088_1931743239",
      "id": "20160326-141634_1192789516",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.functions.expr\nusersWithJoinColumn: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, country: string, creation_date: date, idYearMonth: string]\nactivityWithJoinColumn: org.apache.spark.sql.DataFrame \u003d [id: int, type: string, timestamp: timestamp, params: string, idYearMonth: string]\nsignupMonthActivity2: org.apache.spark.sql.DataFrame \u003d [id: int, name: string, country: string, creation_date: date, idYearMonth: string, id: int, type: string, timestamp: timestamp, params: string, idYearMonth: string]\n+---+------+-------+-------------+------------+---+-------------+--------------------+--------------------+------------+\n| id|  name|country|creation_date| idYearMonth| id|         type|           timestamp|              params| idYearMonth|\n+---+------+-------+-------------+------------+---+-------------+--------------------+--------------------+------------+\n|  2|Claire|Belgium|   2016-01-02|2:::2016:::1|  2|confirm_email|2016-01-02 20:17:...|                null|2:::2016:::1|\n|  2|Claire|Belgium|   2016-01-02|2:::2016:::1|  2|        login|2016-01-11 15:51:...|                null|2:::2016:::1|\n|  2|Claire|Belgium|   2016-01-02|2:::2016:::1|  2|       search|2016-01-11 16:18:...|{\"query\": \"helico...|2:::2016:::1|\n|  1|  John|    USA|   2016-01-01|1:::2016:::1|  1|confirm_email|2016-01-01 10:10:...|                null|1:::2016:::1|\n|  1|  John|    USA|   2016-01-01|1:::2016:::1|  1|        login|2016-01-10 12:28:...|                null|1:::2016:::1|\n|  1|  John|    USA|   2016-01-01|1:::2016:::1|  1|       search|2016-01-10 12:30:...|{\"query\": \"motorc...|1:::2016:::1|\n|  3|  Anne|Belgium|   2016-01-04|3:::2016:::1|  3|confirm_email|2016-01-05 09:23:...|                null|3:::2016:::1|\n+---+------+-------+-------------+------------+---+-------------+--------------------+--------------------+------------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md There is actually not much difference in terms of performance here, but our join became way more simple to read.\r\n\r\n---\r\n**Note**: *We could have used some sort of hashing function as well, but if you do use them, **beware of hash collisions**.*",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194088_1931743239",
      "id": "20160326-141634_453550667",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThere is actually not much difference in terms of performance here, but our join became way more simple to read.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003cem\u003eWe could have used some sort of hashing function as well, but if you do use them, \u003c/em\u003e\u003cem\u003ebeware of hash collisions\u003c/em\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "println(\"Plan with multiple join columns\")\r\nsignupMonthActivity.explain\r\nprintln(\"Plan with single join column\")\r\nsignupMonthActivity2.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194089_1931358490",
      "id": "20160326-141634_1406597066",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Plan with multiple join columns\n\u003d\u003d Physical Plan \u003d\u003d\nSortMergeJoin [id#306,year(creation_date#309),month(creation_date#309)], [id#314,year(cast(timestamp#316 as date)),month(cast(timestamp#316 as date))]\n:- Sort [id#306 ASC,year(creation_date#309) ASC,month(creation_date#309) ASC], false, 0\n:  +- TungstenExchange hashpartitioning(id#306,year(creation_date#309),month(creation_date#309),200), None\n:     +- Project [_1#302 AS id#306,_2#303 AS name#307,_3#304 AS country#308,_4#305 AS creation_date#309]\n:        +- Scan ExistingRDD[_1#302,_2#303,_3#304,_4#305] \n+- Sort [id#314 ASC,year(cast(timestamp#316 as date)) ASC,month(cast(timestamp#316 as date)) ASC], false, 0\n   +- TungstenExchange hashpartitioning(id#314,year(cast(timestamp#316 as date)),month(cast(timestamp#316 as date)),200), None\n      +- Project [_1#310 AS id#314,_2#311 AS type#315,_3#312 AS timestamp#316,_4#313 AS params#317]\n         +- Scan ExistingRDD[_1#310,_2#311,_3#312,_4#313]\nPlan with single join column\n\u003d\u003d Physical Plan \u003d\u003d\nSortMergeJoin [idYearMonth#354], [idYearMonth#355]\n:- Sort [idYearMonth#354 ASC], false, 0\n:  +- TungstenExchange hashpartitioning(idYearMonth#354,200), None\n:     +- Project [_1#302 AS id#306,_2#303 AS name#307,_3#304 AS country#308,_4#305 AS creation_date#309,concat_ws(:::,cast(_1#302 as string),cast(year(_4#305) as string),cast(month(_4#305) as string)) AS idYearMonth#354]\n:        +- Scan ExistingRDD[_1#302,_2#303,_3#304,_4#305] \n+- Sort [idYearMonth#355 ASC], false, 0\n   +- TungstenExchange hashpartitioning(idYearMonth#355,200), None\n      +- Project [_1#310 AS id#314,_2#311 AS type#315,_3#312 AS timestamp#316,_4#313 AS params#317,concat_ws(:::,cast(_1#310 as string),cast(year(cast(_3#312 as date)) as string),cast(month(cast(_3#312 as date)) as string)) AS idYearMonth#355]\n         +- Scan ExistingRDD[_1#310,_2#311,_3#312,_4#313]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Merging DataFrames: unionAll and except\r\nSometimes, you just want to put two DataFrames with the same schema together. In that cases, you can use \"unionAll\" to get a new DataFrame with the rows of both.\r\n\r\nAnother operation you might want to do, is to remove all rows from one DataFrame that exist in another DataFrame. In that cases, you can use except. Note that this is logically equivalent to performing a left anit-join, with the difference that in the case of except, we need to have the same schema.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194089_1931358490",
      "id": "20160326-141634_1176902086",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eMerging DataFrames: unionAll and except\u003c/h3\u003e\n\u003cp\u003eSometimes, you just want to put two DataFrames with the same schema together. In that cases, you can use \u0026ldquo;unionAll\u0026rdquo; to get a new DataFrame with the rows of both.\u003c/p\u003e\n\u003cp\u003eAnother operation you might want to do, is to remove all rows from one DataFrame that exist in another DataFrame. In that cases, you can use except. Note that this is logically equivalent to performing a left anit-join, with the difference that in the case of except, we need to have the same schema.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### unionAll\r\nAs part of many pipelines, sometimes you calculate partial results of your output into different DataFrames. unionAll comes in handy when you need to put these together.\r\n\r\n---\r\n**IMPORTANT** When performing a union between DataFrames, the number of columns and their order is important. Make sure the columns of the different DF in the union are in the right order.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194089_1931358490",
      "id": "20160326-141634_1868970532",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eunionAll\u003c/h3\u003e\n\u003cp\u003eAs part of many pipelines, sometimes you calculate partial results of your output into different DataFrames. unionAll comes in handy when you need to put these together.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eIMPORTANT\u003c/strong\u003e When performing a union between DataFrames, the number of columns and their order is important. Make sure the columns of the different DF in the union are in the right order.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val rawData1 \u003d Array(\r\n  (1, \"Doe\", \"John\", 1200.0, \"x\"),\r\n  (2, \"Smith\", \"Frank\", 2798.45, \"y\")\r\n)\r\nval rawData2 \u003d Array(\r\n  (15, \"Doe\", \"Jane\", 3194.82, \"a\"),\r\n  (32, \"Johnson\", \"Patty\", 1263.08, \"d\")\r\n)\r\nval firstDf \u003d sc.parallelize(rawData1).toDF(\"id\", \"last_name\", \"first_name\", \"salary\", \"category\")\r\nval secondDf \u003d sc.parallelize(rawData2).toDF(\"id\", \"last_name\", \"first_name\", \"salary\", \"category\")\r\n\r\nfirstDf.unionAll(secondDf).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194090_1932512736",
      "id": "20160326-141634_544426655",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "rawData1: Array[(Int, String, String, Double, String)] \u003d Array((1,Doe,John,1200.0,x), (2,Smith,Frank,2798.45,y))\nrawData2: Array[(Int, String, String, Double, String)] \u003d Array((15,Doe,Jane,3194.82,a), (32,Johnson,Patty,1263.08,d))\nfirstDf: org.apache.spark.sql.DataFrame \u003d [id: int, last_name: string, first_name: string, salary: double, category: string]\nsecondDf: org.apache.spark.sql.DataFrame \u003d [id: int, last_name: string, first_name: string, salary: double, category: string]\n+---+---------+----------+-------+--------+\n| id|last_name|first_name| salary|category|\n+---+---------+----------+-------+--------+\n|  1|      Doe|      John| 1200.0|       x|\n|  2|    Smith|     Frank|2798.45|       y|\n| 15|      Doe|      Jane|3194.82|       a|\n| 32|  Johnson|     Patty|1263.08|       d|\n+---+---------+----------+-------+--------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Example: Wrong order in the columns\r\n\r\nWhat would happen if our second DataFrame had a different order in the columns? Let\u0027s try it.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194090_1932512736",
      "id": "20160326-141634_1898454365",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eExample: Wrong order in the columns\u003c/h4\u003e\n\u003cp\u003eWhat would happen if our second DataFrame had a different order in the columns? Let\u0027s try it.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val thirdDf \u003d secondDf.select(\u0027first_name, \u0027salary, \u0027last_name, \u0027id, \u0027category)\r\nval firstAndThirdDf \u003d firstDf.unionAll(thirdDf)\r\nfirstAndThirdDf.show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194091_1932127987",
      "id": "20160326-141634_10485429",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "thirdDf: org.apache.spark.sql.DataFrame \u003d [first_name: string, salary: double, last_name: string, id: int, category: string]\nfirstAndThirdDf: org.apache.spark.sql.DataFrame \u003d [id: string, last_name: string, first_name: string, salary: double, category: string]\n+-----+---------+----------+-------+--------+\n|   id|last_name|first_name| salary|category|\n+-----+---------+----------+-------+--------+\n|    1|      Doe|      John| 1200.0|       x|\n|    2|    Smith|     Frank|2798.45|       y|\n| Jane|  3194.82|       Doe|   15.0|       a|\n|Patty|  1263.08|   Johnson|   32.0|       d|\n+-----+---------+----------+-------+--------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md As you can see, the result is quite weird: The column names are those of the first DataFrame, but the types were somehow changed. Actually, the types were inferred to be the most generic types that could fit the values, that\u0027s why \"id\" is now a string, and \"salary\" (where we also had Int values), stayed as a double.\r\n\r\nSo, how to put them together, making sure that the order is the right one? We can especifically select the columns in the right order. In case the column names are identical, we can use the column names from one DataFrame to select the columns of the second in the right order.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194091_1932127987",
      "id": "20160326-141634_134946149",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eAs you can see, the result is quite weird: The column names are those of the first DataFrame, but the types were somehow changed. Actually, the types were inferred to be the most generic types that could fit the values, that\u0027s why \u0026ldquo;id\u0026rdquo; is now a string, and \u0026ldquo;salary\u0026rdquo; (where we also had Int values), stayed as a double.\u003c/p\u003e\n\u003cp\u003eSo, how to put them together, making sure that the order is the right one? We can especifically select the columns in the right order. In case the column names are identical, we can use the column names from one DataFrame to select the columns of the second in the right order.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val firstAndThirdDfSorted \u003d firstDf.unionAll(thirdDf.selectExpr(firstDf.columns:_*))\r\nfirstAndThirdDfSorted.show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194091_1932127987",
      "id": "20160326-141634_297195011",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "firstAndThirdDfSorted: org.apache.spark.sql.DataFrame \u003d [id: int, last_name: string, first_name: string, salary: double, category: string]\n+---+---------+----------+-------+--------+\n| id|last_name|first_name| salary|category|\n+---+---------+----------+-------+--------+\n|  1|      Doe|      John| 1200.0|       x|\n|  2|    Smith|     Frank|2798.45|       y|\n| 15|      Doe|      Jane|3194.82|       a|\n| 32|  Johnson|     Patty|1263.08|       d|\n+---+---------+----------+-------+--------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md If the names are not all the same, one could for instance create a Scala Map with differences and change ```firstDf.columns``` into ```firstDf.columns.map(c \u003d\u003e nameMap.getOrElse(c))```",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194092_1930204243",
      "id": "20160326-141634_2057070602",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIf the names are not all the same, one could for instance create a Scala Map with differences and change \u003ccode\u003e`firstDf.columns\u003c/code\u003e\u003ccode\u003einto ``\u003c/code\u003efirstDf.columns.map(c \u003d\u003e nameMap.getOrElse(c))```\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### except\r\nExcept is a logical equivalent to a left anti-join, with the extra requirement that both DataFrames need to have the same number of columns.\r\n\r\n---\r\n**IMPORTANT** Except will not fail if the order of the columns is not the same in both DataFrames, but it will not give the expected results either.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194092_1930204243",
      "id": "20160326-141634_846374761",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eexcept\u003c/h4\u003e\n\u003cp\u003eExcept is a logical equivalent to a left anti-join, with the extra requirement that both DataFrames need to have the same number of columns.\u003c/p\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cstrong\u003eIMPORTANT\u003c/strong\u003e Except will not fail if the order of the columns is not the same in both DataFrames, but it will not give the expected results either.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " val guestList \u003d sc.parallelize(Array(\r\n   (1, \"Tina\"),\r\n   (2, \"James\"),\r\n   (3, \"Foo\"),\r\n   (4, \"Bar\")\r\n)).toDF(\"id\", \"name\")\r\nval blacklistedGuests \u003d sc.parallelize(Array(\r\n  (3, \"Foo\"),\r\n  (5, \"James\")\r\n)).toDF(\"id\", \"name\")\r\nguestList.except(blacklistedGuests).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194092_1930204243",
      "id": "20160326-141634_88930399",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "guestList: org.apache.spark.sql.DataFrame \u003d [id: int, name: string]\nblacklistedGuests: org.apache.spark.sql.DataFrame \u003d [id: int, name: string]\n+---+-----+\n| id| name|\n+---+-----+\n|  2|James|\n|  4|  Bar|\n|  1| Tina|\n+---+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "guestList.except(blacklistedGuests.select(\u0027name, \u0027id)).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194093_1929819494",
      "id": "20160326-141634_4597681",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+\n| id| name|\n+---+-----+\n|  3|  Foo|\n|  4|  Bar|\n|  1| Tina|\n|  2|James|\n+---+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Handling null values",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194093_1929819494",
      "id": "20160326-141634_2111296922",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eHandling null values\u003c/h3\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Using NULL / IS NOT NULL\r\nIt is important to be aware of NULL values in your DataFrames. As with SQL, many operations involving a NULL value will return NULL. On top of that, a NULL will \"evaluated\" or at least equivalent to false inside conditions.\r\nLet\u0027s see it with an example:",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194094_1930973741",
      "id": "20160326-141634_1695068673",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eUsing NULL / IS NOT NULL\u003c/h4\u003e\n\u003cp\u003eIt is important to be aware of NULL values in your DataFrames. As with SQL, many operations involving a NULL value will return NULL. On top of that, a NULL will \u0026ldquo;evaluated\u0026rdquo; or at least equivalent to false inside conditions.\n\u003cbr  /\u003eLet\u0027s see it with an example:\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val dfWithNulls \u003d sc.parallelize(Array(\r\n  (1, \"foo\", \"a\"),\r\n  (2, null, \"b\"),\r\n  (3, \"bar\", \"x\")\r\n)).toDF(\"id\", \"title\", \"subtitle\")\r\ndfWithNulls.filter(\"title \u003c\u003e \u0027bar\u0027\").show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194094_1930973741",
      "id": "20160326-141634_501126009",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "dfWithNulls: org.apache.spark.sql.DataFrame \u003d [id: int, title: string, subtitle: string]\n+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n|  1|  foo|       a|\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md Did you notice any missing rows? The second row, \"(2, null, b)\" should be there as well, as the title is not \"bar\", but it\u0027s not there. This happens because we have a condition like \"NULL \u003c\u003e \u0027bar\u0027\", which is evaluated to NULL, so it gets filtered out (the result of the expression is not true). This would happen even if we compare NULL to NULL:",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194094_1930973741",
      "id": "20160326-141634_2141453376",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eDid you notice any missing rows? The second row, \u0026ldquo;(2, null, b)\u0026rdquo; should be there as well, as the title is not \u0026ldquo;bar\u0026rdquo;, but it\u0027s not there. This happens because we have a condition like \u0026ldquo;NULL \u003c\u003e \u0027bar\u0027\u0026ldquo;, which is evaluated to NULL, so it gets filtered out (the result of the expression is not true). This would happen even if we compare NULL to NULL:\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "dfWithNulls.filter(\u0027title \u003d\u003d\u003d null).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194095_1930588992",
      "id": "20160326-141634_1728450359",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md There are many ways to deal with NULL values, for instance, using the \"IS NULL\" / \"IS NOT NULL\" expressions in SQL, or the \"isNull\" / \"isNotNull\" Column methods.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194095_1930588992",
      "id": "20160326-141634_1053890744",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThere are many ways to deal with NULL values, for instance, using the \u0026ldquo;IS NULL\u0026rdquo; / \u0026ldquo;IS NOT NULL\u0026rdquo; expressions in SQL, or the \u0026ldquo;isNull\u0026rdquo; / \u0026ldquo;isNotNull\u0026rdquo; Column methods.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "dfWithNulls.filter(\"title IS NULL OR title \u003c\u003e \u0027bar\u0027\").show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194095_1930588992",
      "id": "20160326-141634_296613952",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n|  1|  foo|       a|\n|  2| null|       b|\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "dfWithNulls.filter(\u0027title.isNull).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194096_1940977212",
      "id": "20160326-141634_881146577",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n|  2| null|       b|\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Coalesce will take the first non-null value from its arguments, left to right\r\ndfWithNulls.filter(\"COALESCE(title, \u0027\u0027) \u003c\u003e \u0027bar\u0027\").show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194096_1940977212",
      "id": "20160326-141634_383289927",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+-----+--------+\n| id|title|subtitle|\n+---+-----+--------+\n|  1|  foo|       a|\n|  2| null|       b|\n+---+-----+--------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### The \"\u003d\u003d\u003d\" and \"\u003c\u003d\u003e\" operators",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194096_1940977212",
      "id": "20160326-141634_1650345736",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eThe \u0026ldquo;\u003d\u003d\u003d\u0026rdquo; and \u0026ldquo;\u003c\u003d\u003e\u0026rdquo; operators\u003c/h4\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md The \"\u003d\u003d\u003d\" operator, used to compare values, is not null-safe. This means that an expression like this:\r\n```\r\nmyDf.filter(\u0027col_a \u003d\u003d\u003d \u0027col_b)\r\n```\r\nWill not include rows where both col\\_a and col\\_b are NULL.\r\n",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194097_1940592463",
      "id": "20160326-141634_462000300",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThe \u0026ldquo;\u003d\u003d\u003d\u0026rdquo; operator, used to compare values, is not null-safe. This means that an expression like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emyDf.filter(\u0027col_a \u003d\u003d\u003d \u0027col_b)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWill not include rows where both col_a and col_b are NULL.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val myDf \u003d sc.parallelize(Array(\r\n  (\"a\", \"z\"),\r\n  (null, \"y\"),\r\n  (\"b\", \"b\"),\r\n  (null, null)\r\n)).toDF(\"col_a\", \"col_b\")\r\nmyDf.filter(\u0027col_a \u003d\u003d\u003d \u0027col_b).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194097_1940592463",
      "id": "20160326-141634_1685658195",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myDf: org.apache.spark.sql.DataFrame \u003d [col_a: string, col_b: string]\n+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    b|    b|\n+-----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md The \"\u003c\u003d\u003e\" operator is null-safe, meaning that it will actually return TRUE if we compare two null values",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194097_1940592463",
      "id": "20160326-141634_1253012141",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThe \u0026ldquo;\u003c\u003d\u003e\u0026rdquo; operator is null-safe, meaning that it will actually return TRUE if we compare two null values\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "myDf.filter(\u0027col_a \u003c\u003d\u003e \u0027col_b).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194098_1941746710",
      "id": "20160326-141634_1150411305",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    b|    b|\n| null| null|\n+-----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md So, should we always use the null-safe operator?\r\n\r\n**No** Actually it depends on the use case. NULL represents value that is not known, so in some cases, you might not want these values to be matched.\r\n\r\nLet\u0027s say we are preparing a training, that will be held in groups of two people, and that we have a list of participants, where we have the name of the participant, and his/her level for the subject. Then, we have a sofisticated algorithm that groups the participants in groups of two. Do we really want to put two people of unknown level in the same class? It could happen that both have the same level, but it could happen as well that one of them has basic knowledge, and the other is an advanced user.\r\n\r\nIn short, even though it might help in most of the cases, think in your use case before adding the null-safe operator.\r\n",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194098_1941746710",
      "id": "20160326-141634_1167529194",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eSo, should we always use the null-safe operator?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNo\u003c/strong\u003e Actually it depends on the use case. NULL represents value that is not known, so in some cases, you might not want these values to be matched.\u003c/p\u003e\n\u003cp\u003eLet\u0027s say we are preparing a training, that will be held in groups of two people, and that we have a list of participants, where we have the name of the participant, and his/her level for the subject. Then, we have a sofisticated algorithm that groups the participants in groups of two. Do we really want to put two people of unknown level in the same class? It could happen that both have the same level, but it could happen as well that one of them has basic knowledge, and the other is an advanced user.\u003c/p\u003e\n\u003cp\u003eIn short, even though it might help in most of the cases, think in your use case before adding the null-safe operator.\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### DataFrameNa functions\n\nTo work with null values in DataFrames, we can use the \"na\" function, that will not return a DataFrame, but an instance of a special class, \"DataFrameNa\", that provides some methods to work on rows containing null values:\n - drop: Allows us to drop rows with null values in the DataFrame. We can use different conditions to decide which rows to drop, for instance, drop any row containing null values, or rows containing more than a certain number of nulls, rows containing null values in specific columns, etc..\n - fill: Can be used to replace null values in certain columns by whatever value we want. So, we could replace a NULL name by \"Unknown\" or a NULL salary by \"0.0\".\n - replace: With this method, we can replace values in some columns, if there are null values in the same row.\n \n\u003chttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions\u003e",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194098_1941746710",
      "id": "20160326-141634_1105080919",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eDataFrameNa functions\u003c/h4\u003e\n\u003cp\u003eTo work with null values in DataFrames, we can use the \u0026ldquo;na\u0026rdquo; function, that will not return a DataFrame, but an instance of a special class, \u0026ldquo;DataFrameNa\u0026rdquo;, that provides some methods to work on rows containing null values:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edrop: Allows us to drop rows with null values in the DataFrame. We can use different conditions to decide which rows to drop, for instance, drop any row containing null values, or rows containing more than a certain number of nulls, rows containing null values in specific columns, etc..\u003c/li\u003e\n\u003cli\u003efill: Can be used to replace null values in certain columns by whatever value we want. So, we could replace a NULL name by \u0026ldquo;Unknown\u0026rdquo; or a NULL salary by \u0026ldquo;0.0\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003ereplace: With this method, we can replace values in some columns, if there are null values in the same row.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions\"\u003ehttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val myDf \u003d sc.parallelize(Array(\r\n  (\"a\", \"z\"),\r\n  (null, \"y\"),\r\n  (\"b\", \"b\"),\r\n  (null, null)\r\n)).toDF(\"col_a\", \"col_b\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194099_1941361961",
      "id": "20160326-141634_1656426377",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myDf: org.apache.spark.sql.DataFrame \u003d [col_a: string, col_b: string]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//Drop all columns with null values\r\nmyDf.na.drop().show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194099_1941361961",
      "id": "20160326-141634_1727602178",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    a|    z|\n|    b|    b|\n+-----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//Drop all columns with more than one null value\r\nmyDf.na.drop(1).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194100_1939438217",
      "id": "20160326-141634_1830898206",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    a|    z|\n| null|    y|\n|    b|    b|\n+-----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Drop values with nulls in col_b only\r\nmyDf.na.drop(Array(\"col_b\")).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194100_1939438217",
      "id": "20160326-141634_612970379",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    a|    z|\n| null|    y|\n|    b|    b|\n+-----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Fill null values in col_a by \"Unknown A\"\r\nmyDf.na.fill(Map(\"col_a\" -\u003e \"Unknown A\")).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194100_1939438217",
      "id": "20160326-141634_714105194",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---------+-----+\n|    col_a|col_b|\n+---------+-----+\n|        a|    z|\n|Unknown A|    y|\n|        b|    b|\n|Unknown A| null|\n+---------+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// When we have a null in the row, replace value \"y\" by \"maybe_y\" in col_b\r\nmyDf.na.replace(\"col_b\", Map(\"y\"-\u003e \"maybe_y\")).show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194101_1939053468",
      "id": "20160326-141634_1827376122",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----+-------+\n|col_a|  col_b|\n+-----+-------+\n|    a|      z|\n| null|maybe_y|\n|    b|      b|\n| null|   null|\n+-----+-------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Actions",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194101_1939053468",
      "id": "20160326-141634_1546846707",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eActions\u003c/h3\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " val myDf \u003d sc.parallelize(Array(\r\n  (\"a\", \"z\"),\r\n  (null, \"y\"),\r\n  (\"b\", \"b\"),\r\n  (null, null)\r\n)).toDF(\"col_a\", \"col_b\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194101_1939053468",
      "id": "20160326-141634_415573967",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myDf: org.apache.spark.sql.DataFrame \u003d [col_a: string, col_b: string]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Saving a DataFrame to / reading from a Parquet file\r\nThe \"mode\" method allows us to control what happens if the directory where we are saving already exists. It can be one of the following:\r\n- error: Default option. An error will be thrown if the destination already exists\r\n- overwrite: The destination will be deleted, so we\u0027ll have only the new data at the end.\r\n- append: If the destination exists, the data will be appended.\r\n- ignore: Do nothing if the destination already exists.\r\n",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194159_1905965062",
      "id": "20160326-141634_934843325",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eSaving a DataFrame to / reading from a Parquet file\u003c/h4\u003e\n\u003cp\u003eThe \u0026ldquo;mode\u0026rdquo; method allows us to control what happens if the directory where we are saving already exists. It can be one of the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eerror: Default option. An error will be thrown if the destination already exists\u003c/li\u003e\n\u003cli\u003eoverwrite: The destination will be deleted, so we\u0027ll have only the new data at the end.\u003c/li\u003e\n\u003cli\u003eappend: If the destination exists, the data will be appended.\u003c/li\u003e\n\u003cli\u003eignore: Do nothing if the destination already exists.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Save the DataFrame. This will fail if \"my_df.parquet\" already exists\r\nmyDf.write.parquet(\"my_df.parquet\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194160_1916353283",
      "id": "20160326-141634_921401823",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Even if the previous step worked, doing it a second time will fail, because the destination already exists\r\nmyDf.write.parquet(\"my_df.parquet\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194160_1916353283",
      "id": "20160326-141634_495860644",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.sql.AnalysisException: path hdfs://sandbox.hortonworks.com:8020/user/zeppelin/my_df.parquet already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:76)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:139)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:329)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:56)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:61)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:63)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:65)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:67)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:69)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:71)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:73)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:75)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:77)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:79)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:81)\n\tat \u003cinit\u003e(\u003cconsole\u003e:83)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:87)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:673)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:666)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:295)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// We can override the existing data..\r\nmyDf.write.mode(\"overwrite\").parquet(\"my_df.parquet\")\r\nval readDf1 \u003d sqlContext.read.parquet(\"my_df.parquet\")\r\nreadDf1.show()",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194161_1915968534",
      "id": "20160326-141634_1693293348",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "readDf1: org.apache.spark.sql.DataFrame \u003d [col_a: string, col_b: string]\n+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    a|    z|\n| null|    y|\n|    b|    b|\n| null| null|\n+-----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Or we can append the data\r\nmyDf.write.mode(\"append\").parquet(\"my_df.parquet\")\r\nval readDf2 \u003d sqlContext.read.parquet(\"my_df.parquet\")\r\nreadDf2.show()\r\n",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194161_1915968534",
      "id": "20160326-141634_728009792",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "readDf2: org.apache.spark.sql.DataFrame \u003d [col_a: string, col_b: string]\n+-----+-----+\n|col_a|col_b|\n+-----+-----+\n|    a|    z|\n| null|    y|\n|    a|    z|\n| null|    y|\n|    b|    b|\n| null| null|\n|    b|    b|\n| null| null|\n+-----+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md Other formats\r\nAmong others, you can save your DF to orc format, json, or text. In the case of text, your DF needs to have a single column, that must be a string. Let\u0027s do it with an example that will create a file with tab-separated values:",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194162_1917122780",
      "id": "20160326-141634_1446237185",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eOther formats\n\u003cbr  /\u003eAmong others, you can save your DF to orc format, json, or text. In the case of text, your DF needs to have a single column, that must be a string. Let\u0027s do it with an example that will create a file with tab-separated values:\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val myDf \u003d sc.parallelize(Array(\r\n  (1, \"a\", 5.0, true),\r\n  (2, null, -1.3, false),\r\n  (3, \"b\", 321.09, true)\r\n)).toDF(\"id\", \"col_a\", \"col_b\", \"col_c\")\r\n\r\nval separator \u003d \"\\t\"\r\nval headers \u003d sc.parallelize(Array(myDf.columns.mkString(separator))).toDF(\"value\")\r\n\r\nimport org.apache.spark.sql.functions.lit\r\n// Contents to export, put together as a string with tab-separated values\r\nval content \u003d myDf.map(_.toSeq.mkString(separator)).toDF(\"value\")\r\nval toExport \u003d headers.withColumn(\"is_header\", lit(true))\r\n  .unionAll(content.withColumn(\"is_header\", lit(false)))\r\n  .coalesce(1)\r\n  .orderBy(\u0027is_header.desc)\r\n  .select(\u0027value)\r\ntoExport.write.mode(\"overwrite\").text(\"myDf.tsv\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194162_1917122780",
      "id": "20160326-141634_1688451549",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myDf: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string, col_b: double, col_c: boolean]\nseparator: String \u003d \"\t\"\nheaders: org.apache.spark.sql.DataFrame \u003d [value: string]\nimport org.apache.spark.sql.functions.lit\ncontent: org.apache.spark.sql.DataFrame \u003d [value: string]\ntoExport: org.apache.spark.sql.DataFrame \u003d [value: string]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Let\u0027s see what was saved\r\nsc.textFile(\"myDf.tsv\").take(5).foreach(println)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194162_1917122780",
      "id": "20160326-141634_956272585",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "id\tcol_a\tcol_b\tcol_c\n1\ta\t5.0\ttrue\n2\tnull\t-1.3\tfalse\n3\tb\t321.09\ttrue\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md After saving the DF to a text file, you could, from a client machine, get that file with a command like this (assuming you are in a Hadoop cluster):\r\n\r\n```\r\nhadoop fs -get /path/to/myDf.tsv/part* myDeliverable.tsv\r\n```\r\n\r\nThis will work because we have a single \"part-XXX\" file. If you have multiple parts (ie, you didn\u0027t use coalesce), you should download the whole directory, and put the different files together if you want, for instance with cat or something similar. Just be careful with the headers: in this example, the headers will be only in one of the partitions. So, in case you don\u0027t want to  / can not coalesce to one partition because your DF is too big, you could proceed like this:\r\n- Create your headers DF, save it to a separate file\r\n- Save your DF as text in a different location\r\n- Get the headers and the different parts with \"hadoop fs -get\"\r\n- Concat the headers and the different part-XXX files together from the shell using \"cat\" or similar.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194163_1916738031",
      "id": "20160326-141634_305721947",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eAfter saving the DF to a text file, you could, from a client machine, get that file with a command like this (assuming you are in a Hadoop cluster):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehadoop fs -get /path/to/myDf.tsv/part* myDeliverable.tsv\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will work because we have a single \u0026ldquo;part-XXX\u0026rdquo; file. If you have multiple parts (ie, you didn\u0027t use coalesce), you should download the whole directory, and put the different files together if you want, for instance with cat or something similar. Just be careful with the headers: in this example, the headers will be only in one of the partitions. So, in case you don\u0027t want to  / can not coalesce to one partition because your DF is too big, you could proceed like this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCreate your headers DF, save it to a separate file\u003c/li\u003e\n\u003cli\u003eSave your DF as text in a different location\u003c/li\u003e\n\u003cli\u003eGet the headers and the different parts with \u0026ldquo;hadoop fs -get\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eConcat the headers and the different part-XXX files together from the shell using \u0026ldquo;cat\u0026rdquo; or similar.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Explanation on the text file creation\r\nGiven a DataFrame, the result will be a single file in HDFS, were the first line contains the headers. There are a few tricks here:\r\n- \"\\_.toSeq\": Converts a row to a sequence, so we can easily make a String out of it.\r\n- Adding the \"is\\_header\" column: Needed to make sure that we can put the headers in the first line by ordering.\r\n- coalesce(1): This will reduce the DF to a single partition. You probably don\u0027t want this if your DF is too big.",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194163_1916738031",
      "id": "20160326-141634_1487879605",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eExplanation on the text file creation\u003c/h3\u003e\n\u003cp\u003eGiven a DataFrame, the result will be a single file in HDFS, were the first line contains the headers. There are a few tricks here:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u0026ldquo;_.toSeq\u0026rdquo;: Converts a row to a sequence, so we can easily make a String out of it.\u003c/li\u003e\n\u003cli\u003eAdding the \u0026ldquo;is_header\u0026rdquo; column: Needed to make sure that we can put the headers in the first line by ordering.\u003c/li\u003e\n\u003cli\u003ecoalesce(1): This will reduce the DF to a single partition. You probably don\u0027t want this if your DF is too big.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Saving to Hive\r\nA nice way to organize our data (and to expose it with other tools, like Tableau), is to use Hive to store our tables. When saving to Hive, apart of setting the mode as we do for files we can specify the format of the table (we usually want parquet, which is the default, but we could store the table as orc, for instance).",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194163_1916738031",
      "id": "20160326-141634_1492935567",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eSaving to Hive\u003c/h4\u003e\n\u003cp\u003eA nice way to organize our data (and to expose it with other tools, like Tableau), is to use Hive to store our tables. When saving to Hive, apart of setting the mode as we do for files we can specify the format of the table (we usually want parquet, which is the default, but we could store the table as orc, for instance).\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " val myDf \u003d sc.parallelize(Array(\r\n  (1, \"a\", 5.0, true),\r\n  (2, null, -1.3, false),\r\n  (3, \"b\", 321.09, true)\r\n)).toDF(\"id\", \"col_a\", \"col_b\", \"col_c\")\r\n\r\nsql(\"CREATE DATABASE df_training\")\r\nmyDf.write.mode(\"overwrite\").saveAsTable(\"df_training.my_df\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194164_1914814287",
      "id": "20160326-141634_569143021",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myDf: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string, col_b: double, col_c: boolean]\nres238: org.apache.spark.sql.DataFrame \u003d [result: string]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Let\u0027s see what was stored in Hive, using SQL\r\nsql(\"DESCRIBE df_training.my_df\").show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194164_1914814287",
      "id": "20160326-141634_694590209",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|      id|      int|       |\n|   col_a|   string|       |\n|   col_b|   double|       |\n|   col_c|  boolean|       |\n+--------+---------+-------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Show the query to create the table\nsql(\"SHOW CREATE TABLE df_training.my_df\").show(false)",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194164_1914814287",
      "id": "20160326-141634_256537571",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-----------------------------------------------------------------------------------------+\n|result                                                                                   |\n+-----------------------------------------------------------------------------------------+\n|CREATE TABLE `df_training.my_df`(                                                        |\n|  `id` int COMMENT \u0027\u0027,                                                                   |\n|  `col_a` string COMMENT \u0027\u0027,                                                             |\n|  `col_b` double COMMENT \u0027\u0027,                                                             |\n|  `col_c` boolean COMMENT \u0027\u0027)                                                            |\n|ROW FORMAT SERDE                                                                         |\n|  \u0027org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\u0027                          |\n|WITH SERDEPROPERTIES (                                                                   |\n|  \u0027path\u0027\u003d\u0027hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/df_training.db/my_df\u0027) |\n|STORED AS INPUTFORMAT                                                                    |\n|  \u0027org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\u0027                        |\n|OUTPUTFORMAT                                                                             |\n|  \u0027org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\u0027                       |\n|LOCATION                                                                                 |\n|  \u0027hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/df_training.db/my_df\u0027         |\n|TBLPROPERTIES (                                                                          |\n|  \u0027COLUMN_STATS_ACCURATE\u0027\u003d\u0027false\u0027,                                                       |\n|  \u0027EXTERNAL\u0027\u003d\u0027FALSE\u0027,                                                                    |\n|  \u0027numFiles\u0027\u003d\u00272\u0027,                                                                        |\n|  \u0027numRows\u0027\u003d\u0027-1\u0027,                                                                        |\n+-----------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Get the table as a DF\r\nval myTable \u003d sqlContext.table(\"df_training.my_df\")\r\n// Is equivalent to this (note that the execution plans might not be exactly the same)\r\nval myTableSql \u003d sql(\"SELECT * FROM df_training.my_df\")\r\nmyTableSql.show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194165_1914429538",
      "id": "20160326-141634_1627276268",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myTable: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string, col_b: double, col_c: boolean]\nmyTableSql: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string, col_b: double, col_c: boolean]\n+---+-----+------+-----+\n| id|col_a| col_b|col_c|\n+---+-----+------+-----+\n|  1|    a|   5.0| true|\n|  2| null|  -1.3|false|\n|  3|    b|321.09| true|\n+---+-----+------+-----+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Troubleshooting and gotchas",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194165_1914429538",
      "id": "20160326-141634_1427991731",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eTroubleshooting and gotchas\u003c/h3\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### To take into account when saving to Hive:\r\n\r\n* If you intend to use your tables outside Spark (Tableau, etc), you might want to disable the metastore conversion by setting the property \"spark.sql.hive.convertMetastoreParquet\" to false. You can do that when starting your shell:\r\n```\r\nspark-shell \u003cother_options\u003e --conf spark.sql.hive.convertMetastoreParquet\u003dfalse\r\n```\r\n* Spark caches table metadata, meaning that, if you update your table from outside the spark shell, you should refresh it:\r\n```\r\nsqlContext.refreshTable(\"my_table\")\r\n```\r\n* In \"old\" versions of Hive, there are a few issues with Parquet:\r\n  - Not all the types were supported (ex.: DECIMAL, DATE)\r\n  - Bugs (now fixed), for instance when you had lots of nulls in your parquet file\r\n\r\nNote that usually, even if your SQLContext is a Hive context, Scala will have it instantiated as a \"regular\" Spark Shell, so might need to do this little trick:\r\n```\r\nsqlContext.asInstanceOf[org.apache.spark.sql.hive.HiveContext].refreshTable(\"my_db.my_table\")\r\n```",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194165_1914429538",
      "id": "20160326-141634_2127267692",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eTo take into account when saving to Hive:\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eIf you intend to use your tables outside Spark (Tableau, etc), you might want to disable the metastore conversion by setting the property \u0026ldquo;spark.sql.hive.convertMetastoreParquet\u0026rdquo; to false. You can do that when starting your shell:\u003cpre\u003e\u003ccode\u003espark-shell \u0026lt;other_options\u0026gt; --conf spark.sql.hive.convertMetastoreParquet\u003dfalse\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003eSpark caches table metadata, meaning that, if you update your table from outside the spark shell, you should refresh it:\u003cpre\u003e\u003ccode\u003esqlContext.refreshTable(\"my_table\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003eIn \u0026ldquo;old\u0026rdquo; versions of Hive, there are a few issues with Parquet:\u003c/li\u003e\n\u003cli\u003eNot all the types were supported (ex.: DECIMAL, DATE)\u003c/li\u003e\n\u003cli\u003eBugs (now fixed), for instance when you had lots of nulls in your parquet file\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that usually, even if your SQLContext is a Hive context, Scala will have it instantiated as a \u0026ldquo;regular\u0026rdquo; Spark Shell, so might need to do this little trick:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.asInstanceOf[org.apache.spark.sql.hive.HiveContext].refreshTable(\"my_db.my_table\")\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//Fails...\nsqlContext.refreshTable(\"my_db.my_table\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194166_1915583785",
      "id": "20160326-141634_1680344188",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:52: error: value refreshTable is not a member of org.apache.spark.sql.SQLContext\n              sqlContext.refreshTable(\"my_db.my_table\")\n                         ^\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sqlContext.asInstanceOf[org.apache.spark.sql.hive.HiveContext].refreshTable(\"my_db.my_table\")",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194166_1915583785",
      "id": "20160326-141634_2134809666",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md #### Large \"pipelines\": Checkpointing your DataFrames",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194166_1915583785",
      "id": "20160326-141634_1736766033",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eLarge \u0026ldquo;pipelines\u0026rdquo;: Checkpointing your DataFrames\u003c/h4\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md When working on large pipelines, you will start filtering, joining, selecting, doing unions of DataFrames.. At the end, chances are that you DF execution plans will become too large, requiring a lot of stages to get there.\r\n \r\nIf possible, you can \"checkpoint\" your DataFrame to a file or table and read it back. This way you \"start fresh\" with your DF, and this will usually enhance the performance. You can also cache your DF, but you can still get the slowness, for instance, if some of parts are removed from memory, if some of the executors die, or if you have dynamic allocation enabled and are not using something like Tachyon (this would mean that the cache is stored in the executors, so when they are decomissioned, so is the cache).\r\n\r\nLet\u0027s show the difference with an example",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194166_1915583785",
      "id": "20160326-141634_1228986607",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eWhen working on large pipelines, you will start filtering, joining, selecting, doing unions of DataFrames.. At the end, chances are that you DF execution plans will become too large, requiring a lot of stages to get there.\u003c/p\u003e\n\u003cp\u003eIf possible, you can \u0026ldquo;checkpoint\u0026rdquo; your DataFrame to a file or table and read it back. This way you \u0026ldquo;start fresh\u0026rdquo; with your DF, and this will usually enhance the performance. You can also cache your DF, but you can still get the slowness, for instance, if some of parts are removed from memory, if some of the executors die, or if you have dynamic allocation enabled and are not using something like Tachyon (this would mean that the cache is stored in the executors, so when they are decomissioned, so is the cache).\u003c/p\u003e\n\u003cp\u003eLet\u0027s show the difference with an example\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val myDf \u003d sc.parallelize(Array(\r\n  (1, \"a\", 5.0, true),\r\n  (2, null, -1.3, false),\r\n  (3, \"b\", 321.09, true)\r\n)).toDF(\"id\", \"col_a\", \"col_b\", \"col_c\")\r\nval myDf2 \u003d myDf.select(\r\n  \u0027id as \"id2\",\r\n  \u0027col_a as \"col_a2\",\r\n  \u0027col_b as \"col_b2\",\r\n  \u0027col_c as \"col_c2\"\r\n)\r\nval myDfStep1 \u003d myDf.join(myDf2)\r\n\r\nval myDfStep2 \u003d myDfStep1.filter(\"col_b \u003e 0\").unionAll(myDfStep1)\r\nmyDfStep2.show",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194167_1915199036",
      "id": "20160326-141634_1080187003",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "myDf: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string, col_b: double, col_c: boolean]\nmyDf2: org.apache.spark.sql.DataFrame \u003d [id2: int, col_a2: string, col_b2: double, col_c2: boolean]\nmyDfStep1: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string, col_b: double, col_c: boolean, id2: int, col_a2: string, col_b2: double, col_c2: boolean]\nmyDfStep2: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string, col_b: double, col_c: boolean, id2: int, col_a2: string, col_b2: double, col_c2: boolean]\n+---+-----+------+-----+---+------+------+------+\n| id|col_a| col_b|col_c|id2|col_a2|col_b2|col_c2|\n+---+-----+------+-----+---+------+------+------+\n|  1|    a|   5.0| true|  1|     a|   5.0|  true|\n|  1|    a|   5.0| true|  2|  null|  -1.3| false|\n|  1|    a|   5.0| true|  3|     b|321.09|  true|\n|  3|    b|321.09| true|  1|     a|   5.0|  true|\n|  3|    b|321.09| true|  2|  null|  -1.3| false|\n|  3|    b|321.09| true|  3|     b|321.09|  true|\n|  1|    a|   5.0| true|  1|     a|   5.0|  true|\n|  1|    a|   5.0| true|  2|  null|  -1.3| false|\n|  1|    a|   5.0| true|  3|     b|321.09|  true|\n|  2| null|  -1.3|false|  1|     a|   5.0|  true|\n|  3|    b|321.09| true|  1|     a|   5.0|  true|\n|  2| null|  -1.3|false|  2|  null|  -1.3| false|\n|  2| null|  -1.3|false|  3|     b|321.09|  true|\n|  3|    b|321.09| true|  2|  null|  -1.3| false|\n|  3|    b|321.09| true|  3|     b|321.09|  true|\n+---+-----+------+-----+---+------+------+------+\n\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "myDfStep2.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194167_1915199036",
      "id": "20160326-141634_2023691330",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003d\u003d Physical Plan \u003d\u003d\nUnion\n:- CartesianProduct\n:  :- ConvertToSafe\n:  :  +- Project [_1#499 AS id#503,_2#500 AS col_a#504,_3#501 AS col_b#505,_4#502 AS col_c#506]\n:  :     +- Filter (_3#501 \u003e 0.0)\n:  :        +- Scan ExistingRDD[_1#499,_2#500,_3#501,_4#502] \n:  +- ConvertToSafe\n:     +- Project [_1#499 AS id2#507,_2#500 AS col_a2#508,_3#501 AS col_b2#509,_4#502 AS col_c2#510]\n:        +- Scan ExistingRDD[_1#499,_2#500,_3#501,_4#502] \n+- CartesianProduct\n   :- ConvertToSafe\n   :  +- Project [_1#499 AS id#503,_2#500 AS col_a#504,_3#501 AS col_b#505,_4#502 AS col_c#506]\n   :     +- Scan ExistingRDD[_1#499,_2#500,_3#501,_4#502] \n   +- ConvertToSafe\n      +- Project [_1#499 AS id2#507,_2#500 AS col_a2#508,_3#501 AS col_b2#509,_4#502 AS col_c2#510]\n         +- Scan ExistingRDD[_1#499,_2#500,_3#501,_4#502]\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md Now, let\u0027s save \"myDfStep2\" to a parquet file and read it back, to compare the physical plans",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194167_1915199036",
      "id": "20160326-141634_334182124",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eNow, let\u0027s save \u0026ldquo;myDfStep2\u0026rdquo; to a parquet file and read it back, to compare the physical plans\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "myDfStep2.write.mode(\"overwrite\").parquet(\"my_df_step2.parquet\")\r\nval checkpointedMyDfStep2 \u003d sqlContext.read.parquet(\"my_df_step2.parquet\")\r\ncheckpointedMyDfStep2.explain",
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194168_1913275291",
      "id": "20160326-141634_929221163",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "checkpointedMyDfStep2: org.apache.spark.sql.DataFrame \u003d [id: int, col_a: string, col_b: double, col_c: boolean, id2: int, col_a2: string, col_b2: double, col_c2: boolean]\n\u003d\u003d Physical Plan \u003d\u003d\nScan ParquetRelation[id#520,col_a#521,col_b#522,col_c#523,id2#524,col_a2#525,col_b2#526,col_c2#527] InputPaths: hdfs://sandbox.hortonworks.com:8020/user/zeppelin/my_df_step2.parquet\n"
      },
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Mar 26, 2016 2:16:34 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459016194168_1913275291",
      "id": "20160326-141634_919005557",
      "dateCreated": "Mar 26, 2016 2:16:34 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Intro To DataFrames",
  "id": "2BFMAUGYS",
  "angularObjects": {
    "2BEE45P7U": [],
    "2BGQEV4Y3": [],
    "2BFDF3BYU": [],
    "2BETJY7PM": [],
    "2BFR8XRYX": [],
    "2BFF67ZST": [],
    "2BE2EM7JR": [],
    "2BGUFQF41": [],
    "2BGG98VU4": [],
    "2BE4WZYUF": [],
    "2BH62WJ6K": [],
    "2BEB795XE": [],
    "2BEBCPQYH": [],
    "2BHU1MN1J": [],
    "2BE95CB1D": [],
    "2BEUC65Z3": [],
    "2BFP1BUYR": [],
    "2BHAXJQ9H": []
  },
  "config": {},
  "info": {}
}